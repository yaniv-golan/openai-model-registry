{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenAI Model Registry Documentation","text":"<p>Welcome to the documentation for OpenAI Model Registry, a lightweight Python package for validating OpenAI model parameters and capabilities.</p>"},{"location":"#why-use-openai-model-registry","title":"Why Use OpenAI Model Registry?","text":"<p>OpenAI's models have different context-window sizes, parameter ranges, and feature support. If you guess wrong, the API returns an error\u2014often in production.</p> <p>OpenAI Model Registry keeps an up-to-date, local catalog of every model's limits and capabilities, letting you validate calls before you send them.</p> <p>Typical benefits:</p> <ul> <li>Catch invalid <code>temperature</code>, <code>top_p</code>, and <code>max_tokens</code> values locally.</li> <li>Swap models confidently by comparing context windows and features.</li> <li>Work fully offline\u2014perfect for CI or air-gapped environments.</li> <li>Automatic updates from GitHub releases keep your data current.</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>OpenAI Model Registry provides a centralized registry of OpenAI model information with automatic updates from GitHub releases. It validates parameters against model-specific schemas, retrieves model capabilities, and includes comprehensive deprecation tracking with accurate model metadata. It offers programmatic access to model-card data (capabilities, parameters, pricing, deprecations) for both OpenAI and Azure providers. Pricing is kept up to date automatically via CI using ostruct. The registry uses semantic versioning for schema compatibility and provides robust fallback mechanisms for offline usage.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install openai-model-registry\n</code></pre>"},{"location":"#for-ai-assistants-and-llms","title":"For AI Assistants and LLMs","text":"<p>This project includes an <code>llms.txt</code> file following the llmstxt.org specification. This provides comprehensive, token-efficient documentation designed specifically for AI assistants to understand and help users work with the OpenAI Model Registry programmatically.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from openai_model_registry import ModelRegistry\n\n# Get the registry instance\nregistry = ModelRegistry.get_default()\n\n# Get model capabilities\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\nprint(f\"Context window: {capabilities.context_window}\")\nprint(f\"Max output tokens: {capabilities.max_output_tokens}\")\nprint(f\"Supports streaming: {capabilities.supports_streaming}\")\n# Expected output: Context window: 128000\n#                  Max output tokens: 16384\n#                  Supports streaming: True\n\n# Check deprecation status\nprint(f\"Deprecation status: {capabilities.deprecation.status}\")\nif capabilities.is_deprecated:\n    print(\"\u26a0\ufe0f  This model is deprecated\")\n# Expected output: Deprecation status: active\n\n# Check for data updates\nif registry.check_data_updates():\n    print(\"Updates are available!\")\n    registry.update_data()  # Update to latest model data\n</code></pre> <p>\u27a1\ufe0f Keeping it fresh: The registry automatically checks for updates, or you can manually run <code>registry.update_data()</code> or the CLI <code>python -m openai_model_registry.scripts.data_update update</code>.</p>"},{"location":"#important-notes","title":"Important Notes","text":"<p>\ud83d\udd35 Azure OpenAI Users: If you're using Azure OpenAI endpoints, please be aware of platform-specific limitations, especially regarding web search capabilities. See our Azure OpenAI Usage Guide for detailed guidance.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Please see the API Reference for detailed information about the package's classes and methods.</p>"},{"location":"#user-guide","title":"User Guide","text":"<p>For more detailed usage instructions, see the User Guide.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please see our Contributing Guide for more details.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>View on GitHub</p> <p>This page is best viewed on GitHub directly: CHANGELOG.md</p> <p>--8\\&lt;-- \"CHANGELOG.md\"</p>"},{"location":"contributing/","title":"Contributing","text":"<p>View on GitHub</p> <p>This page is best viewed on GitHub directly: CONTRIBUTING.md</p> <p>--8\\&lt;-- \"CONTRIBUTING.md\"</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all public classes and functions in the OpenAI Model Registry package.</p>"},{"location":"api/#main-classes","title":"Main Classes","text":"<ul> <li>ModelRegistry: The main entry point for the package, providing access to model capabilities and validation.</li> <li>ModelCapabilities: Represents the capabilities of a specific model, including context window, max tokens, and parameter constraints.</li> </ul>"},{"location":"api/#modules","title":"Modules","text":"<p>The package is organized into the following modules:</p> <ul> <li><code>openai_model_registry.registry</code>: Contains the main registry functionality.</li> <li><code>openai_model_registry.capabilities</code>: Contains classes related to model capabilities.</li> <li><code>openai_model_registry.validation</code>: Contains parameter validation functionality.</li> <li><code>openai_model_registry.config</code>: Contains configuration-related functionality.</li> <li><code>openai_model_registry.exceptions</code>: Contains package-specific exceptions.</li> </ul>"},{"location":"api/model-capabilities/","title":"ModelCapabilities","text":"<p>The <code>ModelCapabilities</code> class represents the capabilities, constraints, and parameters for a specific OpenAI model.</p>"},{"location":"api/model-capabilities/#websearchbilling","title":"WebSearchBilling","text":"<p>Web search billing is represented by a dedicated dataclass:</p> <p>options: show_root_heading: false show_source: true</p>"},{"location":"api/model-capabilities/#openai_model_registry.registry.WebSearchBilling","title":"<code>openai_model_registry.registry.WebSearchBilling</code>  <code>dataclass</code>","text":"<p>Web search billing policy and rates for a model.</p> <ul> <li>call_fee_per_1000: flat fee per 1000 calls</li> <li>content_token_policy: whether content tokens are included or billed at model rate</li> <li>currency: ISO currency code (default USD)</li> <li>notes: optional free-form notes</li> </ul> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@dataclass(frozen=True)\nclass WebSearchBilling:\n    \"\"\"Web search billing policy and rates for a model.\n\n    - call_fee_per_1000: flat fee per 1000 calls\n    - content_token_policy: whether content tokens are included or billed at model rate\n    - currency: ISO currency code (default USD)\n    - notes: optional free-form notes\n    \"\"\"\n\n    call_fee_per_1000: float\n    content_token_policy: Literal[\"included_in_call_fee\", \"billed_at_model_rate\"]\n    currency: str = \"USD\"\n    notes: Optional[str] = None\n\n    def __post_init__(self) -&gt; None:\n        if self.call_fee_per_1000 &lt; 0:\n            raise ValueError(\"call_fee_per_1000 must be non-negative\")\n</code></pre>"},{"location":"api/model-capabilities/#class-reference","title":"Class Reference","text":"<p>options: show_root_heading: false show_source: true</p>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities","title":"<code>openai_model_registry.registry.ModelCapabilities</code>","text":"<p>Represents the capabilities of a model.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class ModelCapabilities:\n    \"\"\"Represents the capabilities of a model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        openai_model_name: str,\n        context_window: int,\n        max_output_tokens: int,\n        deprecation: DeprecationInfo,\n        supports_vision: bool = False,\n        supports_functions: bool = False,\n        supports_streaming: bool = False,\n        supports_structured: bool = False,\n        supports_web_search: bool = False,\n        supports_audio: bool = False,\n        supports_json_mode: bool = False,\n        pricing: Optional[\"PricingInfo\"] = None,\n        input_modalities: Optional[List[str]] = None,\n        output_modalities: Optional[List[str]] = None,\n        min_version: Optional[ModelVersion] = None,\n        aliases: Optional[List[str]] = None,\n        supported_parameters: Optional[List[ParameterReference]] = None,\n        constraints: Optional[Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]]] = None,\n        inline_parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        web_search_billing: Optional[\"WebSearchBilling\"] = None,\n    ):\n        \"\"\"Initialize model capabilities.\n\n        Args:\n            model_name: The model identifier in the registry\n            openai_model_name: The model name to use with OpenAI API\n            context_window: Maximum context window size in tokens\n            max_output_tokens: Maximum output tokens\n            deprecation: Deprecation metadata (mandatory in current schema)\n            supports_vision: Whether the model supports vision inputs\n            supports_functions: Whether the model supports function calling\n            supports_streaming: Whether the model supports streaming\n            supports_structured: Whether the model supports structured output\n            supports_web_search: Whether the model supports web search\n                (Chat API search-preview models or Responses API tool)\n            supports_audio: Whether the model supports audio inputs\n            supports_json_mode: Whether the model supports JSON mode\n            pricing: Pricing information for the model\n            input_modalities: List of supported input modalities (e.g., [\"text\", \"image\"]).\n            output_modalities: List of supported output modalities (e.g., [\"text\", \"image\"]).\n            min_version: Minimum version for dated model variants\n            aliases: List of aliases for this model\n            supported_parameters: List of parameter references supported by this model\n            constraints: Dictionary of constraints for validation\n            inline_parameters: Dictionary of inline parameter configurations from schema\n            web_search_billing: Optional web-search billing policy and rates for the model\n        \"\"\"\n        self.model_name = model_name\n        self.openai_model_name = openai_model_name\n        self.context_window = context_window\n        self.max_output_tokens = max_output_tokens\n        self.deprecation = deprecation\n        self.supports_vision = supports_vision\n        self.supports_functions = supports_functions\n        self.supports_streaming = supports_streaming\n        self.supports_structured = supports_structured\n        self.supports_web_search = supports_web_search\n        self.supports_audio = supports_audio\n        self.supports_json_mode = supports_json_mode\n        self.pricing = pricing\n        self.input_modalities = input_modalities or []\n        self.output_modalities = output_modalities or []\n        self.min_version = min_version\n        self.aliases = aliases or []\n        self.supported_parameters = supported_parameters or []\n        self._constraints = constraints or {}\n        self._inline_parameters = inline_parameters or {}\n        self.web_search_billing = web_search_billing\n\n    @property\n    def inline_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Inline parameter definitions for this model (if any).\"\"\"\n        return self._inline_parameters\n\n    @property\n    def is_sunset(self) -&gt; bool:\n        \"\"\"Check if the model is sunset.\"\"\"\n        return self.deprecation.status == \"sunset\"\n\n    @property\n    def is_deprecated(self) -&gt; bool:\n        \"\"\"Check if the model is deprecated or sunset.\"\"\"\n        return self.deprecation.status in [\"deprecated\", \"sunset\"]\n\n    def get_constraint(self, ref: str) -&gt; Optional[Union[NumericConstraint, EnumConstraint, ObjectConstraint]]:\n        \"\"\"Get a constraint by reference.\n\n        Args:\n            ref: Constraint reference (key in constraints dict)\n\n        Returns:\n            The constraint or None if not found\n        \"\"\"\n        return self._constraints.get(ref)\n\n    def validate_parameter(self, name: str, value: Any, used_params: Optional[Set[str]] = None) -&gt; None:\n        \"\"\"Validate a parameter against constraints.\n\n        Args:\n            name: Parameter name\n            value: Parameter value to validate\n            used_params: Optional set to track used parameters\n\n        Raises:\n            ParameterNotSupportedError: If the parameter is not supported\n            ConstraintNotFoundError: If a constraint reference is invalid\n            ModelRegistryError: If validation fails for other reasons\n        \"\"\"\n        # Track used parameters if requested\n        if used_params is not None:\n            used_params.add(name)\n\n        # Check if we have inline parameter constraints\n        if name in self._inline_parameters:\n            self._validate_inline_parameter(name, value)\n            return\n\n        # Find matching parameter reference\n        param_ref = next(\n            (p for p in self.supported_parameters if p.ref == name or p.ref.split(\".\")[-1] == name),\n            None,\n        )\n\n        if not param_ref:\n            # If we're validating a parameter explicitly, it should be supported\n            raise ParameterNotSupportedError(\n                f\"Parameter '{name}' is not supported for model '{self.model_name}'\",\n                param_name=name,\n                value=value,\n                model=self.model_name,\n            )\n\n        constraint = self.get_constraint(param_ref.ref)\n        if not constraint:\n            # If a parameter references a constraint, the constraint should exist\n            raise ConstraintNotFoundError(\n                f\"Constraint reference '{param_ref.ref}' not found for parameter '{name}'\",\n                ref=param_ref.ref,\n            )\n\n        # Validate based on constraint type\n        if isinstance(constraint, NumericConstraint):\n            constraint.validate(name=name, value=value)\n        elif isinstance(constraint, EnumConstraint):\n            constraint.validate(name=name, value=value)\n        elif isinstance(constraint, ObjectConstraint):\n            constraint.validate(name=name, value=value)\n        else:\n            # This shouldn't happen with proper type checking, but just in case\n            raise TypeError(f\"Unknown constraint type for '{name}': {type(constraint).__name__}\")\n\n    def validate_parameters(self, params: Dict[str, Any], used_params: Optional[Set[str]] = None) -&gt; None:\n        \"\"\"Validate multiple parameters against constraints.\n\n        Args:\n            params: Dictionary of parameter names and values to validate\n            used_params: Optional set to track used parameters\n\n        Raises:\n            ModelRegistryError: If validation fails for any parameter\n        \"\"\"\n        for name, value in params.items():\n            self.validate_parameter(name, value, used_params)\n\n    def _validate_inline_parameter(self, name: str, value: Any) -&gt; None:\n        \"\"\"Validate a parameter using inline parameter constraints.\n\n        Args:\n            name: Parameter name\n            value: Parameter value to validate\n\n        Raises:\n            ValidationError: If validation fails\n        \"\"\"\n        from .errors import ParameterValidationError\n\n        param_config = self._inline_parameters[name]\n        param_type = param_config.get(\"type\")\n\n        # Handle numeric parameters (temperature, top_p, etc.)\n        if param_type == \"number\":\n            if not isinstance(value, (int, float)):\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' expects a numeric value\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n            min_val = param_config.get(\"min\")\n            max_val = param_config.get(\"max\")\n\n            if min_val is not None and value &lt; min_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is below minimum {min_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            if max_val is not None and value &gt; max_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is above maximum {max_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n        # Handle canonical numeric schema (min_value/max_value)\n        elif param_type == \"numeric\":\n            allow_float = param_config.get(\"allow_float\", True)\n            allow_int = param_config.get(\"allow_int\", True)\n\n            if not isinstance(value, (int, float)):\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' expects a numeric value\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            # Enforce numeric subtype rules when provided\n            if isinstance(value, float) and not allow_float:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' does not allow float values\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n            if isinstance(value, int) and not allow_int:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' does not allow integer values\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            min_val = param_config.get(\"min_value\")\n            max_val = param_config.get(\"max_value\")\n\n            if min_val is not None and value &lt; min_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is below minimum {min_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            if max_val is not None and value &gt; max_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is above maximum {max_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n        # Handle integer parameters (max_tokens, etc.)\n        elif param_type == \"integer\":\n            if not isinstance(value, int):\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' expects an integer value\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n            # Support both min/max (models.yaml) and min_value/max_value (constraints)\n            min_val = param_config.get(\"min\") or param_config.get(\"min_value\")\n            max_val = param_config.get(\"max\") or param_config.get(\"max_value\")\n\n            if min_val is not None and value &lt; min_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is below minimum {min_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            if max_val is not None and value &gt; max_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is above maximum {max_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n        # Handle enum parameters declared inline\n        elif param_type == \"enum\":\n            allowed_values = param_config.get(\"enum\", [])\n            if value not in allowed_values:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value '{value}' is not one of: {', '.join(map(str, allowed_values))}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n</code></pre>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities-attributes","title":"Attributes","text":""},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities.inline_parameters","title":"<code>inline_parameters</code>  <code>property</code>","text":"<p>Inline parameter definitions for this model (if any).</p>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities.is_deprecated","title":"<code>is_deprecated</code>  <code>property</code>","text":"<p>Check if the model is deprecated or sunset.</p>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities.is_sunset","title":"<code>is_sunset</code>  <code>property</code>","text":"<p>Check if the model is sunset.</p>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities-functions","title":"Functions","text":""},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities.__init__","title":"<code>__init__(model_name, openai_model_name, context_window, max_output_tokens, deprecation, supports_vision=False, supports_functions=False, supports_streaming=False, supports_structured=False, supports_web_search=False, supports_audio=False, supports_json_mode=False, pricing=None, input_modalities=None, output_modalities=None, min_version=None, aliases=None, supported_parameters=None, constraints=None, inline_parameters=None, web_search_billing=None)</code>","text":"<p>Initialize model capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model identifier in the registry</p> required <code>openai_model_name</code> <code>str</code> <p>The model name to use with OpenAI API</p> required <code>context_window</code> <code>int</code> <p>Maximum context window size in tokens</p> required <code>max_output_tokens</code> <code>int</code> <p>Maximum output tokens</p> required <code>deprecation</code> <code>DeprecationInfo</code> <p>Deprecation metadata (mandatory in current schema)</p> required <code>supports_vision</code> <code>bool</code> <p>Whether the model supports vision inputs</p> <code>False</code> <code>supports_functions</code> <code>bool</code> <p>Whether the model supports function calling</p> <code>False</code> <code>supports_streaming</code> <code>bool</code> <p>Whether the model supports streaming</p> <code>False</code> <code>supports_structured</code> <code>bool</code> <p>Whether the model supports structured output</p> <code>False</code> <code>supports_web_search</code> <code>bool</code> <p>Whether the model supports web search (Chat API search-preview models or Responses API tool)</p> <code>False</code> <code>supports_audio</code> <code>bool</code> <p>Whether the model supports audio inputs</p> <code>False</code> <code>supports_json_mode</code> <code>bool</code> <p>Whether the model supports JSON mode</p> <code>False</code> <code>pricing</code> <code>Optional[PricingInfo]</code> <p>Pricing information for the model</p> <code>None</code> <code>input_modalities</code> <code>Optional[List[str]]</code> <p>List of supported input modalities (e.g., [\"text\", \"image\"]).</p> <code>None</code> <code>output_modalities</code> <code>Optional[List[str]]</code> <p>List of supported output modalities (e.g., [\"text\", \"image\"]).</p> <code>None</code> <code>min_version</code> <code>Optional[ModelVersion]</code> <p>Minimum version for dated model variants</p> <code>None</code> <code>aliases</code> <code>Optional[List[str]]</code> <p>List of aliases for this model</p> <code>None</code> <code>supported_parameters</code> <code>Optional[List[ParameterReference]]</code> <p>List of parameter references supported by this model</p> <code>None</code> <code>constraints</code> <code>Optional[Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]]]</code> <p>Dictionary of constraints for validation</p> <code>None</code> <code>inline_parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Dictionary of inline parameter configurations from schema</p> <code>None</code> <code>web_search_billing</code> <code>Optional[WebSearchBilling]</code> <p>Optional web-search billing policy and rates for the model</p> <code>None</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    openai_model_name: str,\n    context_window: int,\n    max_output_tokens: int,\n    deprecation: DeprecationInfo,\n    supports_vision: bool = False,\n    supports_functions: bool = False,\n    supports_streaming: bool = False,\n    supports_structured: bool = False,\n    supports_web_search: bool = False,\n    supports_audio: bool = False,\n    supports_json_mode: bool = False,\n    pricing: Optional[\"PricingInfo\"] = None,\n    input_modalities: Optional[List[str]] = None,\n    output_modalities: Optional[List[str]] = None,\n    min_version: Optional[ModelVersion] = None,\n    aliases: Optional[List[str]] = None,\n    supported_parameters: Optional[List[ParameterReference]] = None,\n    constraints: Optional[Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]]] = None,\n    inline_parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    web_search_billing: Optional[\"WebSearchBilling\"] = None,\n):\n    \"\"\"Initialize model capabilities.\n\n    Args:\n        model_name: The model identifier in the registry\n        openai_model_name: The model name to use with OpenAI API\n        context_window: Maximum context window size in tokens\n        max_output_tokens: Maximum output tokens\n        deprecation: Deprecation metadata (mandatory in current schema)\n        supports_vision: Whether the model supports vision inputs\n        supports_functions: Whether the model supports function calling\n        supports_streaming: Whether the model supports streaming\n        supports_structured: Whether the model supports structured output\n        supports_web_search: Whether the model supports web search\n            (Chat API search-preview models or Responses API tool)\n        supports_audio: Whether the model supports audio inputs\n        supports_json_mode: Whether the model supports JSON mode\n        pricing: Pricing information for the model\n        input_modalities: List of supported input modalities (e.g., [\"text\", \"image\"]).\n        output_modalities: List of supported output modalities (e.g., [\"text\", \"image\"]).\n        min_version: Minimum version for dated model variants\n        aliases: List of aliases for this model\n        supported_parameters: List of parameter references supported by this model\n        constraints: Dictionary of constraints for validation\n        inline_parameters: Dictionary of inline parameter configurations from schema\n        web_search_billing: Optional web-search billing policy and rates for the model\n    \"\"\"\n    self.model_name = model_name\n    self.openai_model_name = openai_model_name\n    self.context_window = context_window\n    self.max_output_tokens = max_output_tokens\n    self.deprecation = deprecation\n    self.supports_vision = supports_vision\n    self.supports_functions = supports_functions\n    self.supports_streaming = supports_streaming\n    self.supports_structured = supports_structured\n    self.supports_web_search = supports_web_search\n    self.supports_audio = supports_audio\n    self.supports_json_mode = supports_json_mode\n    self.pricing = pricing\n    self.input_modalities = input_modalities or []\n    self.output_modalities = output_modalities or []\n    self.min_version = min_version\n    self.aliases = aliases or []\n    self.supported_parameters = supported_parameters or []\n    self._constraints = constraints or {}\n    self._inline_parameters = inline_parameters or {}\n    self.web_search_billing = web_search_billing\n</code></pre>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities.get_constraint","title":"<code>get_constraint(ref)</code>","text":"<p>Get a constraint by reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>Constraint reference (key in constraints dict)</p> required <p>Returns:</p> Type Description <code>Optional[Union[NumericConstraint, EnumConstraint, ObjectConstraint]]</code> <p>The constraint or None if not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_constraint(self, ref: str) -&gt; Optional[Union[NumericConstraint, EnumConstraint, ObjectConstraint]]:\n    \"\"\"Get a constraint by reference.\n\n    Args:\n        ref: Constraint reference (key in constraints dict)\n\n    Returns:\n        The constraint or None if not found\n    \"\"\"\n    return self._constraints.get(ref)\n</code></pre>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities.validate_parameter","title":"<code>validate_parameter(name, value, used_params=None)</code>","text":"<p>Validate a parameter against constraints.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>Any</code> <p>Parameter value to validate</p> required <code>used_params</code> <code>Optional[Set[str]]</code> <p>Optional set to track used parameters</p> <code>None</code> <p>Raises:</p> Type Description <code>ParameterNotSupportedError</code> <p>If the parameter is not supported</p> <code>ConstraintNotFoundError</code> <p>If a constraint reference is invalid</p> <code>ModelRegistryError</code> <p>If validation fails for other reasons</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def validate_parameter(self, name: str, value: Any, used_params: Optional[Set[str]] = None) -&gt; None:\n    \"\"\"Validate a parameter against constraints.\n\n    Args:\n        name: Parameter name\n        value: Parameter value to validate\n        used_params: Optional set to track used parameters\n\n    Raises:\n        ParameterNotSupportedError: If the parameter is not supported\n        ConstraintNotFoundError: If a constraint reference is invalid\n        ModelRegistryError: If validation fails for other reasons\n    \"\"\"\n    # Track used parameters if requested\n    if used_params is not None:\n        used_params.add(name)\n\n    # Check if we have inline parameter constraints\n    if name in self._inline_parameters:\n        self._validate_inline_parameter(name, value)\n        return\n\n    # Find matching parameter reference\n    param_ref = next(\n        (p for p in self.supported_parameters if p.ref == name or p.ref.split(\".\")[-1] == name),\n        None,\n    )\n\n    if not param_ref:\n        # If we're validating a parameter explicitly, it should be supported\n        raise ParameterNotSupportedError(\n            f\"Parameter '{name}' is not supported for model '{self.model_name}'\",\n            param_name=name,\n            value=value,\n            model=self.model_name,\n        )\n\n    constraint = self.get_constraint(param_ref.ref)\n    if not constraint:\n        # If a parameter references a constraint, the constraint should exist\n        raise ConstraintNotFoundError(\n            f\"Constraint reference '{param_ref.ref}' not found for parameter '{name}'\",\n            ref=param_ref.ref,\n        )\n\n    # Validate based on constraint type\n    if isinstance(constraint, NumericConstraint):\n        constraint.validate(name=name, value=value)\n    elif isinstance(constraint, EnumConstraint):\n        constraint.validate(name=name, value=value)\n    elif isinstance(constraint, ObjectConstraint):\n        constraint.validate(name=name, value=value)\n    else:\n        # This shouldn't happen with proper type checking, but just in case\n        raise TypeError(f\"Unknown constraint type for '{name}': {type(constraint).__name__}\")\n</code></pre>"},{"location":"api/model-capabilities/#openai_model_registry.registry.ModelCapabilities.validate_parameters","title":"<code>validate_parameters(params, used_params=None)</code>","text":"<p>Validate multiple parameters against constraints.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>Dictionary of parameter names and values to validate</p> required <code>used_params</code> <code>Optional[Set[str]]</code> <p>Optional set to track used parameters</p> <code>None</code> <p>Raises:</p> Type Description <code>ModelRegistryError</code> <p>If validation fails for any parameter</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def validate_parameters(self, params: Dict[str, Any], used_params: Optional[Set[str]] = None) -&gt; None:\n    \"\"\"Validate multiple parameters against constraints.\n\n    Args:\n        params: Dictionary of parameter names and values to validate\n        used_params: Optional set to track used parameters\n\n    Raises:\n        ModelRegistryError: If validation fails for any parameter\n    \"\"\"\n    for name, value in params.items():\n        self.validate_parameter(name, value, used_params)\n</code></pre>"},{"location":"api/model-capabilities/#notes","title":"Notes","text":"<ul> <li><code>input_modalities</code> and <code>output_modalities</code> are provided in addition to the legacy <code>modalities</code> (input) for clarity.</li> </ul>"},{"location":"api/model-capabilities/#usage-examples","title":"Usage Examples","text":""},{"location":"api/model-capabilities/#accessing-basic-properties","title":"Accessing Basic Properties","text":"<pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Access basic properties\nprint(f\"Model name: {capabilities.openai_model_name}\")\nprint(f\"Context window: {capabilities.context_window}\")\nprint(f\"Max output tokens: {capabilities.max_output_tokens}\")\nprint(f\"Supports streaming: {capabilities.supports_streaming}\")\nprint(f\"Supports structured output: {capabilities.supports_structured}\")\n\n# Expected output: Model name: gpt-4o\n#                  Context window: 128000\n#                  Max output tokens: 16384\n#                  Supports streaming: True\n#                  Supports structured output: True\n</code></pre>"},{"location":"api/model-capabilities/#validating-parameters","title":"Validating Parameters","text":"<pre><code>from openai_model_registry import ModelRegistry, ModelRegistryError\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Validate a parameter\ntry:\n    capabilities.validate_parameter(\"temperature\", 0.7)\n    print(\"Temperature 0.7 is valid\")\nexcept ModelRegistryError as e:\n    print(f\"Invalid parameter: {e}\")\n\n# Validate with context (tracking used parameters)\nused_params = set()\ncapabilities.validate_parameter(\"temperature\", 0.7, used_params)\nprint(f\"Used parameters: {used_params}\")  # Contains 'temperature'\n# Expected output: Temperature 0.7 is valid\n#                  Used parameters: {'temperature'}\n\n# Validate multiple parameters\nparams_to_validate = {\"temperature\": 0.7, \"top_p\": 0.9, \"max_completion_tokens\": 500}\n\nfor param_name, value in params_to_validate.items():\n    try:\n        capabilities.validate_parameter(param_name, value, used_params)\n        print(f\"\u2713 {param_name}={value} is valid\")\n    except ModelRegistryError as e:\n        print(f\"\u2717 {param_name}={value} is invalid: {e}\")\n</code></pre>"},{"location":"api/model-capabilities/#working-with-parameter-constraints","title":"Working with Parameter Constraints","text":"<pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Get a specific constraint\ntemperature_constraint = capabilities.get_constraint(\"temperature\")\nif temperature_constraint:\n    print(f\"Type: {type(temperature_constraint).__name__}\")\n    print(f\"Min value: {temperature_constraint.min_value}\")\n    print(f\"Max value: {temperature_constraint.max_value}\")\n    print(f\"Description: {temperature_constraint.description}\")\n\n# List all parameter references\nfor param_ref in capabilities.supported_parameters:\n    constraint = capabilities.get_constraint(param_ref.ref)\n    print(f\"Parameter: {param_ref.ref}\")\n    print(f\"  Description: {param_ref.description}\")\n    print(f\"  Constraint type: {type(constraint).__name__ if constraint else 'None'}\")\n</code></pre>"},{"location":"api/model-capabilities/#creating-custom-capabilities","title":"Creating Custom Capabilities","text":"<pre><code>from openai_model_registry import ModelRegistry\nfrom openai_model_registry.registry import ModelCapabilities\nfrom openai_model_registry.constraints import NumericConstraint, EnumConstraint\nfrom typing import Dict, Union\n\n# Get existing constraints for reference\nregistry = ModelRegistry.get_default()\nbase_capabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Create custom capabilities (with basic properties)\ncustom_capabilities = ModelCapabilities(\n    model_name=\"custom-model\",\n    openai_model_name=\"custom-model\",\n    context_window=8192,\n    max_output_tokens=4096,\n    supports_streaming=True,\n    supports_structured=True,\n)\n\n# Copy supported parameters from base model\ncustom_capabilities.supported_parameters = base_capabilities.supported_parameters\n\n# Add constraints manually\nconstraints: Dict[str, Union[NumericConstraint, EnumConstraint]] = {\n    \"temperature\": NumericConstraint(\n        min_value=0.0,\n        max_value=1.0,\n        allow_float=True,\n        allow_int=True,\n        description=\"Custom temperature description\",\n    ),\n    \"response_format\": EnumConstraint(\n        allowed_values=[\"text\", \"json_schema\"],\n        description=\"Custom response format description\",\n    ),\n}\ncustom_capabilities._constraints = constraints\n\n# Use custom capabilities\ncustom_capabilities.validate_parameter(\"temperature\", 0.7)\n</code></pre> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Check if model is deprecated\nif capabilities.is_deprecated:\n    print(f\"\u26a0\ufe0f  Model is deprecated since {capabilities.deprecation.deprecation_date}\")\n    if capabilities.deprecation.sunset_date:\n        print(f\"\ud83d\udeab Model will be sunset on {capabilities.deprecation.sunset_date}\")\n</code></pre> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Validate parameters\ntry:\n    capabilities.validate_parameter(\"temperature\", 0.7)\n    print(\"\u2705 Temperature value is valid\")\nexcept ValueError as e:\n    print(f\"\u274c Invalid temperature: {e}\")\n</code></pre> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Check feature support\nif capabilities.supports_structured:\n    print(\"\u2705 Model supports structured output\")\nif capabilities.supports_streaming:\n    print(\"\u2705 Model supports streaming\")\n</code></pre>"},{"location":"api/model-registry/","title":"ModelRegistry","text":"<p>The <code>ModelRegistry</code> class is the primary entry point for accessing model capabilities and validating parameters.</p>"},{"location":"api/model-registry/#class-reference","title":"Class Reference","text":"<p>options: show_root_heading: false show_source: true</p> <p>options: show_root_heading: true show_source: true</p>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry","title":"<code>openai_model_registry.registry.ModelRegistry</code>","text":"<p>Registry for model capabilities and validation.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class ModelRegistry:\n    \"\"\"Registry for model capabilities and validation.\"\"\"\n\n    _default_instance: Optional[\"ModelRegistry\"] = None\n    # Pre-compile regex patterns for improved performance\n    _DATE_PATTERN = re.compile(r\"^(.*)-(\\d{4}-\\d{2}-\\d{2})$\")\n    _IS_DATED_MODEL_PATTERN = re.compile(r\".*-\\d{4}-\\d{2}-\\d{2}$\")\n    _instance_lock = threading.RLock()\n\n    @classmethod\n    def get_instance(cls) -&gt; \"ModelRegistry\":\n        \"\"\"Get the default registry instance.\n\n        Prefer :py:meth:`get_default` for clarity; this alias remains for\n        brevity and historical usage but is *not* a separate code path.\n\n        Returns:\n            The singleton :class:`ModelRegistry` instance.\n        \"\"\"\n        return cls.get_default()\n\n    @classmethod\n    def get_default(cls) -&gt; \"ModelRegistry\":\n        \"\"\"Get the default registry instance with standard configuration.\n\n        Returns:\n            The default ModelRegistry instance\n        \"\"\"\n        with cls._instance_lock:\n            if cls._default_instance is None:\n                cls._default_instance = cls()\n            return cls._default_instance\n\n    def __init__(self, config: Optional[RegistryConfig] = None):\n        \"\"\"Initialize a new registry instance.\n\n        Args:\n            config: Configuration for this registry instance. If None, default\n                   configuration is used.\n        \"\"\"\n        self.config = config or RegistryConfig()\n        self._capabilities: Dict[str, ModelCapabilities] = {}\n        self._constraints: Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]] = {}\n        self._capabilities_lock = threading.RLock()\n        # Stats for last load/dump operations (for observability)\n        self._last_load_stats: Dict[str, Any] = {}\n\n        # Initialize DataManager for model and overrides data\n        self._data_manager = DataManager()\n\n        # Set up caching for get_capabilities\n        self.get_capabilities = functools.lru_cache(maxsize=self.config.cache_size)(self._get_capabilities_impl)\n\n        # Auto-copy default constraint files to user directory if they don't exist\n        if not config or not config.constraints_path:\n            try:\n                copy_default_to_user_config(PARAM_CONSTRAINTS_FILENAME)\n            except OSError as e:\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to copy default constraint config: {e}\",\n                    error=str(e),\n                )\n\n        # Check for data updates if auto_update is enabled\n        if self.config.auto_update and self._data_manager.should_update_data():\n            try:\n                success = self._data_manager.check_for_updates()\n                if success:\n                    log_info(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Auto-update completed successfully\",\n                    )\n                    # Reload capabilities after successful auto-update\n                    self._load_capabilities()\n            except Exception as e:\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to auto-update data: {e}\",\n                    error=str(e),\n                )\n\n        self._load_constraints()\n        self._load_capabilities()\n\n    def _load_config(self) -&gt; ConfigResult:\n        \"\"\"Load model configuration from file using DataManager.\n\n        Returns:\n            ConfigResult: Result of the configuration loading operation\n        \"\"\"\n        try:\n            # Use DataManager to get models.yaml content\n            content = self._data_manager.get_data_file_content(\"models.yaml\")\n            if content is None:\n                error_msg = \"Could not load models.yaml from DataManager\"\n                log_error(LogEvent.MODEL_REGISTRY, error_msg)\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            # Validate YAML content before parsing\n            if not content.strip():\n                error_msg = \"models.yaml file is empty\"\n                log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            # Check for obvious corruption patterns (heuristic)\n            if \"&amp;\" in content and \"*\" in content:\n                try:\n                    import re\n\n                    anchor_pattern = r\"&amp;(\\w+)\"\n                    reference_pattern = r\"\\*(\\w+)\"\n                    anchors = set(re.findall(anchor_pattern, content))\n                    references = set(re.findall(reference_pattern, content))\n\n                    for anchor in anchors:\n                        if anchor in references:\n                            circular_pattern = rf\"&amp;{anchor}.*?\\*{anchor}\"\n                            if re.search(circular_pattern, content, re.DOTALL):\n                                error_msg = f\"Detected circular reference in YAML with anchor '{anchor}'\"\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    error_msg,\n                                    path=\"models.yaml\",\n                                )\n                                return ConfigResult(\n                                    success=False,\n                                    error=error_msg,\n                                    path=\"models.yaml\",\n                                )\n                except Exception:\n                    # If our heuristic check fails, continue with normal parsing\n                    pass\n\n            data = yaml.safe_load(content)\n\n            # Additional validation after YAML parsing\n            if data is None:\n                error_msg = \"YAML parsing resulted in None - file may be corrupted\"\n                log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            if not isinstance(data, dict):\n                error_msg = (\n                    f\"Invalid configuration format in models.yaml: expected dictionary, got {type(data).__name__}\"\n                )\n                log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            # Load and apply provider overrides\n            try:\n                overrides_data = self._load_overrides()\n                if overrides_data:\n                    data = self._apply_overrides(data, overrides_data)\n            except Exception as e:\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to load or apply overrides: {e}\",\n                    error=str(e),\n                )\n\n            # Schema version is declared inside the YAML itself; the loader\n            # supports schema v1.x with top-level ``models`` mapping.\n            return ConfigResult(success=True, data=data, path=\"models.yaml\")\n        except yaml.YAMLError as e:\n            error_msg = f\"YAML parsing error in models.yaml: {e}\"\n            log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n            return ConfigResult(\n                success=False,\n                error=error_msg,\n                exception=e,\n                path=\"models.yaml\",\n            )\n        except Exception as e:\n            error_msg = f\"Error loading model registry config: {e}\"\n            log_warning(LogEvent.MODEL_REGISTRY, error_msg)\n            return ConfigResult(\n                success=False,\n                error=error_msg,\n                exception=e,\n                path=\"models.yaml\",\n            )\n\n    def _load_overrides(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Load provider overrides from overrides.yaml.\n\n        Returns:\n            Dictionary containing overrides data, or None if not available\n        \"\"\"\n        try:\n            content = self._data_manager.get_data_file_content(\"overrides.yaml\")\n            if content is None:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"No overrides.yaml file available\",\n                )\n                return None\n\n            if not content.strip():\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"overrides.yaml file is empty\",\n                )\n                return None\n\n            data = yaml.safe_load(content)\n            if not isinstance(data, dict):\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Invalid overrides format: expected dictionary, got {type(data).__name__}\",\n                )\n                return None\n\n            return data\n        except yaml.YAMLError as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"YAML parsing error in overrides.yaml: {e}\",\n            )\n            return None\n        except Exception as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Error loading overrides.yaml: {e}\",\n            )\n            return None\n\n    def _apply_overrides(self, base_data: Dict[str, Any], overrides_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Apply provider overrides to base model data.\n\n        Args:\n            base_data: Base model configuration data\n            overrides_data: Provider overrides data\n\n        Returns:\n            Updated model configuration with overrides applied\n        \"\"\"\n        if \"overrides\" not in overrides_data:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"No 'overrides' key found in overrides.yaml\",\n            )\n            return base_data\n\n        # Get provider from environment variable, default to OpenAI\n        import os\n\n        provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n\n        # Validate provider\n        if provider not in [\"openai\", \"azure\"]:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Invalid provider '{provider}', defaulting to 'openai'\",\n            )\n            provider = \"openai\"\n        provider_overrides = overrides_data[\"overrides\"].get(provider)\n\n        if not provider_overrides:\n            if provider != \"openai\":\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"No overrides found for provider '{provider}', using base OpenAI data\",\n                )\n            # For OpenAI provider, no overrides is expected - return base data\n            return base_data\n\n        if \"models\" not in provider_overrides:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"No 'models' section found in {provider} overrides\",\n            )\n            return base_data\n\n        # Deep copy base data to avoid modifying original\n        import copy\n\n        result_data = copy.deepcopy(base_data)\n\n        # Apply model-specific overrides\n        for model_name, override_config in provider_overrides[\"models\"].items():\n            if model_name in result_data.get(\"models\", {}):\n                self._merge_model_override(result_data[\"models\"][model_name], override_config)\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Applied {provider} overrides to model '{model_name}'\",\n                )\n\n        return result_data\n\n    def _merge_model_override(self, base_model: Dict[str, Any], override_config: Dict[str, Any]) -&gt; None:\n        \"\"\"Merge override configuration into base model configuration.\n\n        Args:\n            base_model: Base model configuration (modified in place)\n            override_config: Override configuration to merge\n        \"\"\"\n        for key, value in override_config.items():\n            if key == \"pricing\" and isinstance(value, dict) and isinstance(base_model.get(\"pricing\"), dict):\n                # Merge pricing information\n                base_model[\"pricing\"].update(value)\n            elif key == \"capabilities\" and isinstance(value, dict):\n                # Replace or merge capabilities\n                if \"capabilities\" not in base_model:\n                    base_model[\"capabilities\"] = {}\n                base_model[\"capabilities\"].update(value)\n            elif key == \"parameters\" and isinstance(value, dict) and isinstance(base_model.get(\"parameters\"), dict):\n                # Merge parameters\n                base_model[\"parameters\"].update(value)\n            else:\n                # For other keys, replace entirely\n                base_model[key] = value\n\n    def _load_constraints(self) -&gt; None:\n        \"\"\"Load parameter constraints from file.\"\"\"\n        try:\n            with open(self.config.constraints_path, \"r\") as f:\n                data = yaml.safe_load(f)\n                if not isinstance(data, dict):\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Constraints file must contain a dictionary\",\n                    )\n                    return\n\n                # Handle nested structure: numeric_constraints and enum_constraints\n                for category_name, category_data in data.items():\n                    if not isinstance(category_data, dict):\n                        log_error(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Constraint category '{category_name}' must be a dictionary\",\n                            category=category_data,\n                        )\n                        continue\n\n                    # Process each constraint in the category\n                    for constraint_name, constraint in category_data.items():\n                        if not isinstance(constraint, dict):\n                            log_error(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Constraint '{constraint_name}' must be a dictionary\",\n                                constraint=constraint,\n                            )\n                            continue\n\n                        constraint_type = constraint.get(\"type\", \"\")\n                        if not constraint_type:\n                            log_error(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Constraint '{constraint_name}' missing required 'type' field\",\n                                constraint=constraint,\n                            )\n                            continue\n\n                        # Create full reference name (e.g., \"numeric_constraints.temperature\")\n                        full_ref = f\"{category_name}.{constraint_name}\"\n\n                        if constraint_type == \"numeric\":\n                            min_value = constraint.get(\"min_value\")\n                            max_value = constraint.get(\"max_value\")\n                            allow_float = constraint.get(\"allow_float\", True)\n                            allow_int = constraint.get(\"allow_int\", True)\n                            description = constraint.get(\"description\", \"\")\n\n                            # Type validation\n                            if min_value is not None and not isinstance(min_value, (int, float)):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-numeric 'min_value' value\",\n                                    min_value=min_value,\n                                )\n                                continue\n\n                            if max_value is not None and not isinstance(max_value, (int, float)):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-numeric 'max_value' value\",\n                                    max_value=max_value,\n                                )\n                                continue\n\n                            if not isinstance(allow_float, bool) or not isinstance(allow_int, bool):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-boolean 'allow_float' or 'allow_int'\",\n                                    allow_float=allow_float,\n                                    allow_int=allow_int,\n                                )\n                                continue\n\n                            # Create constraint\n                            self._constraints[full_ref] = NumericConstraint(\n                                min_value=min_value if min_value is not None else 0.0,\n                                max_value=max_value,\n                                allow_float=allow_float,\n                                allow_int=allow_int,\n                                description=description,\n                            )\n                        elif constraint_type == \"enum\":\n                            allowed_values = constraint.get(\"allowed_values\")\n                            description = constraint.get(\"description\", \"\")\n\n                            # Required field validation\n                            if allowed_values is None:\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' missing required 'allowed_values' field\",\n                                    constraint=constraint,\n                                )\n                                continue\n\n                            # Type validation\n                            if not isinstance(allowed_values, list):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-list 'allowed_values' field\",\n                                    allowed_values=allowed_values,\n                                )\n                                continue\n\n                            # Validate all values are strings\n                            if not all(isinstance(val, str) for val in allowed_values):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-string values in 'allowed_values' list\",\n                                    allowed_values=allowed_values,\n                                )\n                                continue\n\n                            # Create constraint\n                            self._constraints[full_ref] = EnumConstraint(\n                                allowed_values=allowed_values,\n                                description=description,\n                            )\n                        elif constraint_type == \"object\":\n                            # Implementation for object constraint type\n                            description = constraint.get(\"description\", \"\")\n                            required_keys = constraint.get(\"required_keys\", [])\n                            allowed_keys = constraint.get(\"allowed_keys\")\n\n                            # Type validation\n                            if not isinstance(required_keys, list):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-list 'required_keys' field\",\n                                    required_keys=required_keys,\n                                )\n                                continue\n\n                            if allowed_keys is not None and not isinstance(allowed_keys, list):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-list 'allowed_keys' field\",\n                                    allowed_keys=allowed_keys,\n                                )\n                                continue\n\n                            # Create constraint\n                            self._constraints[full_ref] = ObjectConstraint(\n                                description=description,\n                                required_keys=required_keys,\n                                allowed_keys=allowed_keys,\n                            )\n                        else:\n                            log_error(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Unknown constraint type '{constraint_type}' for '{constraint_name}'\",\n                                constraint=constraint,\n                            )\n\n        except FileNotFoundError:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"Parameter constraints file not found\",\n                path=self.config.constraints_path,\n            )\n        except Exception as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to load parameter constraints\",\n                path=self.config.constraints_path,\n                error=str(e),\n            )\n\n    def _load_capabilities(self) -&gt; None:\n        \"\"\"Load model capabilities from config.\"\"\"\n        config_result = self._load_config()\n        # Abort if configuration failed to load\n        if not config_result.success or config_result.data is None:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to load registry configuration\",\n                path=self.config.registry_path,\n                error=getattr(config_result, \"error\", None),\n            )\n            return\n\n        # -------------------------------\n        # Schema version detection\n        # -------------------------------\n        from .schema_version import SchemaVersionValidator\n\n        try:\n            # Get and validate schema version\n            schema_version = SchemaVersionValidator.get_schema_version(config_result.data)\n\n            # Log schema version detection\n            from .logging import log_info\n\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                \"Schema version detected\",\n                version=schema_version,\n                compatible_range=SchemaVersionValidator.get_compatible_range(schema_version),\n                path=config_result.path,\n            )\n\n            # Check compatibility\n            if not SchemaVersionValidator.is_compatible_schema(schema_version):\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Unsupported schema version\",\n                    version=schema_version,\n                    supported_ranges=list(SchemaVersionValidator.SUPPORTED_SCHEMA_VERSIONS.values()),\n                    path=config_result.path,\n                )\n                return\n\n            # Validate structure matches version\n            if not SchemaVersionValidator.validate_schema_structure(config_result.data, schema_version):\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Schema structure validation failed\",\n                    version=schema_version,\n                    path=config_result.path,\n                )\n                return\n\n            # Route to appropriate loader\n            loader_method = SchemaVersionValidator.get_loader_method_name(schema_version)\n            if loader_method and hasattr(self, loader_method):\n                getattr(self, loader_method)(config_result.data.get(\"models\", {}))\n                return\n            else:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"No loader available for schema version\",\n                    version=schema_version,\n                    path=config_result.path,\n                )\n                return\n\n        except ValueError as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Schema version validation failed\",\n                error=str(e),\n                path=config_result.path,\n            )\n            return\n\n    def _load_capabilities_modern(self, models_data: Dict[str, Any]) -&gt; None:\n        \"\"\"Load capabilities from modern schema (1.x) where models are top-level.\n\n        The modern schema (1.0.0+) places every model \u2013 base or dated \u2013 as a\n        direct child of the ``models`` mapping and groups feature flags beneath\n        a ``capabilities`` key. This helper converts the structure into\n        the ``ModelCapabilities`` dataclass so the public API remains\n        unchanged.\n        \"\"\"\n        from datetime import date, datetime\n\n        loaded_count: int = 0\n        skipped_count: int = 0\n        first_error: Optional[str] = None\n\n        for model_name, model_config in models_data.items():\n            try:\n                # -------------------\n                # Context window size\n                # -------------------\n                context_window_raw = model_config.get(\"context_window\", 0)\n                if isinstance(context_window_raw, dict):\n                    context_window = int(context_window_raw.get(\"total\", 0) or 0)\n                    output_tokens_raw = context_window_raw.get(\"output\") or model_config.get(\"max_output_tokens\", 0)\n                    max_output_tokens = int(output_tokens_raw or 0)\n                else:\n                    context_window = int(context_window_raw or 0)\n                    max_output_tokens = int(model_config.get(\"max_output_tokens\", 0) or 0)\n\n                # -------------\n                # Capabilities\n                # -------------\n                caps_block: Dict[str, Any] = model_config.get(\"capabilities\", {})\n\n                supports_vision = bool(\n                    caps_block.get(\n                        \"supports_vision\",\n                        model_config.get(\"supports_vision\", False),\n                    )\n                )\n                supports_functions = bool(\n                    caps_block.get(\n                        \"supports_function_calling\",\n                        model_config.get(\"supports_functions\", False),\n                    )\n                )\n                supports_streaming = bool(\n                    caps_block.get(\n                        \"supports_streaming\",\n                        model_config.get(\"supports_streaming\", False),\n                    )\n                )\n                supports_structured = bool(\n                    caps_block.get(\n                        \"supports_structured_output\",\n                        model_config.get(\"supports_structured\", False),\n                    )\n                )\n                supports_web_search = bool(\n                    caps_block.get(\n                        \"supports_web_search\",\n                        model_config.get(\"supports_web_search\", False),\n                    )\n                )\n                supports_audio = bool(\n                    caps_block.get(\n                        \"supports_audio\",\n                        model_config.get(\"supports_audio\", False),\n                    )\n                )\n                supports_json_mode = bool(\n                    caps_block.get(\n                        \"supports_json_mode\",\n                        model_config.get(\"supports_json_mode\", False),\n                    )\n                )\n\n                # -------------\n                # Deprecation\n                # -------------\n                dep_block: Dict[str, Any] = model_config.get(\"deprecation\", {})\n                dep_status = dep_block.get(\"status\", \"active\")\n\n                def _parse_date(val: Any) -&gt; Optional[date]:\n                    if val in (None, \"\", \"null\"):\n                        return None\n                    try:\n                        return datetime.fromisoformat(str(val)).date()\n                    except Exception:\n                        return None\n\n                deprecates_on = _parse_date(dep_block.get(\"deprecates_on\"))\n                sunsets_on = _parse_date(dep_block.get(\"sunsets_on\")) or _parse_date(dep_block.get(\"sunset_date\"))\n\n                deprecation = DeprecationInfo(\n                    status=dep_status,\n                    deprecates_on=deprecates_on,\n                    sunsets_on=sunsets_on,\n                    replacement=dep_block.get(\"replacement\"),\n                    migration_guide=dep_block.get(\"migration_guide\"),\n                    reason=dep_block.get(\"reason\", dep_status),\n                )\n\n                # -------------\n                # Min version\n                # -------------\n                min_version_data = model_config.get(\"min_version\")\n                min_version: Optional[ModelVersion] = None\n                if min_version_data:\n                    try:\n                        if isinstance(min_version_data, dict):\n                            year = min_version_data.get(\"year\")\n                            month = min_version_data.get(\"month\")\n                            day = min_version_data.get(\"day\")\n                            if year and month and day:\n                                min_version = ModelVersion(year=year, month=month, day=day)\n                        else:\n                            min_version = ModelVersion.from_string(str(min_version_data))\n                    except (ValueError, TypeError):\n                        # Ignore bad min_version values\n                        min_version = None\n\n                # ----------------------\n                # Supported parameters\n                # ----------------------\n                param_refs: List[ParameterReference] = []\n\n                # Extract parameters from parameters block\n                parameters_block = model_config.get(\"parameters\", {})\n                if parameters_block and isinstance(parameters_block, dict):\n                    for param_name, param_config in parameters_block.items():\n                        if isinstance(param_config, dict):\n                            # Create parameter reference\n                            param_refs.append(\n                                ParameterReference(\n                                    ref=param_name,\n                                    description=f\"Parameter {param_name}\",\n                                )\n                            )\n                    # If we collected inline parameters but there were no explicit supported_parameters,\n                    # use the inline list as supported parameters to allow validation.\n                    if not param_refs:\n                        for param_name in parameters_block.keys():\n                            param_refs.append(ParameterReference(ref=param_name, description=f\"Parameter {param_name}\"))\n\n                # Note: legacy 'supported_parameters' is intentionally not supported.\n\n                # -------------\n                # Pricing block\n                # -------------\n                pricing_block = model_config.get(\"pricing\")\n                pricing_obj: Optional[PricingInfo] = None\n                if isinstance(pricing_block, dict):\n                    try:\n                        # Support both unified pricing (scheme/unit) and legacy per-million-token keys\n                        if \"scheme\" in pricing_block and \"unit\" in pricing_block:\n                            pricing_obj = PricingInfo(\n                                scheme=typing.cast(\n                                    typing.Literal[\n                                        \"per_token\",\n                                        \"per_minute\",\n                                        \"per_image\",\n                                        \"per_request\",\n                                    ],\n                                    str(pricing_block.get(\"scheme\")),\n                                ),\n                                unit=typing.cast(\n                                    typing.Literal[\n                                        \"million_tokens\",\n                                        \"minute\",\n                                        \"image\",\n                                        \"request\",\n                                    ],\n                                    str(pricing_block.get(\"unit\")),\n                                ),\n                                input_cost_per_unit=float(pricing_block.get(\"input_cost_per_unit\", 0.0)),\n                                output_cost_per_unit=float(pricing_block.get(\"output_cost_per_unit\", 0.0)),\n                                currency=str(pricing_block.get(\"currency\", \"USD\")),\n                                tiers=typing.cast(\n                                    typing.Optional[typing.List[typing.Dict[str, typing.Any]]],\n                                    pricing_block.get(\"tiers\"),\n                                ),\n                            )\n                        else:\n                            # Legacy support (pre-unified): interpret as per_token/million_tokens\n                            pricing_obj = PricingInfo(\n                                scheme=\"per_token\",\n                                unit=\"million_tokens\",\n                                input_cost_per_unit=float(pricing_block.get(\"input_cost_per_million_tokens\", 0.0)),\n                                output_cost_per_unit=float(pricing_block.get(\"output_cost_per_million_tokens\", 0.0)),\n                                currency=str(pricing_block.get(\"currency\", \"USD\")),\n                            )\n                    except Exception as e:  # pragma: no cover\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            \"Invalid pricing block ignored\",\n                            model=model_name,\n                            error=str(e),\n                        )\n\n                # -------------\n                # Web search billing block (optional)\n                # -------------\n                web_search_billing: Optional[WebSearchBilling] = None\n                billing_block = model_config.get(\"billing\")\n                if isinstance(billing_block, dict):\n                    ws = billing_block.get(\"web_search\")\n                    if isinstance(ws, dict):\n                        try:\n                            policy = str(ws.get(\"content_token_policy\", \"\")).strip()\n                            if policy in {\"included_in_call_fee\", \"billed_at_model_rate\"}:\n                                web_search_billing = WebSearchBilling(\n                                    call_fee_per_1000=float(ws.get(\"call_fee_per_1000\", 0.0)),\n                                    content_token_policy=policy,  # type: ignore[arg-type]\n                                    currency=str(ws.get(\"currency\", \"USD\")),\n                                    notes=str(ws.get(\"notes\")) if \"notes\" in ws else None,\n                                )\n                        except Exception:\n                            web_search_billing = None\n\n                # -------------\n                # Build object\n                # -------------\n                capabilities = ModelCapabilities(\n                    model_name=model_name,\n                    openai_model_name=model_config.get(\"openai_name\", model_name),\n                    context_window=context_window,\n                    max_output_tokens=max_output_tokens,\n                    deprecation=deprecation,\n                    supports_vision=supports_vision,\n                    supports_functions=supports_functions,\n                    supports_streaming=supports_streaming,\n                    supports_structured=supports_structured,\n                    supports_web_search=supports_web_search,\n                    supports_audio=supports_audio,\n                    supports_json_mode=supports_json_mode,\n                    pricing=pricing_obj,\n                    input_modalities=model_config.get(\"input_modalities\"),\n                    output_modalities=model_config.get(\"output_modalities\"),\n                    min_version=min_version,\n                    aliases=[],\n                    supported_parameters=param_refs,\n                    constraints=copy.deepcopy(self._constraints),\n                    inline_parameters=parameters_block,\n                    web_search_billing=web_search_billing,\n                )\n\n                with self._capabilities_lock:\n                    self._capabilities[model_name] = capabilities\n                loaded_count += 1\n            except Exception as e:  # pragma: no cover \u2013 best-effort parsing\n                if first_error is None:\n                    first_error = f\"{type(e).__name__}: {e}\"\n                skipped_count += 1\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Failed to load model capabilities\",\n                    model=model_name,\n                    error=str(e),\n                )\n\n        # Bookkeep and log summary for observability\n        try:\n            self._last_load_stats = {\n                \"total\": len(models_data),\n                \"loaded\": loaded_count,\n                \"skipped\": skipped_count,\n                \"first_error\": first_error,\n            }\n        except Exception:\n            pass\n\n        log_info(\n            LogEvent.MODEL_REGISTRY,\n            \"Model load summary\",\n            total=len(models_data),\n            loaded=loaded_count,\n            skipped=skipped_count,\n        )\n\n    def _get_capabilities_impl(self, model: str) -&gt; ModelCapabilities:\n        \"\"\"Implementation of get_capabilities without caching.\n\n        Args:\n            model: Model name, which can be:\n                  - Dated model (e.g. \"gpt-4o-2024-08-06\")\n                  - Alias (e.g. \"gpt-4o\")\n                  - Versioned model (e.g. \"gpt-4o-2024-09-01\")\n\n        Returns:\n            ModelCapabilities for the requested model\n\n        Raises:\n            ModelNotSupportedError: If the model is not supported\n            InvalidDateError: If the date components are invalid\n            VersionTooOldError: If the version is older than minimum supported\n        \"\"\"\n        # First check for exact match (dated model or alias)\n        with self._capabilities_lock:\n            if model in self._capabilities:\n                return self._capabilities[model]\n\n        # Check if this is a versioned model\n        version_match = self._DATE_PATTERN.match(model)\n        if version_match:\n            base_name = version_match.group(1)\n            version_str = version_match.group(2)\n\n            # Find all capabilities for this base model\n            with self._capabilities_lock:\n                model_versions = [(k, v) for k, v in self._capabilities.items() if k.startswith(f\"{base_name}-\")]\n\n            if not model_versions:\n                # No versions found for this base model\n                # Find aliases that might provide a valid alternative\n                with self._capabilities_lock:\n                    aliases = [\n                        name for name in self._capabilities.keys() if not self._IS_DATED_MODEL_PATTERN.match(name)\n                    ]\n\n                # Find if any alias might match the base model\n                matching_aliases = [alias for alias in aliases if alias == base_name]\n\n                if matching_aliases:\n                    raise ModelNotSupportedError(\n                        f\"Model '{model}' not found.\",\n                        model=model,\n                        available_models=matching_aliases,\n                    )\n                else:\n                    # No matching aliases either\n                    with self._capabilities_lock:\n                        available_base_models: set[str] = set(\n                            k for k in self._capabilities.keys() if not self._IS_DATED_MODEL_PATTERN.match(k)\n                        )\n                    raise ModelNotSupportedError(\n                        f\"Model '{model}' not found. Available base models: {', '.join(sorted(available_base_models))}\",\n                        model=model,\n                        available_models=list(available_base_models),\n                    )\n\n            try:\n                # Parse version\n                requested_version = ModelVersion.from_string(version_str)\n            except ValueError as e:\n                raise InvalidDateError(str(e))\n\n            # Find the model with the minimum version\n            for _unused, caps in model_versions:\n                if caps.min_version and requested_version &lt; caps.min_version:\n                    raise VersionTooOldError(\n                        f\"Model version '{model}' is older than the minimum supported \"\n                        f\"version {caps.min_version} for {base_name}.\",\n                        model=model,\n                        min_version=str(caps.min_version),\n                        alias=None,\n                    )\n\n            # Find the best matching model\n            base_model_caps = None\n            for _dated_model, caps in model_versions:\n                if base_model_caps is None or (\n                    caps.min_version\n                    and caps.min_version &lt;= requested_version\n                    and (not base_model_caps.min_version or caps.min_version &gt; base_model_caps.min_version)\n                ):\n                    base_model_caps = caps\n\n            if base_model_caps:\n                # Create a copy with the requested model name\n                new_caps = ModelCapabilities(\n                    model_name=base_model_caps.model_name,\n                    openai_model_name=model,\n                    context_window=base_model_caps.context_window,\n                    max_output_tokens=base_model_caps.max_output_tokens,\n                    deprecation=base_model_caps.deprecation,\n                    supports_vision=base_model_caps.supports_vision,\n                    supports_functions=base_model_caps.supports_functions,\n                    supports_streaming=base_model_caps.supports_streaming,\n                    supports_structured=base_model_caps.supports_structured,\n                    supports_web_search=base_model_caps.supports_web_search,\n                    supports_audio=base_model_caps.supports_audio,\n                    supports_json_mode=base_model_caps.supports_json_mode,\n                    pricing=base_model_caps.pricing,\n                    input_modalities=base_model_caps.input_modalities,\n                    output_modalities=base_model_caps.output_modalities,\n                    min_version=base_model_caps.min_version,\n                    aliases=base_model_caps.aliases,\n                    supported_parameters=base_model_caps.supported_parameters,\n                    constraints=base_model_caps._constraints,\n                )\n                return new_caps\n\n        # If we get here, the model is not supported\n        with self._capabilities_lock:\n            available_models: set[str] = set(\n                k for k in self._capabilities.keys() if not self._IS_DATED_MODEL_PATTERN.match(k)\n            )\n        raise ModelNotSupportedError(\n            f\"Model '{model}' not found. Available base models: {', '.join(sorted(available_models))}\",\n            model=model,\n            available_models=list(available_models),\n        )\n\n    def get_parameter_constraint(self, ref: str) -&gt; Union[NumericConstraint, EnumConstraint, ObjectConstraint]:\n        \"\"\"Get a parameter constraint by reference.\n\n        Args:\n            ref: Reference string (e.g., \"numeric_constraints.temperature\")\n\n        Returns:\n            The constraint object (NumericConstraint or EnumConstraint or ObjectConstraint)\n\n        Raises:\n            ConstraintNotFoundError: If the constraint is not found\n        \"\"\"\n        if ref not in self._constraints:\n            raise ConstraintNotFoundError(\n                f\"Constraint reference '{ref}' not found in registry\",\n                ref=ref,\n            )\n        return self._constraints[ref]\n\n    def assert_model_active(self, model: str) -&gt; None:\n        \"\"\"Assert that a model is active and warn if deprecated.\n\n        Args:\n            model: Model name to check\n\n        Raises:\n            ModelSunsetError: If the model is sunset\n            ModelNotSupportedError: If the model is not found\n\n        Warns:\n            DeprecationWarning: If the model is deprecated\n        \"\"\"\n        capabilities = self.get_capabilities(model)\n        assert_model_active(model, capabilities.deprecation)\n\n    def get_sunset_headers(self, model: str) -&gt; dict[str, str]:\n        \"\"\"Get RFC-compliant HTTP headers for model deprecation status.\n\n        Args:\n            model: Model name\n\n        Returns:\n            Dictionary of HTTP headers\n\n        Raises:\n            ModelNotSupportedError: If the model is not found\n        \"\"\"\n        capabilities = self.get_capabilities(model)\n        return sunset_headers(capabilities.deprecation)\n\n    def _get_conditional_headers(self, force: bool = False) -&gt; Dict[str, str]:\n        \"\"\"Get conditional headers for HTTP requests.\n\n        Args:\n            force: If True, bypass conditional headers\n\n        Returns:\n            Dictionary of HTTP headers\n        \"\"\"\n        if force:\n            return {}\n\n        headers = {}\n        meta_path = self._get_metadata_path()\n        if meta_path and os.path.exists(meta_path):\n            try:\n                with open(meta_path, \"r\") as f:\n                    metadata = yaml.safe_load(f)\n                    if metadata and isinstance(metadata, dict):\n                        if \"etag\" in metadata:\n                            headers[\"If-None-Match\"] = metadata[\"etag\"]\n                        if \"last_modified\" in metadata:\n                            headers[\"If-Modified-Since\"] = metadata[\"last_modified\"]\n            except Exception as e:\n                log_debug(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Could not load cache metadata, skipping conditional headers\",\n                    error=str(e),\n                )\n        return headers\n\n    def _get_metadata_path(self) -&gt; Optional[str]:\n        \"\"\"Get the path to the cache metadata file.\n\n        Returns:\n            Optional[str]: Path to the metadata file, or None if config_path is not set\n        \"\"\"\n        if not self.config.registry_path:\n            return None\n        return f\"{self.config.registry_path}.meta\"\n\n    def _save_cache_metadata(self, metadata: Dict[str, str]) -&gt; None:\n        \"\"\"Save cache metadata to file.\n\n        Args:\n            metadata: Dictionary of metadata to save\n        \"\"\"\n        meta_path = self._get_metadata_path()\n        if not meta_path:\n            return\n\n        try:\n            with open(meta_path, \"w\") as f:\n                yaml.safe_dump(metadata, f)\n        except Exception as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"Could not save cache metadata\",\n                error=str(e),\n                path=str(meta_path),  # Convert to string in case meta_path is None\n            )\n\n    def _fetch_remote_config(self, url: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Fetch the remote configuration from the specified URL.\n\n        Args:\n            url: URL to fetch the configuration from\n\n        Returns:\n            Parsed configuration dictionary or None if fetch failed\n        \"\"\"\n        try:\n            import requests\n        except ImportError:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Could not import requests module\",\n            )\n            return None\n\n        try:\n            # Add a timeout of 10 seconds to prevent indefinite hanging\n            response = requests.get(url, timeout=10)\n            try:\n                if response.status_code != 200:\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"HTTP error {response.status_code}\",\n                        url=url,\n                    )\n                    return None\n\n                # Parse the YAML content\n                config = yaml.safe_load(response.text)\n                if not isinstance(config, dict):\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Remote config is not a dictionary\",\n                        url=url,\n                    )\n                    return None\n\n                return config\n            finally:\n                # Ensure response is closed to prevent resource leaks\n                response.close()\n        except (requests.RequestException, yaml.YAMLError) as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to fetch or parse remote config: {str(e)}\",\n                url=url,\n            )\n            return None\n\n    def _validate_remote_config(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Validate the remote configuration before applying it.\n\n        Args:\n            config: Configuration dictionary to validate\n\n        Raises:\n            ValueError: If the configuration is invalid\n        \"\"\"\n        # Check version\n        if \"version\" not in config:\n            raise ValueError(\"Remote configuration missing version field\")\n\n        # Check required sections for both schema versions\n        if \"models\" in config:\n            # New schema \u2013 nothing else to validate here for presence\n            pass\n        else:\n            raise ValueError(\"Remote configuration missing 'models' section\")\n\n    def refresh_from_remote(\n        self,\n        url: Optional[str] = None,\n        force: bool = False,\n        validate_only: bool = False,\n    ) -&gt; RefreshResult:\n        \"\"\"Refresh the registry configuration from remote source.\n\n        Args:\n            url: Optional custom URL to fetch registry from\n            force: Force refresh even if version is current\n            validate_only: Only validate remote config without updating\n\n        Returns:\n            Result of the refresh operation\n        \"\"\"\n        try:\n            # Get remote config\n            config_url = url or (\n                \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n            )\n            remote_config = self._fetch_remote_config(config_url)\n            if not remote_config:\n                raise ValueError(\"Failed to fetch remote configuration\")\n\n            # Validate the remote config\n            self._validate_remote_config(remote_config)\n\n            if validate_only:\n                # Only validation was requested\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.VALIDATED,\n                    message=\"Remote registry configuration validated successfully\",\n                )\n\n            # Check for updates only if not forcing and not validating\n            if not force:\n                result = self.check_for_updates(url=url)\n                if result.status == RefreshStatus.ALREADY_CURRENT:\n                    return RefreshResult(\n                        success=True,\n                        status=RefreshStatus.ALREADY_CURRENT,\n                        message=\"Registry is already up to date\",\n                    )\n\n            # Use DataManager to handle the update\n            try:\n                # Force update through DataManager\n                if self._data_manager.force_update():\n                    log_info(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Successfully updated registry data via DataManager\",\n                    )\n                else:\n                    # Fallback to manual update if DataManager fails\n                    # Note: This fallback downloads models.yaml and attempts overrides.yaml\n                    log_warning(\n                        LogEvent.MODEL_REGISTRY,\n                        \"DataManager update failed, using limited fallback (models.yaml only)\",\n                    )\n                    target_path = get_user_data_dir() / \"models.yaml\"\n                    with open(target_path, \"w\") as f:\n                        yaml.safe_dump(remote_config, f)\n\n                    # Try to download overrides.yaml if possible\n                    try:\n                        overrides_url = \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/overrides.yaml\"\n\n                        # Simple fallback downloads\n                        try:\n                            import requests\n                        except ImportError:\n                            requests = None  # type: ignore\n\n                        if requests is not None:\n                            try:\n                                # Download overrides.yaml\n                                overrides_resp = requests.get(overrides_url, timeout=30)\n                                if overrides_resp.status_code == 200:\n                                    overrides_content = overrides_resp.text\n                                    overrides_path = get_user_data_dir() / \"overrides.yaml\"\n                                    with open(overrides_path, \"w\") as f:\n                                        f.write(overrides_content)\n                                    log_info(\n                                        LogEvent.MODEL_REGISTRY,\n                                        \"Downloaded overrides.yaml in fallback\",\n                                    )\n\n                            except requests.RequestException as e:\n                                log_warning(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Failed to download additional files in fallback: {e}\",\n                                )\n                    except Exception as e:\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Error in fallback additional file download: {e}\",\n                        )\n            except PermissionError as e:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Permission denied when writing registry configuration\",\n                    path=str(target_path),\n                    error=str(e),\n                )\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"Permission denied when writing to {target_path}\",\n                )\n            except OSError as e:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"File system error when writing registry configuration\",\n                    path=str(target_path),\n                    error=str(e),\n                )\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"Error writing to {target_path}: {str(e)}\",\n                )\n\n            # Reload the registry with new configuration\n            self._load_constraints()\n            self._load_capabilities()\n\n            # Verify that the reload was successful\n            if not self._capabilities:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Failed to reload registry after update\",\n                    path=str(target_path),\n                )\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=\"Registry update failed: could not load capabilities after update\",\n                )\n\n            # Log success\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                \"Registry updated from remote\",\n                version=remote_config.get(\"version\", \"unknown\"),\n            )\n\n            return RefreshResult(\n                success=True,\n                status=RefreshStatus.UPDATED,\n                message=\"Registry updated successfully\",\n            )\n\n        except Exception as e:\n            error_msg = f\"Error refreshing registry: {str(e)}\"\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                error_msg,\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=error_msg,\n            )\n\n    def check_for_updates(self, url: Optional[str] = None) -&gt; RefreshResult:\n        \"\"\"Check if updates are available for the model registry.\n\n        Args:\n            url: Optional custom URL to check for updates\n\n        Returns:\n            Result of the update check\n        \"\"\"\n        try:\n            import requests\n        except ImportError:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=\"Could not import requests module\",\n            )\n\n        # Set up the URL with fallback handling\n        primary_url = url or (\n            \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n        )\n\n        # Define fallback URLs in case primary fails\n        fallback_urls = [\n            \"https://github.com/yaniv-golan/openai-model-registry/raw/main/data/models.yaml\",\n        ]\n\n        urls_to_try = [primary_url] + fallback_urls\n\n        try:\n            # Use a lock when checking and comparing versions to prevent race conditions\n            with self.__class__._instance_lock:\n                # First check with DataManager\n                if self._data_manager.should_update_data():\n                    latest_release = self._data_manager._fetch_latest_data_release()\n                    if latest_release:\n                        latest_version = latest_release.get(\"tag_name\", \"\")\n                        current_version = self._data_manager._get_current_version()\n                        if (\n                            current_version\n                            and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0\n                        ):\n                            return RefreshResult(\n                                success=True,\n                                status=RefreshStatus.ALREADY_CURRENT,\n                                message=f\"Registry is up to date (version {current_version})\",\n                            )\n                        else:\n                            return RefreshResult(\n                                success=True,\n                                status=RefreshStatus.UPDATE_AVAILABLE,\n                                message=f\"Update available: {current_version or 'bundled'} -&gt; {latest_version}\",\n                            )\n\n                # Fallback to original HTTP check with URL fallback\n                remote_config = None\n                for config_url in urls_to_try:\n                    try:\n                        response = requests.get(config_url, timeout=10)\n                        response.raise_for_status()\n\n                        # Parse the remote config\n                        remote_config = yaml.safe_load(response.text)\n                        if isinstance(remote_config, dict):\n                            break\n                        else:\n                            log_warning(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Remote config from {config_url} is not a valid dictionary\",\n                            )\n                    except (requests.RequestException, yaml.YAMLError) as e:\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Failed to fetch from {config_url}: {e}\",\n                        )\n                        continue\n\n                if remote_config is None:\n                    return RefreshResult(\n                        success=False,\n                        status=RefreshStatus.ERROR,\n                        message=\"Could not fetch remote config from any URL\",\n                    )\n\n                # Get local config for comparison\n                local_config = self._load_config()\n                if not local_config.success:\n                    return RefreshResult(\n                        success=False,\n                        status=RefreshStatus.ERROR,\n                        message=f\"Could not load local config: {local_config.error}\",\n                    )\n\n                # Compare versions (simplified comparison)\n                remote_version = remote_config.get(\"version\", \"unknown\")\n                local_version = local_config.data.get(\"version\", \"unknown\") if local_config.data else \"unknown\"\n\n                if remote_version == local_version:\n                    return RefreshResult(\n                        success=True,\n                        status=RefreshStatus.ALREADY_CURRENT,\n                        message=f\"Registry is up to date (version {local_version})\",\n                    )\n                else:\n                    return RefreshResult(\n                        success=True,\n                        status=RefreshStatus.UPDATE_AVAILABLE,\n                        message=f\"Update available: {local_version} -&gt; {remote_version}\",\n                    )\n\n        except requests.HTTPError as e:\n            if e.response.status_code == 404:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=\"Registry not found at any of the configured URLs\",\n                )\n            else:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"HTTP error {e.response.status_code}: {e}\",\n                )\n        except requests.RequestException as e:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Network error: {e}\",\n            )\n        except Exception as e:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Unexpected error: {e}\",\n            )\n\n    def check_data_updates(self) -&gt; bool:\n        \"\"\"Check if data updates are available using DataManager.\n\n        Returns:\n            True if updates are available, False otherwise\n        \"\"\"\n        try:\n            if not self._data_manager.should_update_data():\n                return False\n\n            latest_release = self._data_manager._fetch_latest_data_release()\n            if not latest_release:\n                return False\n\n            latest_version = latest_release.get(\"tag_name\", \"\")\n            current_version = self._data_manager._get_current_version()\n\n            return not (current_version and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0)\n        except Exception:\n            return False\n\n    def get_update_info(self) -&gt; UpdateInfo:\n        \"\"\"Get detailed information about available updates.\n\n        Returns:\n            UpdateInfo object containing update details\n        \"\"\"\n        try:\n            if not self._data_manager.should_update_data():\n                return UpdateInfo(\n                    update_available=False,\n                    current_version=self._data_manager._get_current_version(),\n                    current_version_date=None,\n                    latest_version=None,\n                    latest_version_date=None,\n                    download_url=None,\n                    update_size_estimate=None,\n                    latest_version_description=None,\n                    accumulated_changes=[],\n                    error_message=\"Updates are disabled via environment variable\",\n                )\n\n            latest_release = self._data_manager._fetch_latest_data_release()\n            if not latest_release:\n                return UpdateInfo(\n                    update_available=False,\n                    current_version=self._data_manager._get_current_version(),\n                    current_version_date=None,\n                    latest_version=None,\n                    latest_version_date=None,\n                    download_url=None,\n                    update_size_estimate=None,\n                    latest_version_description=None,\n                    accumulated_changes=[],\n                    error_message=\"No releases found on GitHub\",\n                )\n\n            latest_version_raw = latest_release.get(\"tag_name\", \"\")\n            current_version = self._data_manager._get_current_version()\n\n            # Clean the latest version to match current version format (remove data-v prefix)\n            latest_version = latest_version_raw\n            if latest_version_raw.startswith(\"data-v\"):\n                latest_version = latest_version_raw[len(\"data-v\") :]\n\n            # Get current version info with date\n            current_version_info = self._data_manager._get_current_version_info()\n            current_version_date = current_version_info.get(\"published_at\") if current_version_info else None\n\n            update_available = not (\n                current_version and self._data_manager._compare_versions(latest_version_raw, current_version) &lt;= 0\n            )\n\n            # Get accumulated changes between current and latest version\n            accumulated_changes = []\n            if update_available:\n                accumulated_changes = self._data_manager.get_accumulated_changes(current_version, latest_version_raw)\n\n            # Estimate update size based on assets\n            update_size_estimate = None\n            total_size = 0\n            assets = latest_release.get(\"assets\", [])\n            for asset in assets:\n                if asset.get(\"name\") in [\"models.yaml\", \"overrides.yaml\"]:\n                    total_size += asset.get(\"size\", 0)\n\n            if total_size &gt; 0:\n                if total_size &lt; 1024:\n                    update_size_estimate = f\"{total_size} bytes\"\n                elif total_size &lt; 1024 * 1024:\n                    update_size_estimate = f\"{total_size / 1024:.1f} KB\"\n                else:\n                    update_size_estimate = f\"{total_size / (1024 * 1024):.1f} MB\"\n\n            # Extract one-sentence description from latest release body\n            latest_version_description = None\n            if latest_release.get(\"body\"):\n                latest_version_description = self._data_manager._extract_change_summary(latest_release.get(\"body\", \"\"))\n\n            return UpdateInfo(\n                update_available=update_available,\n                current_version=current_version,\n                current_version_date=current_version_date,\n                latest_version=latest_version,\n                latest_version_date=latest_release.get(\"published_at\"),\n                download_url=latest_release.get(\"html_url\"),\n                update_size_estimate=update_size_estimate,\n                latest_version_description=latest_version_description,\n                accumulated_changes=accumulated_changes,\n                error_message=None,\n            )\n\n        except Exception as e:\n            return UpdateInfo(\n                update_available=False,\n                current_version=self._data_manager._get_current_version(),\n                current_version_date=None,\n                latest_version=None,\n                latest_version_date=None,\n                download_url=None,\n                update_size_estimate=None,\n                latest_version_description=None,\n                accumulated_changes=[],\n                error_message=str(e),\n            )\n\n    def update_data(self, force: bool = False) -&gt; bool:\n        \"\"\"Update model registry data using DataManager.\n\n        Args:\n            force: If True, force update regardless of current version\n\n        Returns:\n            True if update was successful, False otherwise\n        \"\"\"\n        try:\n            if force:\n                success = self._data_manager.force_update()\n            else:\n                success = self._data_manager.check_for_updates()\n\n            if success:\n                # Reload capabilities after successful update\n                self._load_capabilities()\n\n            return success\n        except Exception:\n            return False\n\n    def manual_update_workflow(self, prompt_user_func: Optional[Callable[[UpdateInfo], bool]] = None) -&gt; bool:\n        \"\"\"Manual update workflow with user approval.\n\n        Args:\n            prompt_user_func: Optional function to prompt user for approval.\n                            Should take UpdateInfo as parameter and return bool.\n                            If None, uses a default console prompt.\n\n        Returns:\n            True if update was performed, False otherwise\n        \"\"\"\n        try:\n            # Get update information\n            update_info = self.get_update_info()\n\n            if update_info.error_message:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to check for updates: {update_info.error_message}\",\n                )\n                return False\n\n            if not update_info.update_available:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Registry is up to date (version {update_info.current_version or 'bundled'})\",\n                )\n                return False\n\n            # Use custom prompt function or default\n            if prompt_user_func is None:\n                prompt_user_func = self._default_update_prompt\n\n            # Ask user for approval\n            if prompt_user_func(update_info):\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"User approved update from {update_info.current_version or 'bundled'} to {update_info.latest_version}\",\n                )\n\n                # Perform the update\n                success = self.update_data()\n\n                if success:\n                    log_info(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"Successfully updated to {update_info.latest_version}\",\n                    )\n                else:\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Update failed\",\n                    )\n\n                return success\n            else:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"User declined update\",\n                )\n                return False\n\n        except Exception as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                f\"Manual update workflow failed: {e}\",\n            )\n            return False\n\n    def _default_update_prompt(self, update_info: UpdateInfo) -&gt; bool:\n        \"\"\"Default console prompt for update approval.\n\n        Args:\n            update_info: Information about the available update\n\n        Returns:\n            True if user approves, False otherwise\n        \"\"\"\n        print(\"\\n\ud83d\udd04 OpenAI Model Registry Update Available\")\n        print(f\"   Current version: {update_info.current_version or 'bundled'}\")\n        print(f\"   Latest version:  {update_info.latest_version}\")\n\n        if update_info.current_version_date:\n            print(f\"   Current date:    {update_info.current_version_date}\")\n        if update_info.latest_version_date:\n            print(f\"   Latest date:     {update_info.latest_version_date}\")\n        if update_info.update_size_estimate:\n            print(f\"   Download size:   {update_info.update_size_estimate}\")\n        if update_info.latest_version_description:\n            print(f\"   Description:     {update_info.latest_version_description}\")\n\n        # Show accumulated changes\n        if update_info.accumulated_changes:\n            print(\"\\n\ud83d\udcdd Changes since your last update:\")\n            for change in update_info.accumulated_changes:\n                print(f\"   \u2022 {change['version']} ({change['date'][:10] if change['date'] else 'Unknown date'})\")\n                print(f\"     {change['description']}\")\n\n        print(f\"\\n\ud83d\udd17 Release info: {update_info.download_url}\")\n\n        try:\n            response = input(\"\\nWould you like to update now? [y/N]: \").strip().lower()\n            return response in (\"y\", \"yes\")\n        except (KeyboardInterrupt, EOFError):\n            return False\n\n    def get_data_version(self) -&gt; Optional[str]:\n        \"\"\"Get the current data version.\n\n        Returns:\n            Current data version string, or None if using bundled data\n        \"\"\"\n        try:\n            return self._data_manager._get_current_version()\n        except Exception:\n            return None\n\n    def get_data_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get information about data configuration and status.\n\n        Returns:\n            Dictionary containing data configuration information\n        \"\"\"\n        try:\n            info: Dict[str, Any] = {\n                \"data_directory\": str(self._data_manager._data_dir),\n                \"current_version\": self._data_manager._get_current_version(),\n                \"updates_enabled\": self._data_manager.should_update_data(),\n                \"environment_variables\": {\n                    \"OMR_DISABLE_DATA_UPDATES\": os.getenv(\"OMR_DISABLE_DATA_UPDATES\"),\n                    \"OMR_DATA_VERSION_PIN\": os.getenv(\"OMR_DATA_VERSION_PIN\"),\n                    \"OMR_DATA_DIR\": os.getenv(\"OMR_DATA_DIR\"),\n                },\n                \"data_files\": {},\n            }\n\n            # Check data file status\n            for filename in [\"models.yaml\", \"overrides.yaml\"]:\n                file_path = self._data_manager.get_data_file_path(filename)\n                info[\"data_files\"][filename] = {\n                    \"path\": str(file_path) if file_path else None,\n                    \"exists\": file_path is not None,\n                    \"using_bundled\": file_path is None,\n                }\n\n            return info\n        except Exception as e:\n            return {\"error\": str(e)}\n\n    @staticmethod\n    def cleanup() -&gt; None:\n        \"\"\"Clean up the registry instance.\"\"\"\n        with ModelRegistry._instance_lock:\n            ModelRegistry._default_instance = None\n\n    def list_providers(self) -&gt; List[str]:\n        \"\"\"List all providers available in the overrides configuration.\n\n        Returns:\n            List of provider names found in overrides data\n        \"\"\"\n        import os\n\n        providers = set()\n\n        # Add the current provider\n        current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n        providers.add(current_provider)\n\n        # Add providers from overrides\n        if hasattr(self, \"_overrides\") and self._overrides:\n            overrides_data = self._overrides.get(\"overrides\", {})\n            for provider_name in overrides_data.keys():\n                providers.add(provider_name.lower())\n\n        return sorted(list(providers))\n\n    def dump_effective(self) -&gt; Dict[str, Any]:\n        \"\"\"Return the fully merged provider-adjusted dataset for the current provider.\n\n        Returns:\n            Dictionary containing the effective model capabilities after provider overrides\n        \"\"\"\n        import os\n        from datetime import datetime\n\n        current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n        effective_data: Dict[str, Any] = {}\n        total_models = 0\n        serialized = 0\n        skipped_for_dump = 0\n\n        for model_name in self.models:\n            total_models += 1\n            try:\n                capabilities = self.get_capabilities(model_name)\n                effective_data[model_name] = {\n                    \"context_window\": {\n                        \"total\": capabilities.context_window,\n                        \"input\": getattr(capabilities, \"input_context_window\", None),\n                        \"output\": capabilities.max_output_tokens,\n                    },\n                    \"pricing\": (\n                        {\n                            \"scheme\": getattr(capabilities.pricing, \"scheme\", \"per_token\"),\n                            \"unit\": getattr(capabilities.pricing, \"unit\", \"million_tokens\"),\n                            \"input_cost_per_unit\": getattr(capabilities.pricing, \"input_cost_per_unit\", 0.0),\n                            \"output_cost_per_unit\": getattr(capabilities.pricing, \"output_cost_per_unit\", 0.0),\n                            \"currency\": getattr(capabilities.pricing, \"currency\", \"USD\"),\n                            \"tiers\": getattr(capabilities.pricing, \"tiers\", None),\n                        }\n                        if getattr(capabilities, \"pricing\", None)\n                        else {\n                            \"scheme\": \"per_token\",\n                            \"unit\": \"million_tokens\",\n                            \"input_cost_per_unit\": 0.0,\n                            \"output_cost_per_unit\": 0.0,\n                            \"currency\": \"USD\",\n                            \"tiers\": None,\n                        }\n                    ),\n                    \"supports_vision\": capabilities.supports_vision,\n                    \"supports_function_calling\": getattr(capabilities, \"supports_functions\", False),\n                    \"supports_streaming\": capabilities.supports_streaming,\n                    \"supports_structured_output\": getattr(capabilities, \"supports_structured\", False),\n                    \"supports_json_mode\": getattr(capabilities, \"supports_json_mode\", False),\n                    \"supports_web_search\": getattr(capabilities, \"supports_web_search\", False),\n                    \"supports_audio\": getattr(capabilities, \"supports_audio\", False),\n                    \"billing\": (\n                        {\"web_search\": asdict(cast(WebSearchBilling, capabilities.web_search_billing))}\n                        if getattr(capabilities, \"web_search_billing\", None)\n                        else None\n                    ),\n                    \"provider\": current_provider,\n                    \"parameters\": getattr(capabilities, \"parameters\", {}),\n                    \"input_modalities\": getattr(\n                        capabilities, \"input_modalities\", getattr(capabilities, \"modalities\", [])\n                    ),\n                    \"output_modalities\": getattr(capabilities, \"output_modalities\", []),\n                    \"deprecation\": {\n                        \"status\": capabilities.deprecation.status,\n                        \"deprecates_on\": getattr(capabilities.deprecation, \"deprecates_on\", None),\n                        \"sunsets_on\": getattr(capabilities.deprecation, \"sunsets_on\", None),\n                        \"replacement\": getattr(capabilities.deprecation, \"replacement\", None),\n                        \"reason\": getattr(capabilities.deprecation, \"reason\", None),\n                        \"migration_guide\": getattr(capabilities.deprecation, \"migration_guide\", None),\n                    },\n                }\n                serialized += 1\n            except Exception as e:\n                skipped_for_dump += 1\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Failed to serialize model for dump_effective\",\n                    model=model_name,\n                    error=str(e),\n                )\n\n        return {\n            \"provider\": current_provider,\n            \"models\": effective_data,\n            \"metadata\": {\n                \"schema_version\": \"1.0.0\",\n                \"generated_at\": str(datetime.now().isoformat()),\n                \"data_sources\": self.get_data_info(),\n                \"summary\": {\n                    \"total\": total_models,\n                    \"serialized\": serialized,\n                    \"skipped\": skipped_for_dump,\n                    \"load_stats\": getattr(self, \"_last_load_stats\", None),\n                },\n            },\n        }\n\n    def get_raw_data_paths(self) -&gt; Dict[str, Optional[str]]:\n        \"\"\"Return canonical paths for raw data files (models.yaml and overrides.yaml).\n\n        Returns:\n            Dictionary with 'models' and 'overrides' keys containing file paths or None if bundled\n        \"\"\"\n        import os\n        from pathlib import Path\n\n        paths: Dict[str, Optional[str]] = {}\n\n        # Get models.yaml path\n        if hasattr(self, \"_data_manager\"):\n            # Try to get the actual file path from data manager\n            user_data_dir = get_user_data_dir()\n            models_path = user_data_dir / \"models.yaml\"\n            if models_path.exists():\n                paths[\"models\"] = str(models_path)\n            else:\n                # Check for environment override\n                env_path = os.getenv(\"OMR_MODEL_REGISTRY_PATH\")\n                if env_path and Path(env_path).exists():\n                    paths[\"models\"] = env_path\n                else:\n                    paths[\"models\"] = None  # Bundled\n\n            # Get overrides.yaml path\n            overrides_path = user_data_dir / \"overrides.yaml\"\n            if overrides_path.exists():\n                paths[\"overrides\"] = str(overrides_path)\n            else:\n                paths[\"overrides\"] = None  # Bundled\n        else:\n            paths[\"models\"] = None\n            paths[\"overrides\"] = None\n\n        return paths\n\n    def clear_cache(self, files: Optional[List[str]] = None) -&gt; None:\n        \"\"\"Delete cached data files in the user data directory.\n\n        Args:\n            files: Optional list of specific files to clear. If None, clears all known cache files.\n        \"\"\"\n        user_data_dir = get_user_data_dir()\n\n        # Default files to clear if none specified\n        if files is None:\n            files = [\"models.yaml\", \"overrides.yaml\"]\n\n        cleared_files = []\n        for filename in files:\n            file_path = user_data_dir / filename\n            try:\n                if file_path.exists():\n                    file_path.unlink()\n                    cleared_files.append(str(file_path))\n            except (OSError, PermissionError) as e:\n                log_warning(LogEvent.MODEL_REGISTRY, f\"Failed to clear cache file {file_path}: {e}\")\n\n        if cleared_files:\n            log_info(LogEvent.MODEL_REGISTRY, f\"Cleared {len(cleared_files)} cache files: {', '.join(cleared_files)}\")\n\n    def get_bundled_data_content(self, filename: str) -&gt; Optional[str]:\n        \"\"\"Get bundled data file content using public APIs.\n\n        Args:\n            filename: Name of the data file (e.g., 'models.yaml', 'overrides.yaml')\n\n        Returns:\n            File content as string, or None if not available\n        \"\"\"\n        if hasattr(self, \"_data_manager\"):\n            return self._data_manager._get_bundled_data_content(filename)\n        return None\n\n    def get_raw_model_data(self, model_name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get raw model data without provider overrides.\n\n        Args:\n            model_name: Name of the model to get raw data for\n\n        Returns:\n            Raw model data dictionary, or None if not found\n        \"\"\"\n        try:\n            # Get raw models.yaml content\n            raw_paths = self.get_raw_data_paths()\n            models_path = raw_paths.get(\"models\")\n\n            from pathlib import Path\n\n            if models_path and Path(models_path).exists():\n                # Load from user data file\n                with open(models_path, \"r\") as f:\n                    import yaml\n\n                    raw_data = yaml.safe_load(f)\n            else:\n                # Load from bundled data\n                content = self.get_bundled_data_content(\"models.yaml\")\n                if content:\n                    import yaml\n\n                    raw_data = yaml.safe_load(content)\n                else:\n                    return None\n\n            # Extract the specific model from raw data\n            if isinstance(raw_data, dict) and \"models\" in raw_data:\n                models = typing.cast(Dict[str, Any], raw_data[\"models\"])  # ensure typed\n                if model_name in models:\n                    base_obj = models[model_name]\n                    model_data: Dict[str, Any] = dict(base_obj) if isinstance(base_obj, dict) else {}\n                    model_data[\"name\"] = model_name\n                    model_data[\"metadata\"] = {\"source\": \"raw\", \"provider_applied\": None}\n                    return model_data\n\n            return None\n\n        except Exception:\n            return None\n\n    @property\n    def models(self) -&gt; Dict[str, ModelCapabilities]:\n        \"\"\"Get a read-only view of registered models.\"\"\"\n        with self._capabilities_lock:\n            return dict(self._capabilities)\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry-attributes","title":"Attributes","text":""},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.models","title":"<code>models</code>  <code>property</code>","text":"<p>Get a read-only view of registered models.</p>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry-functions","title":"Functions","text":""},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize a new registry instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[RegistryConfig]</code> <p>Configuration for this registry instance. If None, default    configuration is used.</p> <code>None</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def __init__(self, config: Optional[RegistryConfig] = None):\n    \"\"\"Initialize a new registry instance.\n\n    Args:\n        config: Configuration for this registry instance. If None, default\n               configuration is used.\n    \"\"\"\n    self.config = config or RegistryConfig()\n    self._capabilities: Dict[str, ModelCapabilities] = {}\n    self._constraints: Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]] = {}\n    self._capabilities_lock = threading.RLock()\n    # Stats for last load/dump operations (for observability)\n    self._last_load_stats: Dict[str, Any] = {}\n\n    # Initialize DataManager for model and overrides data\n    self._data_manager = DataManager()\n\n    # Set up caching for get_capabilities\n    self.get_capabilities = functools.lru_cache(maxsize=self.config.cache_size)(self._get_capabilities_impl)\n\n    # Auto-copy default constraint files to user directory if they don't exist\n    if not config or not config.constraints_path:\n        try:\n            copy_default_to_user_config(PARAM_CONSTRAINTS_FILENAME)\n        except OSError as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to copy default constraint config: {e}\",\n                error=str(e),\n            )\n\n    # Check for data updates if auto_update is enabled\n    if self.config.auto_update and self._data_manager.should_update_data():\n        try:\n            success = self._data_manager.check_for_updates()\n            if success:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Auto-update completed successfully\",\n                )\n                # Reload capabilities after successful auto-update\n                self._load_capabilities()\n        except Exception as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to auto-update data: {e}\",\n                error=str(e),\n            )\n\n    self._load_constraints()\n    self._load_capabilities()\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.assert_model_active","title":"<code>assert_model_active(model)</code>","text":"<p>Assert that a model is active and warn if deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name to check</p> required <p>Raises:</p> Type Description <code>ModelSunsetError</code> <p>If the model is sunset</p> <code>ModelNotSupportedError</code> <p>If the model is not found</p> <p>Warns:</p> Type Description <code>DeprecationWarning</code> <p>If the model is deprecated</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def assert_model_active(self, model: str) -&gt; None:\n    \"\"\"Assert that a model is active and warn if deprecated.\n\n    Args:\n        model: Model name to check\n\n    Raises:\n        ModelSunsetError: If the model is sunset\n        ModelNotSupportedError: If the model is not found\n\n    Warns:\n        DeprecationWarning: If the model is deprecated\n    \"\"\"\n    capabilities = self.get_capabilities(model)\n    assert_model_active(model, capabilities.deprecation)\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.check_data_updates","title":"<code>check_data_updates()</code>","text":"<p>Check if data updates are available using DataManager.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if updates are available, False otherwise</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def check_data_updates(self) -&gt; bool:\n    \"\"\"Check if data updates are available using DataManager.\n\n    Returns:\n        True if updates are available, False otherwise\n    \"\"\"\n    try:\n        if not self._data_manager.should_update_data():\n            return False\n\n        latest_release = self._data_manager._fetch_latest_data_release()\n        if not latest_release:\n            return False\n\n        latest_version = latest_release.get(\"tag_name\", \"\")\n        current_version = self._data_manager._get_current_version()\n\n        return not (current_version and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0)\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.check_for_updates","title":"<code>check_for_updates(url=None)</code>","text":"<p>Check if updates are available for the model registry.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Optional[str]</code> <p>Optional custom URL to check for updates</p> <code>None</code> <p>Returns:</p> Type Description <code>RefreshResult</code> <p>Result of the update check</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def check_for_updates(self, url: Optional[str] = None) -&gt; RefreshResult:\n    \"\"\"Check if updates are available for the model registry.\n\n    Args:\n        url: Optional custom URL to check for updates\n\n    Returns:\n        Result of the update check\n    \"\"\"\n    try:\n        import requests\n    except ImportError:\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=\"Could not import requests module\",\n        )\n\n    # Set up the URL with fallback handling\n    primary_url = url or (\n        \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n    )\n\n    # Define fallback URLs in case primary fails\n    fallback_urls = [\n        \"https://github.com/yaniv-golan/openai-model-registry/raw/main/data/models.yaml\",\n    ]\n\n    urls_to_try = [primary_url] + fallback_urls\n\n    try:\n        # Use a lock when checking and comparing versions to prevent race conditions\n        with self.__class__._instance_lock:\n            # First check with DataManager\n            if self._data_manager.should_update_data():\n                latest_release = self._data_manager._fetch_latest_data_release()\n                if latest_release:\n                    latest_version = latest_release.get(\"tag_name\", \"\")\n                    current_version = self._data_manager._get_current_version()\n                    if (\n                        current_version\n                        and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0\n                    ):\n                        return RefreshResult(\n                            success=True,\n                            status=RefreshStatus.ALREADY_CURRENT,\n                            message=f\"Registry is up to date (version {current_version})\",\n                        )\n                    else:\n                        return RefreshResult(\n                            success=True,\n                            status=RefreshStatus.UPDATE_AVAILABLE,\n                            message=f\"Update available: {current_version or 'bundled'} -&gt; {latest_version}\",\n                        )\n\n            # Fallback to original HTTP check with URL fallback\n            remote_config = None\n            for config_url in urls_to_try:\n                try:\n                    response = requests.get(config_url, timeout=10)\n                    response.raise_for_status()\n\n                    # Parse the remote config\n                    remote_config = yaml.safe_load(response.text)\n                    if isinstance(remote_config, dict):\n                        break\n                    else:\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Remote config from {config_url} is not a valid dictionary\",\n                        )\n                except (requests.RequestException, yaml.YAMLError) as e:\n                    log_warning(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"Failed to fetch from {config_url}: {e}\",\n                    )\n                    continue\n\n            if remote_config is None:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=\"Could not fetch remote config from any URL\",\n                )\n\n            # Get local config for comparison\n            local_config = self._load_config()\n            if not local_config.success:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"Could not load local config: {local_config.error}\",\n                )\n\n            # Compare versions (simplified comparison)\n            remote_version = remote_config.get(\"version\", \"unknown\")\n            local_version = local_config.data.get(\"version\", \"unknown\") if local_config.data else \"unknown\"\n\n            if remote_version == local_version:\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.ALREADY_CURRENT,\n                    message=f\"Registry is up to date (version {local_version})\",\n                )\n            else:\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.UPDATE_AVAILABLE,\n                    message=f\"Update available: {local_version} -&gt; {remote_version}\",\n                )\n\n    except requests.HTTPError as e:\n        if e.response.status_code == 404:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=\"Registry not found at any of the configured URLs\",\n            )\n        else:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"HTTP error {e.response.status_code}: {e}\",\n            )\n    except requests.RequestException as e:\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=f\"Network error: {e}\",\n        )\n    except Exception as e:\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=f\"Unexpected error: {e}\",\n        )\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.cleanup","title":"<code>cleanup()</code>  <code>staticmethod</code>","text":"<p>Clean up the registry instance.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@staticmethod\ndef cleanup() -&gt; None:\n    \"\"\"Clean up the registry instance.\"\"\"\n    with ModelRegistry._instance_lock:\n        ModelRegistry._default_instance = None\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.clear_cache","title":"<code>clear_cache(files=None)</code>","text":"<p>Delete cached data files in the user data directory.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Optional[List[str]]</code> <p>Optional list of specific files to clear. If None, clears all known cache files.</p> <code>None</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def clear_cache(self, files: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Delete cached data files in the user data directory.\n\n    Args:\n        files: Optional list of specific files to clear. If None, clears all known cache files.\n    \"\"\"\n    user_data_dir = get_user_data_dir()\n\n    # Default files to clear if none specified\n    if files is None:\n        files = [\"models.yaml\", \"overrides.yaml\"]\n\n    cleared_files = []\n    for filename in files:\n        file_path = user_data_dir / filename\n        try:\n            if file_path.exists():\n                file_path.unlink()\n                cleared_files.append(str(file_path))\n        except (OSError, PermissionError) as e:\n            log_warning(LogEvent.MODEL_REGISTRY, f\"Failed to clear cache file {file_path}: {e}\")\n\n    if cleared_files:\n        log_info(LogEvent.MODEL_REGISTRY, f\"Cleared {len(cleared_files)} cache files: {', '.join(cleared_files)}\")\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.dump_effective","title":"<code>dump_effective()</code>","text":"<p>Return the fully merged provider-adjusted dataset for the current provider.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the effective model capabilities after provider overrides</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def dump_effective(self) -&gt; Dict[str, Any]:\n    \"\"\"Return the fully merged provider-adjusted dataset for the current provider.\n\n    Returns:\n        Dictionary containing the effective model capabilities after provider overrides\n    \"\"\"\n    import os\n    from datetime import datetime\n\n    current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n    effective_data: Dict[str, Any] = {}\n    total_models = 0\n    serialized = 0\n    skipped_for_dump = 0\n\n    for model_name in self.models:\n        total_models += 1\n        try:\n            capabilities = self.get_capabilities(model_name)\n            effective_data[model_name] = {\n                \"context_window\": {\n                    \"total\": capabilities.context_window,\n                    \"input\": getattr(capabilities, \"input_context_window\", None),\n                    \"output\": capabilities.max_output_tokens,\n                },\n                \"pricing\": (\n                    {\n                        \"scheme\": getattr(capabilities.pricing, \"scheme\", \"per_token\"),\n                        \"unit\": getattr(capabilities.pricing, \"unit\", \"million_tokens\"),\n                        \"input_cost_per_unit\": getattr(capabilities.pricing, \"input_cost_per_unit\", 0.0),\n                        \"output_cost_per_unit\": getattr(capabilities.pricing, \"output_cost_per_unit\", 0.0),\n                        \"currency\": getattr(capabilities.pricing, \"currency\", \"USD\"),\n                        \"tiers\": getattr(capabilities.pricing, \"tiers\", None),\n                    }\n                    if getattr(capabilities, \"pricing\", None)\n                    else {\n                        \"scheme\": \"per_token\",\n                        \"unit\": \"million_tokens\",\n                        \"input_cost_per_unit\": 0.0,\n                        \"output_cost_per_unit\": 0.0,\n                        \"currency\": \"USD\",\n                        \"tiers\": None,\n                    }\n                ),\n                \"supports_vision\": capabilities.supports_vision,\n                \"supports_function_calling\": getattr(capabilities, \"supports_functions\", False),\n                \"supports_streaming\": capabilities.supports_streaming,\n                \"supports_structured_output\": getattr(capabilities, \"supports_structured\", False),\n                \"supports_json_mode\": getattr(capabilities, \"supports_json_mode\", False),\n                \"supports_web_search\": getattr(capabilities, \"supports_web_search\", False),\n                \"supports_audio\": getattr(capabilities, \"supports_audio\", False),\n                \"billing\": (\n                    {\"web_search\": asdict(cast(WebSearchBilling, capabilities.web_search_billing))}\n                    if getattr(capabilities, \"web_search_billing\", None)\n                    else None\n                ),\n                \"provider\": current_provider,\n                \"parameters\": getattr(capabilities, \"parameters\", {}),\n                \"input_modalities\": getattr(\n                    capabilities, \"input_modalities\", getattr(capabilities, \"modalities\", [])\n                ),\n                \"output_modalities\": getattr(capabilities, \"output_modalities\", []),\n                \"deprecation\": {\n                    \"status\": capabilities.deprecation.status,\n                    \"deprecates_on\": getattr(capabilities.deprecation, \"deprecates_on\", None),\n                    \"sunsets_on\": getattr(capabilities.deprecation, \"sunsets_on\", None),\n                    \"replacement\": getattr(capabilities.deprecation, \"replacement\", None),\n                    \"reason\": getattr(capabilities.deprecation, \"reason\", None),\n                    \"migration_guide\": getattr(capabilities.deprecation, \"migration_guide\", None),\n                },\n            }\n            serialized += 1\n        except Exception as e:\n            skipped_for_dump += 1\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to serialize model for dump_effective\",\n                model=model_name,\n                error=str(e),\n            )\n\n    return {\n        \"provider\": current_provider,\n        \"models\": effective_data,\n        \"metadata\": {\n            \"schema_version\": \"1.0.0\",\n            \"generated_at\": str(datetime.now().isoformat()),\n            \"data_sources\": self.get_data_info(),\n            \"summary\": {\n                \"total\": total_models,\n                \"serialized\": serialized,\n                \"skipped\": skipped_for_dump,\n                \"load_stats\": getattr(self, \"_last_load_stats\", None),\n            },\n        },\n    }\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_bundled_data_content","title":"<code>get_bundled_data_content(filename)</code>","text":"<p>Get bundled data file content using public APIs.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the data file (e.g., 'models.yaml', 'overrides.yaml')</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>File content as string, or None if not available</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_bundled_data_content(self, filename: str) -&gt; Optional[str]:\n    \"\"\"Get bundled data file content using public APIs.\n\n    Args:\n        filename: Name of the data file (e.g., 'models.yaml', 'overrides.yaml')\n\n    Returns:\n        File content as string, or None if not available\n    \"\"\"\n    if hasattr(self, \"_data_manager\"):\n        return self._data_manager._get_bundled_data_content(filename)\n    return None\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_data_info","title":"<code>get_data_info()</code>","text":"<p>Get information about data configuration and status.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing data configuration information</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_data_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get information about data configuration and status.\n\n    Returns:\n        Dictionary containing data configuration information\n    \"\"\"\n    try:\n        info: Dict[str, Any] = {\n            \"data_directory\": str(self._data_manager._data_dir),\n            \"current_version\": self._data_manager._get_current_version(),\n            \"updates_enabled\": self._data_manager.should_update_data(),\n            \"environment_variables\": {\n                \"OMR_DISABLE_DATA_UPDATES\": os.getenv(\"OMR_DISABLE_DATA_UPDATES\"),\n                \"OMR_DATA_VERSION_PIN\": os.getenv(\"OMR_DATA_VERSION_PIN\"),\n                \"OMR_DATA_DIR\": os.getenv(\"OMR_DATA_DIR\"),\n            },\n            \"data_files\": {},\n        }\n\n        # Check data file status\n        for filename in [\"models.yaml\", \"overrides.yaml\"]:\n            file_path = self._data_manager.get_data_file_path(filename)\n            info[\"data_files\"][filename] = {\n                \"path\": str(file_path) if file_path else None,\n                \"exists\": file_path is not None,\n                \"using_bundled\": file_path is None,\n            }\n\n        return info\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_data_version","title":"<code>get_data_version()</code>","text":"<p>Get the current data version.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Current data version string, or None if using bundled data</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_data_version(self) -&gt; Optional[str]:\n    \"\"\"Get the current data version.\n\n    Returns:\n        Current data version string, or None if using bundled data\n    \"\"\"\n    try:\n        return self._data_manager._get_current_version()\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_default","title":"<code>get_default()</code>  <code>classmethod</code>","text":"<p>Get the default registry instance with standard configuration.</p> <p>Returns:</p> Type Description <code>ModelRegistry</code> <p>The default ModelRegistry instance</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@classmethod\ndef get_default(cls) -&gt; \"ModelRegistry\":\n    \"\"\"Get the default registry instance with standard configuration.\n\n    Returns:\n        The default ModelRegistry instance\n    \"\"\"\n    with cls._instance_lock:\n        if cls._default_instance is None:\n            cls._default_instance = cls()\n        return cls._default_instance\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Get the default registry instance.</p> <p>Prefer :py:meth:<code>get_default</code> for clarity; this alias remains for brevity and historical usage but is not a separate code path.</p> <p>Returns:</p> Type Description <code>ModelRegistry</code> <p>The singleton :class:<code>ModelRegistry</code> instance.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@classmethod\ndef get_instance(cls) -&gt; \"ModelRegistry\":\n    \"\"\"Get the default registry instance.\n\n    Prefer :py:meth:`get_default` for clarity; this alias remains for\n    brevity and historical usage but is *not* a separate code path.\n\n    Returns:\n        The singleton :class:`ModelRegistry` instance.\n    \"\"\"\n    return cls.get_default()\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_parameter_constraint","title":"<code>get_parameter_constraint(ref)</code>","text":"<p>Get a parameter constraint by reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>Reference string (e.g., \"numeric_constraints.temperature\")</p> required <p>Returns:</p> Type Description <code>Union[NumericConstraint, EnumConstraint, ObjectConstraint]</code> <p>The constraint object (NumericConstraint or EnumConstraint or ObjectConstraint)</p> <p>Raises:</p> Type Description <code>ConstraintNotFoundError</code> <p>If the constraint is not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_parameter_constraint(self, ref: str) -&gt; Union[NumericConstraint, EnumConstraint, ObjectConstraint]:\n    \"\"\"Get a parameter constraint by reference.\n\n    Args:\n        ref: Reference string (e.g., \"numeric_constraints.temperature\")\n\n    Returns:\n        The constraint object (NumericConstraint or EnumConstraint or ObjectConstraint)\n\n    Raises:\n        ConstraintNotFoundError: If the constraint is not found\n    \"\"\"\n    if ref not in self._constraints:\n        raise ConstraintNotFoundError(\n            f\"Constraint reference '{ref}' not found in registry\",\n            ref=ref,\n        )\n    return self._constraints[ref]\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_raw_data_paths","title":"<code>get_raw_data_paths()</code>","text":"<p>Return canonical paths for raw data files (models.yaml and overrides.yaml).</p> <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dictionary with 'models' and 'overrides' keys containing file paths or None if bundled</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_raw_data_paths(self) -&gt; Dict[str, Optional[str]]:\n    \"\"\"Return canonical paths for raw data files (models.yaml and overrides.yaml).\n\n    Returns:\n        Dictionary with 'models' and 'overrides' keys containing file paths or None if bundled\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    paths: Dict[str, Optional[str]] = {}\n\n    # Get models.yaml path\n    if hasattr(self, \"_data_manager\"):\n        # Try to get the actual file path from data manager\n        user_data_dir = get_user_data_dir()\n        models_path = user_data_dir / \"models.yaml\"\n        if models_path.exists():\n            paths[\"models\"] = str(models_path)\n        else:\n            # Check for environment override\n            env_path = os.getenv(\"OMR_MODEL_REGISTRY_PATH\")\n            if env_path and Path(env_path).exists():\n                paths[\"models\"] = env_path\n            else:\n                paths[\"models\"] = None  # Bundled\n\n        # Get overrides.yaml path\n        overrides_path = user_data_dir / \"overrides.yaml\"\n        if overrides_path.exists():\n            paths[\"overrides\"] = str(overrides_path)\n        else:\n            paths[\"overrides\"] = None  # Bundled\n    else:\n        paths[\"models\"] = None\n        paths[\"overrides\"] = None\n\n    return paths\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_raw_model_data","title":"<code>get_raw_model_data(model_name)</code>","text":"<p>Get raw model data without provider overrides.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to get raw data for</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Raw model data dictionary, or None if not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_raw_model_data(self, model_name: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get raw model data without provider overrides.\n\n    Args:\n        model_name: Name of the model to get raw data for\n\n    Returns:\n        Raw model data dictionary, or None if not found\n    \"\"\"\n    try:\n        # Get raw models.yaml content\n        raw_paths = self.get_raw_data_paths()\n        models_path = raw_paths.get(\"models\")\n\n        from pathlib import Path\n\n        if models_path and Path(models_path).exists():\n            # Load from user data file\n            with open(models_path, \"r\") as f:\n                import yaml\n\n                raw_data = yaml.safe_load(f)\n        else:\n            # Load from bundled data\n            content = self.get_bundled_data_content(\"models.yaml\")\n            if content:\n                import yaml\n\n                raw_data = yaml.safe_load(content)\n            else:\n                return None\n\n        # Extract the specific model from raw data\n        if isinstance(raw_data, dict) and \"models\" in raw_data:\n            models = typing.cast(Dict[str, Any], raw_data[\"models\"])  # ensure typed\n            if model_name in models:\n                base_obj = models[model_name]\n                model_data: Dict[str, Any] = dict(base_obj) if isinstance(base_obj, dict) else {}\n                model_data[\"name\"] = model_name\n                model_data[\"metadata\"] = {\"source\": \"raw\", \"provider_applied\": None}\n                return model_data\n\n        return None\n\n    except Exception:\n        return None\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_sunset_headers","title":"<code>get_sunset_headers(model)</code>","text":"<p>Get RFC-compliant HTTP headers for model deprecation status.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary of HTTP headers</p> <p>Raises:</p> Type Description <code>ModelNotSupportedError</code> <p>If the model is not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_sunset_headers(self, model: str) -&gt; dict[str, str]:\n    \"\"\"Get RFC-compliant HTTP headers for model deprecation status.\n\n    Args:\n        model: Model name\n\n    Returns:\n        Dictionary of HTTP headers\n\n    Raises:\n        ModelNotSupportedError: If the model is not found\n    \"\"\"\n    capabilities = self.get_capabilities(model)\n    return sunset_headers(capabilities.deprecation)\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.get_update_info","title":"<code>get_update_info()</code>","text":"<p>Get detailed information about available updates.</p> <p>Returns:</p> Type Description <code>UpdateInfo</code> <p>UpdateInfo object containing update details</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_update_info(self) -&gt; UpdateInfo:\n    \"\"\"Get detailed information about available updates.\n\n    Returns:\n        UpdateInfo object containing update details\n    \"\"\"\n    try:\n        if not self._data_manager.should_update_data():\n            return UpdateInfo(\n                update_available=False,\n                current_version=self._data_manager._get_current_version(),\n                current_version_date=None,\n                latest_version=None,\n                latest_version_date=None,\n                download_url=None,\n                update_size_estimate=None,\n                latest_version_description=None,\n                accumulated_changes=[],\n                error_message=\"Updates are disabled via environment variable\",\n            )\n\n        latest_release = self._data_manager._fetch_latest_data_release()\n        if not latest_release:\n            return UpdateInfo(\n                update_available=False,\n                current_version=self._data_manager._get_current_version(),\n                current_version_date=None,\n                latest_version=None,\n                latest_version_date=None,\n                download_url=None,\n                update_size_estimate=None,\n                latest_version_description=None,\n                accumulated_changes=[],\n                error_message=\"No releases found on GitHub\",\n            )\n\n        latest_version_raw = latest_release.get(\"tag_name\", \"\")\n        current_version = self._data_manager._get_current_version()\n\n        # Clean the latest version to match current version format (remove data-v prefix)\n        latest_version = latest_version_raw\n        if latest_version_raw.startswith(\"data-v\"):\n            latest_version = latest_version_raw[len(\"data-v\") :]\n\n        # Get current version info with date\n        current_version_info = self._data_manager._get_current_version_info()\n        current_version_date = current_version_info.get(\"published_at\") if current_version_info else None\n\n        update_available = not (\n            current_version and self._data_manager._compare_versions(latest_version_raw, current_version) &lt;= 0\n        )\n\n        # Get accumulated changes between current and latest version\n        accumulated_changes = []\n        if update_available:\n            accumulated_changes = self._data_manager.get_accumulated_changes(current_version, latest_version_raw)\n\n        # Estimate update size based on assets\n        update_size_estimate = None\n        total_size = 0\n        assets = latest_release.get(\"assets\", [])\n        for asset in assets:\n            if asset.get(\"name\") in [\"models.yaml\", \"overrides.yaml\"]:\n                total_size += asset.get(\"size\", 0)\n\n        if total_size &gt; 0:\n            if total_size &lt; 1024:\n                update_size_estimate = f\"{total_size} bytes\"\n            elif total_size &lt; 1024 * 1024:\n                update_size_estimate = f\"{total_size / 1024:.1f} KB\"\n            else:\n                update_size_estimate = f\"{total_size / (1024 * 1024):.1f} MB\"\n\n        # Extract one-sentence description from latest release body\n        latest_version_description = None\n        if latest_release.get(\"body\"):\n            latest_version_description = self._data_manager._extract_change_summary(latest_release.get(\"body\", \"\"))\n\n        return UpdateInfo(\n            update_available=update_available,\n            current_version=current_version,\n            current_version_date=current_version_date,\n            latest_version=latest_version,\n            latest_version_date=latest_release.get(\"published_at\"),\n            download_url=latest_release.get(\"html_url\"),\n            update_size_estimate=update_size_estimate,\n            latest_version_description=latest_version_description,\n            accumulated_changes=accumulated_changes,\n            error_message=None,\n        )\n\n    except Exception as e:\n        return UpdateInfo(\n            update_available=False,\n            current_version=self._data_manager._get_current_version(),\n            current_version_date=None,\n            latest_version=None,\n            latest_version_date=None,\n            download_url=None,\n            update_size_estimate=None,\n            latest_version_description=None,\n            accumulated_changes=[],\n            error_message=str(e),\n        )\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.list_providers","title":"<code>list_providers()</code>","text":"<p>List all providers available in the overrides configuration.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of provider names found in overrides data</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def list_providers(self) -&gt; List[str]:\n    \"\"\"List all providers available in the overrides configuration.\n\n    Returns:\n        List of provider names found in overrides data\n    \"\"\"\n    import os\n\n    providers = set()\n\n    # Add the current provider\n    current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n    providers.add(current_provider)\n\n    # Add providers from overrides\n    if hasattr(self, \"_overrides\") and self._overrides:\n        overrides_data = self._overrides.get(\"overrides\", {})\n        for provider_name in overrides_data.keys():\n            providers.add(provider_name.lower())\n\n    return sorted(list(providers))\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.manual_update_workflow","title":"<code>manual_update_workflow(prompt_user_func=None)</code>","text":"<p>Manual update workflow with user approval.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_user_func</code> <code>Optional[Callable[[UpdateInfo], bool]]</code> <p>Optional function to prompt user for approval.             Should take UpdateInfo as parameter and return bool.             If None, uses a default console prompt.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was performed, False otherwise</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def manual_update_workflow(self, prompt_user_func: Optional[Callable[[UpdateInfo], bool]] = None) -&gt; bool:\n    \"\"\"Manual update workflow with user approval.\n\n    Args:\n        prompt_user_func: Optional function to prompt user for approval.\n                        Should take UpdateInfo as parameter and return bool.\n                        If None, uses a default console prompt.\n\n    Returns:\n        True if update was performed, False otherwise\n    \"\"\"\n    try:\n        # Get update information\n        update_info = self.get_update_info()\n\n        if update_info.error_message:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to check for updates: {update_info.error_message}\",\n            )\n            return False\n\n        if not update_info.update_available:\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                f\"Registry is up to date (version {update_info.current_version or 'bundled'})\",\n            )\n            return False\n\n        # Use custom prompt function or default\n        if prompt_user_func is None:\n            prompt_user_func = self._default_update_prompt\n\n        # Ask user for approval\n        if prompt_user_func(update_info):\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                f\"User approved update from {update_info.current_version or 'bundled'} to {update_info.latest_version}\",\n            )\n\n            # Perform the update\n            success = self.update_data()\n\n            if success:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Successfully updated to {update_info.latest_version}\",\n                )\n            else:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Update failed\",\n                )\n\n            return success\n        else:\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                \"User declined update\",\n            )\n            return False\n\n    except Exception as e:\n        log_error(\n            LogEvent.MODEL_REGISTRY,\n            f\"Manual update workflow failed: {e}\",\n        )\n        return False\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.refresh_from_remote","title":"<code>refresh_from_remote(url=None, force=False, validate_only=False)</code>","text":"<p>Refresh the registry configuration from remote source.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Optional[str]</code> <p>Optional custom URL to fetch registry from</p> <code>None</code> <code>force</code> <code>bool</code> <p>Force refresh even if version is current</p> <code>False</code> <code>validate_only</code> <code>bool</code> <p>Only validate remote config without updating</p> <code>False</code> <p>Returns:</p> Type Description <code>RefreshResult</code> <p>Result of the refresh operation</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def refresh_from_remote(\n    self,\n    url: Optional[str] = None,\n    force: bool = False,\n    validate_only: bool = False,\n) -&gt; RefreshResult:\n    \"\"\"Refresh the registry configuration from remote source.\n\n    Args:\n        url: Optional custom URL to fetch registry from\n        force: Force refresh even if version is current\n        validate_only: Only validate remote config without updating\n\n    Returns:\n        Result of the refresh operation\n    \"\"\"\n    try:\n        # Get remote config\n        config_url = url or (\n            \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n        )\n        remote_config = self._fetch_remote_config(config_url)\n        if not remote_config:\n            raise ValueError(\"Failed to fetch remote configuration\")\n\n        # Validate the remote config\n        self._validate_remote_config(remote_config)\n\n        if validate_only:\n            # Only validation was requested\n            return RefreshResult(\n                success=True,\n                status=RefreshStatus.VALIDATED,\n                message=\"Remote registry configuration validated successfully\",\n            )\n\n        # Check for updates only if not forcing and not validating\n        if not force:\n            result = self.check_for_updates(url=url)\n            if result.status == RefreshStatus.ALREADY_CURRENT:\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.ALREADY_CURRENT,\n                    message=\"Registry is already up to date\",\n                )\n\n        # Use DataManager to handle the update\n        try:\n            # Force update through DataManager\n            if self._data_manager.force_update():\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Successfully updated registry data via DataManager\",\n                )\n            else:\n                # Fallback to manual update if DataManager fails\n                # Note: This fallback downloads models.yaml and attempts overrides.yaml\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"DataManager update failed, using limited fallback (models.yaml only)\",\n                )\n                target_path = get_user_data_dir() / \"models.yaml\"\n                with open(target_path, \"w\") as f:\n                    yaml.safe_dump(remote_config, f)\n\n                # Try to download overrides.yaml if possible\n                try:\n                    overrides_url = \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/overrides.yaml\"\n\n                    # Simple fallback downloads\n                    try:\n                        import requests\n                    except ImportError:\n                        requests = None  # type: ignore\n\n                    if requests is not None:\n                        try:\n                            # Download overrides.yaml\n                            overrides_resp = requests.get(overrides_url, timeout=30)\n                            if overrides_resp.status_code == 200:\n                                overrides_content = overrides_resp.text\n                                overrides_path = get_user_data_dir() / \"overrides.yaml\"\n                                with open(overrides_path, \"w\") as f:\n                                    f.write(overrides_content)\n                                log_info(\n                                    LogEvent.MODEL_REGISTRY,\n                                    \"Downloaded overrides.yaml in fallback\",\n                                )\n\n                        except requests.RequestException as e:\n                            log_warning(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Failed to download additional files in fallback: {e}\",\n                            )\n                except Exception as e:\n                    log_warning(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"Error in fallback additional file download: {e}\",\n                    )\n        except PermissionError as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Permission denied when writing registry configuration\",\n                path=str(target_path),\n                error=str(e),\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Permission denied when writing to {target_path}\",\n            )\n        except OSError as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"File system error when writing registry configuration\",\n                path=str(target_path),\n                error=str(e),\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Error writing to {target_path}: {str(e)}\",\n            )\n\n        # Reload the registry with new configuration\n        self._load_constraints()\n        self._load_capabilities()\n\n        # Verify that the reload was successful\n        if not self._capabilities:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to reload registry after update\",\n                path=str(target_path),\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=\"Registry update failed: could not load capabilities after update\",\n            )\n\n        # Log success\n        log_info(\n            LogEvent.MODEL_REGISTRY,\n            \"Registry updated from remote\",\n            version=remote_config.get(\"version\", \"unknown\"),\n        )\n\n        return RefreshResult(\n            success=True,\n            status=RefreshStatus.UPDATED,\n            message=\"Registry updated successfully\",\n        )\n\n    except Exception as e:\n        error_msg = f\"Error refreshing registry: {str(e)}\"\n        log_error(\n            LogEvent.MODEL_REGISTRY,\n            error_msg,\n        )\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=error_msg,\n        )\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.ModelRegistry.update_data","title":"<code>update_data(force=False)</code>","text":"<p>Update model registry data using DataManager.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>If True, force update regardless of current version</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful, False otherwise</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def update_data(self, force: bool = False) -&gt; bool:\n    \"\"\"Update model registry data using DataManager.\n\n    Args:\n        force: If True, force update regardless of current version\n\n    Returns:\n        True if update was successful, False otherwise\n    \"\"\"\n    try:\n        if force:\n            success = self._data_manager.force_update()\n        else:\n            success = self._data_manager.check_for_updates()\n\n        if success:\n            # Reload capabilities after successful update\n            self._load_capabilities()\n\n        return success\n    except Exception:\n        return False\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.RegistryConfig","title":"<code>openai_model_registry.registry.RegistryConfig</code>","text":"<p>Configuration for the model registry.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class RegistryConfig:\n    \"\"\"Configuration for the model registry.\"\"\"\n\n    def __init__(\n        self,\n        registry_path: Optional[str] = None,\n        constraints_path: Optional[str] = None,\n        auto_update: bool = False,\n        cache_size: int = 100,\n    ):\n        \"\"\"Initialize registry configuration.\n\n        Args:\n            registry_path: Custom path to registry YAML file. If None,\n                           default location is used.\n            constraints_path: Custom path to constraints YAML file. If None,\n                              default location is used.\n            auto_update: Whether to automatically update the registry.\n            cache_size: Size of model capabilities cache.\n        \"\"\"\n        self.registry_path = registry_path  # Will be handled by DataManager\n        self.constraints_path = constraints_path or get_parameter_constraints_path()\n        self.auto_update = auto_update\n\n        # Validate cache_size bounds to prevent excessive memory usage\n        if cache_size &lt; 1:\n            raise ValueError(\"cache_size must be at least 1\")\n        if cache_size &gt; 10000:\n            raise ValueError(\"cache_size must not exceed 10000 to prevent excessive memory usage\")\n        self.cache_size = cache_size\n</code></pre>"},{"location":"api/model-registry/#openai_model_registry.registry.RegistryConfig-functions","title":"Functions","text":""},{"location":"api/model-registry/#openai_model_registry.registry.RegistryConfig.__init__","title":"<code>__init__(registry_path=None, constraints_path=None, auto_update=False, cache_size=100)</code>","text":"<p>Initialize registry configuration.</p> <p>Parameters:</p> Name Type Description Default <code>registry_path</code> <code>Optional[str]</code> <p>Custom path to registry YAML file. If None,            default location is used.</p> <code>None</code> <code>constraints_path</code> <code>Optional[str]</code> <p>Custom path to constraints YAML file. If None,               default location is used.</p> <code>None</code> <code>auto_update</code> <code>bool</code> <p>Whether to automatically update the registry.</p> <code>False</code> <code>cache_size</code> <code>int</code> <p>Size of model capabilities cache.</p> <code>100</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def __init__(\n    self,\n    registry_path: Optional[str] = None,\n    constraints_path: Optional[str] = None,\n    auto_update: bool = False,\n    cache_size: int = 100,\n):\n    \"\"\"Initialize registry configuration.\n\n    Args:\n        registry_path: Custom path to registry YAML file. If None,\n                       default location is used.\n        constraints_path: Custom path to constraints YAML file. If None,\n                          default location is used.\n        auto_update: Whether to automatically update the registry.\n        cache_size: Size of model capabilities cache.\n    \"\"\"\n    self.registry_path = registry_path  # Will be handled by DataManager\n    self.constraints_path = constraints_path or get_parameter_constraints_path()\n    self.auto_update = auto_update\n\n    # Validate cache_size bounds to prevent excessive memory usage\n    if cache_size &lt; 1:\n        raise ValueError(\"cache_size must be at least 1\")\n    if cache_size &gt; 10000:\n        raise ValueError(\"cache_size must not exceed 10000 to prevent excessive memory usage\")\n    self.cache_size = cache_size\n</code></pre>"},{"location":"api/model-registry/#pricinginfo","title":"PricingInfo","text":"<p><code>PricingInfo</code> now supports optional <code>tiers</code> to capture per-image tiered pricing (e.g., DALL\u00b7E models). The tiers structure is emitted under <code>pricing.tiers</code> in <code>dump_effective()</code> and surfaced by the CLI.</p>"},{"location":"api/model-registry/#usage-examples","title":"Usage Examples","text":""},{"location":"api/model-registry/#initializing-the-registry","title":"Initializing the Registry","text":"<pre><code>from openai_model_registry import ModelRegistry\nfrom openai_model_registry.registry import RegistryConfig\n\n# Get the default singleton instance\nregistry = ModelRegistry.get_default()\n\n# Or create a custom instance with specific configuration\nconfig = RegistryConfig(\n    registry_path=\"/custom/path/registry.yml\",\n    constraints_path=\"/custom/path/constraints.yml\",\n    auto_update=False,\n    cache_size=200,\n)\ncustom_registry = ModelRegistry(config)\n</code></pre>"},{"location":"api/model-registry/#getting-model-capabilities","title":"Getting Model Capabilities","text":"<pre><code>from openai_model_registry import ModelRegistry\n\n# Get the default registry instance\nregistry = ModelRegistry.get_default()\n\n# Get capabilities for a specific model\ncapabilities = registry.get_capabilities(\"gpt-4o\")\nprint(f\"Context window: {capabilities.context_window}\")\n</code></pre>"},{"location":"api/model-registry/#listing-available-models","title":"Listing Available Models","text":"<pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\n# List all available models\nmodels = registry.list_models()\nfor model in models:\n    print(f\"Model: {model}\")\n</code></pre>"},{"location":"api/model-registry/#updating-the-registry","title":"Updating the Registry","text":"<pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\n# Check if a model exists\nif registry.has_model(\"gpt-4o\"):\n    print(\"Model exists in registry\")\nelse:\n    print(\"Model not found\")\n</code></pre>"},{"location":"api/model-registry/#working-with-model-versions","title":"Working with Model Versions","text":"<pre><code>from openai_model_registry import ModelRegistry, ModelVersion\n\nregistry = ModelRegistry.get_default()\n\n# Parse a version from a model string\nmodel = \"gpt-4o-2024-05-13\"\nversion = ModelVersion.from_string(\"2024-05-13\")\n\nprint(f\"Year: {version.year}\")\nprint(f\"Month: {version.month}\")\nprint(f\"Day: {version.day}\")\n\n# Check if a version is newer than another\nnewer_version = ModelVersion.from_string(\"2024-06-01\")\nif newer_version &gt; version:\n    print(f\"{newer_version} is newer than {version}\")\n\n# Check if a model name follows the dated format pattern\nif hasattr(ModelVersion, \"is_dated_model\"):\n    is_dated = ModelVersion.is_dated_model(\"gpt-4o-2024-05-13\")\n    print(f\"Is a dated model: {is_dated}\")  # True\n\n    is_dated = ModelVersion.is_dated_model(\"gpt-4o\")\n    print(f\"Is a dated model: {is_dated}\")  # False\n</code></pre> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\n# Update registry from remote source\ntry:\n    result = registry.refresh_from_remote()\n    print(f\"Update result: {result}\")\nexcept Exception as e:\n    print(f\"Update failed: {e}\")\n</code></pre> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\n# Check for updates without applying them\ntry:\n    result = registry.check_for_updates()\n    if result.needs_update:\n        print(\"Updates available!\")\n        print(f\"Current version: {result.current_version}\")\n        print(f\"Latest version: {result.latest_version}\")\nexcept Exception as e:\n    print(f\"Update check failed: {e}\")\n</code></pre>"},{"location":"api_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> <p>cli</p> <ul> <li> <p>app</p> </li> <li> <p>commands</p> <ul> <li> <p>cache</p> </li> <li> <p>data</p> </li> <li> <p>models</p> </li> <li> <p>providers</p> </li> <li> <p>update</p> </li> </ul> </li> <li> <p>formatters</p> <ul> <li> <p>json</p> </li> <li> <p>table</p> </li> </ul> </li> <li> <p>utils</p> <ul> <li> <p>helpers</p> </li> <li> <p>options</p> </li> </ul> </li> </ul> </li> <li> <p>config_paths</p> </li> <li> <p>config_result</p> </li> <li> <p>constraints</p> </li> <li> <p>data_manager</p> </li> <li> <p>deprecation</p> </li> <li> <p>errors</p> </li> <li> <p>logging</p> </li> <li> <p>model_version</p> </li> <li> <p>pricing</p> </li> <li> <p>registry</p> </li> <li> <p>schema_version</p> </li> <li> <p>scripts</p> <ul> <li> <p>data_update</p> </li> <li> <p>fetch_pricing_ostruct</p> </li> <li> <p>update_registry</p> </li> </ul> </li> </ul>"},{"location":"api_reference/config_paths/","title":"Config paths","text":""},{"location":"api_reference/config_paths/#openai_model_registry.config_paths","title":"<code>openai_model_registry.config_paths</code>","text":"<p>Configuration path handling for model registry.</p> <p>This module implements path resolution for config files following the XDG Base Directory Specification. User-editable configuration goes in config directories, while programmatically updated data (like <code>models.yaml</code>) goes in data directories.</p>"},{"location":"api_reference/config_paths/#openai_model_registry.config_paths-functions","title":"Functions","text":""},{"location":"api_reference/config_paths/#openai_model_registry.config_paths.copy_default_to_user_config","title":"<code>copy_default_to_user_config(filename)</code>","text":"<p>Copy a default config file to the user config directory if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the config file to copy</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if file was copied, False if no action was taken</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If there is an error creating directory or copying file</p> Source code in <code>src/openai_model_registry/config_paths.py</code> <pre><code>def copy_default_to_user_config(filename: str) -&gt; bool:\n    \"\"\"Copy a default config file to the user config directory if it doesn't exist.\n\n    Args:\n        filename: Name of the config file to copy\n\n    Returns:\n        True if file was copied, False if no action was taken\n\n    Raises:\n        OSError: If there is an error creating directory or copying file\n    \"\"\"\n    package_file = get_package_config_dir() / filename\n    user_file = get_user_config_dir() / filename\n\n    # Don't copy if user file already exists\n    if user_file.exists():\n        return False\n\n    # Ensure directory exists\n    try:\n        ensure_user_config_dir_exists()\n    except OSError as e:\n        import logging\n\n        logging.getLogger(__name__).error(f\"Failed to create user config directory: {e}\")\n        raise  # Re-raise the exception for the caller to handle\n\n    # Only copy if package file exists\n    if package_file.exists():\n        try:\n            user_file.write_bytes(package_file.read_bytes())\n            return True\n        except (OSError, PermissionError) as e:\n            import logging\n\n            logging.getLogger(__name__).error(f\"Failed to copy config file {filename}: {e}\")\n            raise  # Re-raise the exception for the caller to handle\n\n    return False\n</code></pre>"},{"location":"api_reference/config_paths/#openai_model_registry.config_paths.ensure_user_config_dir_exists","title":"<code>ensure_user_config_dir_exists()</code>","text":"<p>Ensure that the user config directory exists.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission errors or other IO issues</p> <code>PermissionError</code> <p>If the directory exists but is not writable</p> Source code in <code>src/openai_model_registry/config_paths.py</code> <pre><code>def ensure_user_config_dir_exists() -&gt; None:\n    \"\"\"Ensure that the user config directory exists.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission errors or other IO issues\n        PermissionError: If the directory exists but is not writable\n    \"\"\"\n    user_dir = get_user_config_dir()\n\n    # If directory already exists, check if it's writable\n    if user_dir.exists():\n        if not os.access(user_dir, os.W_OK):\n            raise PermissionError(f\"Config directory exists but is not writable: {user_dir}\")\n        return\n\n    # Create the directory and its parents if needed\n    os.makedirs(user_dir, exist_ok=True)\n\n    # Verify the directory is writable after creation\n    if not os.access(user_dir, os.W_OK):\n        raise PermissionError(f\"Created config directory but it is not writable: {user_dir}\")\n</code></pre>"},{"location":"api_reference/config_paths/#openai_model_registry.config_paths.ensure_user_data_dir_exists","title":"<code>ensure_user_data_dir_exists()</code>","text":"<p>Ensure that the user data directory exists.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the directory cannot be created due to permission errors or other IO issues</p> <code>PermissionError</code> <p>If the directory exists but is not writable</p> Source code in <code>src/openai_model_registry/config_paths.py</code> <pre><code>def ensure_user_data_dir_exists() -&gt; None:\n    \"\"\"Ensure that the user data directory exists.\n\n    Raises:\n        OSError: If the directory cannot be created due to permission errors or other IO issues\n        PermissionError: If the directory exists but is not writable\n    \"\"\"\n    user_dir = get_user_data_dir()\n\n    # If directory already exists, check if it's writable\n    if user_dir.exists():\n        if not os.access(user_dir, os.W_OK):\n            raise PermissionError(f\"Data directory exists but is not writable: {user_dir}\")\n        return\n\n    # Create the directory and its parents if needed\n    os.makedirs(user_dir, exist_ok=True)\n\n    # Verify the directory is writable after creation\n    if not os.access(user_dir, os.W_OK):\n        raise PermissionError(f\"Created data directory but it is not writable: {user_dir}\")\n</code></pre>"},{"location":"api_reference/config_paths/#openai_model_registry.config_paths.get_package_config_dir","title":"<code>get_package_config_dir()</code>","text":"<p>Get the path to the package's config directory.</p> Source code in <code>src/openai_model_registry/config_paths.py</code> <pre><code>def get_package_config_dir() -&gt; Path:\n    \"\"\"Get the path to the package's config directory.\"\"\"\n    return Path(__file__).parent / \"config\"\n</code></pre>"},{"location":"api_reference/config_paths/#openai_model_registry.config_paths.get_parameter_constraints_path","title":"<code>get_parameter_constraints_path()</code>","text":"<p>Get the path to the parameter constraints file, respecting XDG specification.</p> <p>Returns:</p> Type Description <code>str</code> <p>Path to the parameter constraints file</p> Source code in <code>src/openai_model_registry/config_paths.py</code> <pre><code>def get_parameter_constraints_path() -&gt; str:\n    \"\"\"Get the path to the parameter constraints file, respecting XDG specification.\n\n    Returns:\n        Path to the parameter constraints file\n    \"\"\"\n    # 1. Check environment variable\n    env_path = os.environ.get(ENV_PARAM_CONSTRAINTS)\n    if env_path and Path(env_path).is_file():\n        return env_path\n\n    # 2. Check user config directory\n    user_path = get_user_config_dir() / PARAM_CONSTRAINTS_FILENAME\n    if user_path.is_file():\n        return str(user_path)\n\n    # 3. Fall back to package directory\n    return str(get_package_config_dir() / PARAM_CONSTRAINTS_FILENAME)\n</code></pre>"},{"location":"api_reference/config_paths/#openai_model_registry.config_paths.get_user_config_dir","title":"<code>get_user_config_dir()</code>","text":"<p>Get the path to the user's config directory for this application.</p> <p>Used for user-editable preferences and settings.</p> Source code in <code>src/openai_model_registry/config_paths.py</code> <pre><code>def get_user_config_dir() -&gt; Path:\n    \"\"\"Get the path to the user's config directory for this application.\n\n    Used for user-editable preferences and settings.\n    \"\"\"\n    return Path(platformdirs.user_config_dir(APP_NAME))\n</code></pre>"},{"location":"api_reference/config_paths/#openai_model_registry.config_paths.get_user_data_dir","title":"<code>get_user_data_dir()</code>","text":"<p>Get the path to the user's data directory for this application.</p> <p>Used for programmatically updated files like the model registry.</p> Source code in <code>src/openai_model_registry/config_paths.py</code> <pre><code>def get_user_data_dir() -&gt; Path:\n    \"\"\"Get the path to the user's data directory for this application.\n\n    Used for programmatically updated files like the model registry.\n    \"\"\"\n    return Path(platformdirs.user_data_dir(APP_NAME))\n</code></pre>"},{"location":"api_reference/config_result/","title":"Config result","text":""},{"location":"api_reference/config_result/#openai_model_registry.config_result","title":"<code>openai_model_registry.config_result</code>","text":"<p>Configuration loading result object.</p> <p>This module defines a standard result object for configuration loading operations.</p>"},{"location":"api_reference/config_result/#openai_model_registry.config_result-classes","title":"Classes","text":""},{"location":"api_reference/config_result/#openai_model_registry.config_result.ConfigResult","title":"<code>ConfigResult</code>  <code>dataclass</code>","text":"<p>Result of a configuration loading operation.</p> <p>This class provides a standardized way to handle results of configuration loading operations, including success/failure status and error information.</p> <p>Attributes:</p> Name Type Description <code>success</code> <code>bool</code> <p>Whether the operation was successful</p> <code>data</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration data (if successful)</p> <code>error</code> <code>Optional[str]</code> <p>Error message (if unsuccessful)</p> <code>exception</code> <code>Optional[Exception]</code> <p>Original exception (if an error occurred)</p> <code>path</code> <code>Optional[str]</code> <p>Path to the configuration file (if applicable)</p> Source code in <code>src/openai_model_registry/config_result.py</code> <pre><code>@dataclass\nclass ConfigResult:\n    \"\"\"Result of a configuration loading operation.\n\n    This class provides a standardized way to handle results of configuration\n    loading operations, including success/failure status and error information.\n\n    Attributes:\n        success: Whether the operation was successful\n        data: Configuration data (if successful)\n        error: Error message (if unsuccessful)\n        exception: Original exception (if an error occurred)\n        path: Path to the configuration file (if applicable)\n    \"\"\"\n\n    success: bool\n    data: Optional[Dict[str, Any]] = None\n    error: Optional[str] = None\n    exception: Optional[Exception] = None\n    path: Optional[str] = None\n</code></pre>"},{"location":"api_reference/constraints/","title":"Constraints","text":""},{"location":"api_reference/constraints/#openai_model_registry.constraints","title":"<code>openai_model_registry.constraints</code>","text":"<p>Parameter constraints for the model registry.</p> <p>This module defines the constraint types used to validate parameters for model calls.</p>"},{"location":"api_reference/constraints/#openai_model_registry.constraints-classes","title":"Classes","text":""},{"location":"api_reference/constraints/#openai_model_registry.constraints.EnumConstraint","title":"<code>EnumConstraint</code>","text":"<p>Constraint for enumerated parameters.</p> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>class EnumConstraint:\n    \"\"\"Constraint for enumerated parameters.\"\"\"\n\n    def __init__(\n        self,\n        allowed_values: List[str],\n        description: str = \"\",\n    ):\n        \"\"\"Initialize enum constraint.\n\n        Args:\n            allowed_values: List of allowed string values\n            description: Description of the parameter\n        \"\"\"\n        self.allowed_values = allowed_values\n        self.description = description\n\n    def validate(self, name: str, value: Any) -&gt; None:\n        \"\"\"Validate a value against this constraint.\n\n        Args:\n            name: Parameter name for error messages\n            value: Value to validate\n\n        Raises:\n            ModelRegistryError: If validation fails\n        \"\"\"\n        # Validate type\n        if not isinstance(value, str):\n            raise ModelRegistryError(\n                f\"Parameter '{name}' must be a string, got {type(value).__name__}.\\n\" f\"Description: {self.description}\"\n            )\n\n        # Validate allowed values\n        if value not in self.allowed_values:\n            raise ModelRegistryError(\n                f\"Invalid value '{value}' for parameter '{name}'.\\n\"\n                f\"Description: {self.description}\\n\"\n                f\"Allowed values: {', '.join(map(str, sorted(self.allowed_values)))}\"\n            )\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.EnumConstraint-functions","title":"Functions","text":""},{"location":"api_reference/constraints/#openai_model_registry.constraints.EnumConstraint.__init__","title":"<code>__init__(allowed_values, description='')</code>","text":"<p>Initialize enum constraint.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_values</code> <code>List[str]</code> <p>List of allowed string values</p> required <code>description</code> <code>str</code> <p>Description of the parameter</p> <code>''</code> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>def __init__(\n    self,\n    allowed_values: List[str],\n    description: str = \"\",\n):\n    \"\"\"Initialize enum constraint.\n\n    Args:\n        allowed_values: List of allowed string values\n        description: Description of the parameter\n    \"\"\"\n    self.allowed_values = allowed_values\n    self.description = description\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.EnumConstraint.validate","title":"<code>validate(name, value)</code>","text":"<p>Validate a value against this constraint.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name for error messages</p> required <code>value</code> <code>Any</code> <p>Value to validate</p> required <p>Raises:</p> Type Description <code>ModelRegistryError</code> <p>If validation fails</p> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>def validate(self, name: str, value: Any) -&gt; None:\n    \"\"\"Validate a value against this constraint.\n\n    Args:\n        name: Parameter name for error messages\n        value: Value to validate\n\n    Raises:\n        ModelRegistryError: If validation fails\n    \"\"\"\n    # Validate type\n    if not isinstance(value, str):\n        raise ModelRegistryError(\n            f\"Parameter '{name}' must be a string, got {type(value).__name__}.\\n\" f\"Description: {self.description}\"\n        )\n\n    # Validate allowed values\n    if value not in self.allowed_values:\n        raise ModelRegistryError(\n            f\"Invalid value '{value}' for parameter '{name}'.\\n\"\n            f\"Description: {self.description}\\n\"\n            f\"Allowed values: {', '.join(map(str, sorted(self.allowed_values)))}\"\n        )\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.NumericConstraint","title":"<code>NumericConstraint</code>","text":"<p>Constraint for numeric parameters.</p> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>class NumericConstraint:\n    \"\"\"Constraint for numeric parameters.\"\"\"\n\n    def __init__(\n        self,\n        min_value: float = 0.0,\n        max_value: Optional[float] = None,\n        allow_float: bool = True,\n        allow_int: bool = True,\n        description: str = \"\",\n    ):\n        \"\"\"Initialize numeric constraint.\n\n        Args:\n            min_value: Minimum allowed value\n            max_value: Maximum allowed value, or None for no upper limit\n            allow_float: Whether floating point values are allowed\n            allow_int: Whether integer values are allowed\n            description: Description of the parameter\n        \"\"\"\n        self.min_value = min_value\n        self.max_value = max_value\n        self.allow_float = allow_float\n        self.allow_int = allow_int\n        self.description = description\n\n    def validate(self, name: str, value: Any) -&gt; None:\n        \"\"\"Validate a value against this constraint.\n\n        Args:\n            name: Parameter name for error messages\n            value: Value to validate\n\n        Raises:\n            ModelRegistryError: If validation fails\n        \"\"\"\n        # Validate numeric type\n        if not isinstance(value, (int, float)):\n            raise ModelRegistryError(\n                f\"Parameter '{name}' must be a number, got {type(value).__name__}.\\n\"\n                \"Allowed types: \"\n                + (\n                    \"float and integer\"\n                    if self.allow_float and self.allow_int\n                    else (\"float only\" if self.allow_float else \"integer only\")\n                )\n            )\n\n        # Validate integer/float requirements\n        if isinstance(value, float) and not self.allow_float:\n            raise ModelRegistryError(\n                f\"Parameter '{name}' must be an integer, got float {value}.\\n\" f\"Description: {self.description}\"\n            )\n        if isinstance(value, int) and not self.allow_int:\n            raise ModelRegistryError(\n                f\"Parameter '{name}' must be a float, got integer {value}.\\n\" f\"Description: {self.description}\"\n            )\n\n        # Handle special float values (NaN and infinity)\n        if isinstance(value, float):\n            if math.isnan(value):\n                raise ModelRegistryError(\n                    f\"Parameter '{name}' cannot be NaN (not a number).\\n\" f\"Description: {self.description}\"\n                )\n            if math.isinf(value):\n                raise ModelRegistryError(f\"Parameter '{name}' cannot be infinity.\\n\" f\"Description: {self.description}\")\n\n        # Validate range\n        min_val = self.min_value\n        max_val = self.max_value\n\n        if value &lt; min_val or (max_val is not None and value &gt; max_val):\n            max_desc = str(max_val) if max_val is not None else \"unlimited\"\n            raise ModelRegistryError(\n                f\"Parameter '{name}' must be between {min_val} and {max_desc}.\\n\"\n                f\"Description: {self.description}\\n\"\n                f\"Current value: {value}\"\n            )\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.NumericConstraint-functions","title":"Functions","text":""},{"location":"api_reference/constraints/#openai_model_registry.constraints.NumericConstraint.__init__","title":"<code>__init__(min_value=0.0, max_value=None, allow_float=True, allow_int=True, description='')</code>","text":"<p>Initialize numeric constraint.</p> <p>Parameters:</p> Name Type Description Default <code>min_value</code> <code>float</code> <p>Minimum allowed value</p> <code>0.0</code> <code>max_value</code> <code>Optional[float]</code> <p>Maximum allowed value, or None for no upper limit</p> <code>None</code> <code>allow_float</code> <code>bool</code> <p>Whether floating point values are allowed</p> <code>True</code> <code>allow_int</code> <code>bool</code> <p>Whether integer values are allowed</p> <code>True</code> <code>description</code> <code>str</code> <p>Description of the parameter</p> <code>''</code> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>def __init__(\n    self,\n    min_value: float = 0.0,\n    max_value: Optional[float] = None,\n    allow_float: bool = True,\n    allow_int: bool = True,\n    description: str = \"\",\n):\n    \"\"\"Initialize numeric constraint.\n\n    Args:\n        min_value: Minimum allowed value\n        max_value: Maximum allowed value, or None for no upper limit\n        allow_float: Whether floating point values are allowed\n        allow_int: Whether integer values are allowed\n        description: Description of the parameter\n    \"\"\"\n    self.min_value = min_value\n    self.max_value = max_value\n    self.allow_float = allow_float\n    self.allow_int = allow_int\n    self.description = description\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.NumericConstraint.validate","title":"<code>validate(name, value)</code>","text":"<p>Validate a value against this constraint.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name for error messages</p> required <code>value</code> <code>Any</code> <p>Value to validate</p> required <p>Raises:</p> Type Description <code>ModelRegistryError</code> <p>If validation fails</p> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>def validate(self, name: str, value: Any) -&gt; None:\n    \"\"\"Validate a value against this constraint.\n\n    Args:\n        name: Parameter name for error messages\n        value: Value to validate\n\n    Raises:\n        ModelRegistryError: If validation fails\n    \"\"\"\n    # Validate numeric type\n    if not isinstance(value, (int, float)):\n        raise ModelRegistryError(\n            f\"Parameter '{name}' must be a number, got {type(value).__name__}.\\n\"\n            \"Allowed types: \"\n            + (\n                \"float and integer\"\n                if self.allow_float and self.allow_int\n                else (\"float only\" if self.allow_float else \"integer only\")\n            )\n        )\n\n    # Validate integer/float requirements\n    if isinstance(value, float) and not self.allow_float:\n        raise ModelRegistryError(\n            f\"Parameter '{name}' must be an integer, got float {value}.\\n\" f\"Description: {self.description}\"\n        )\n    if isinstance(value, int) and not self.allow_int:\n        raise ModelRegistryError(\n            f\"Parameter '{name}' must be a float, got integer {value}.\\n\" f\"Description: {self.description}\"\n        )\n\n    # Handle special float values (NaN and infinity)\n    if isinstance(value, float):\n        if math.isnan(value):\n            raise ModelRegistryError(\n                f\"Parameter '{name}' cannot be NaN (not a number).\\n\" f\"Description: {self.description}\"\n            )\n        if math.isinf(value):\n            raise ModelRegistryError(f\"Parameter '{name}' cannot be infinity.\\n\" f\"Description: {self.description}\")\n\n    # Validate range\n    min_val = self.min_value\n    max_val = self.max_value\n\n    if value &lt; min_val or (max_val is not None and value &gt; max_val):\n        max_desc = str(max_val) if max_val is not None else \"unlimited\"\n        raise ModelRegistryError(\n            f\"Parameter '{name}' must be between {min_val} and {max_desc}.\\n\"\n            f\"Description: {self.description}\\n\"\n            f\"Current value: {value}\"\n        )\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.ObjectConstraint","title":"<code>ObjectConstraint</code>","text":"<p>Constraint for object/dictionary parameters.</p> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>class ObjectConstraint:\n    \"\"\"Constraint for object/dictionary parameters.\"\"\"\n\n    def __init__(\n        self,\n        description: str = \"\",\n        required_keys: Optional[List[str]] = None,\n        allowed_keys: Optional[List[str]] = None,\n    ):\n        \"\"\"Initialize object constraint.\n\n        Args:\n            description: Description of the parameter\n            required_keys: List of required keys in the object\n            allowed_keys: List of allowed keys (if None, any keys are allowed)\n        \"\"\"\n        self.description = description\n        self.required_keys = required_keys or []\n        self.allowed_keys = allowed_keys\n\n    def validate(self, name: str, value: Any) -&gt; None:\n        \"\"\"Validate a value against this constraint.\n\n        Args:\n            name: Parameter name for error messages\n            value: Value to validate\n\n        Raises:\n            ModelRegistryError: If validation fails\n        \"\"\"\n        # Validate type\n        if not isinstance(value, dict):\n            raise ModelRegistryError(\n                f\"Parameter '{name}' must be an object/dictionary, got {type(value).__name__}.\\n\"\n                f\"Description: {self.description}\"\n            )\n\n        # Check required keys\n        missing_keys = [key for key in self.required_keys if key not in value]\n        if missing_keys:\n            raise ModelRegistryError(\n                f\"Parameter '{name}' is missing required keys: {', '.join(missing_keys)}.\\n\"\n                f\"Description: {self.description}\"\n            )\n\n        # Check allowed keys (if specified)\n        if self.allowed_keys is not None:\n            invalid_keys = [key for key in value.keys() if key not in self.allowed_keys]\n            if invalid_keys:\n                raise ModelRegistryError(\n                    f\"Parameter '{name}' contains invalid keys: {', '.join(invalid_keys)}.\\n\"\n                    f\"Description: {self.description}\\n\"\n                    f\"Allowed keys: {', '.join(self.allowed_keys)}\"\n                )\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.ObjectConstraint-functions","title":"Functions","text":""},{"location":"api_reference/constraints/#openai_model_registry.constraints.ObjectConstraint.__init__","title":"<code>__init__(description='', required_keys=None, allowed_keys=None)</code>","text":"<p>Initialize object constraint.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>Description of the parameter</p> <code>''</code> <code>required_keys</code> <code>Optional[List[str]]</code> <p>List of required keys in the object</p> <code>None</code> <code>allowed_keys</code> <code>Optional[List[str]]</code> <p>List of allowed keys (if None, any keys are allowed)</p> <code>None</code> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>def __init__(\n    self,\n    description: str = \"\",\n    required_keys: Optional[List[str]] = None,\n    allowed_keys: Optional[List[str]] = None,\n):\n    \"\"\"Initialize object constraint.\n\n    Args:\n        description: Description of the parameter\n        required_keys: List of required keys in the object\n        allowed_keys: List of allowed keys (if None, any keys are allowed)\n    \"\"\"\n    self.description = description\n    self.required_keys = required_keys or []\n    self.allowed_keys = allowed_keys\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.ObjectConstraint.validate","title":"<code>validate(name, value)</code>","text":"<p>Validate a value against this constraint.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name for error messages</p> required <code>value</code> <code>Any</code> <p>Value to validate</p> required <p>Raises:</p> Type Description <code>ModelRegistryError</code> <p>If validation fails</p> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>def validate(self, name: str, value: Any) -&gt; None:\n    \"\"\"Validate a value against this constraint.\n\n    Args:\n        name: Parameter name for error messages\n        value: Value to validate\n\n    Raises:\n        ModelRegistryError: If validation fails\n    \"\"\"\n    # Validate type\n    if not isinstance(value, dict):\n        raise ModelRegistryError(\n            f\"Parameter '{name}' must be an object/dictionary, got {type(value).__name__}.\\n\"\n            f\"Description: {self.description}\"\n        )\n\n    # Check required keys\n    missing_keys = [key for key in self.required_keys if key not in value]\n    if missing_keys:\n        raise ModelRegistryError(\n            f\"Parameter '{name}' is missing required keys: {', '.join(missing_keys)}.\\n\"\n            f\"Description: {self.description}\"\n        )\n\n    # Check allowed keys (if specified)\n    if self.allowed_keys is not None:\n        invalid_keys = [key for key in value.keys() if key not in self.allowed_keys]\n        if invalid_keys:\n            raise ModelRegistryError(\n                f\"Parameter '{name}' contains invalid keys: {', '.join(invalid_keys)}.\\n\"\n                f\"Description: {self.description}\\n\"\n                f\"Allowed keys: {', '.join(self.allowed_keys)}\"\n            )\n</code></pre>"},{"location":"api_reference/constraints/#openai_model_registry.constraints.ParameterReference","title":"<code>ParameterReference</code>  <code>dataclass</code>","text":"<p>Reference to a parameter constraint with optional metadata.</p> Source code in <code>src/openai_model_registry/constraints.py</code> <pre><code>@dataclass\nclass ParameterReference:\n    \"\"\"Reference to a parameter constraint with optional metadata.\"\"\"\n\n    ref: str\n    description: str = \"\"\n    max_value: Optional[float] = None\n</code></pre>"},{"location":"api_reference/data_manager/","title":"Data manager","text":""},{"location":"api_reference/data_manager/#openai_model_registry.data_manager","title":"<code>openai_model_registry.data_manager</code>","text":"<p>Data manager for OpenAI Model Registry.</p> <p>This module handles fetching, caching, and managing model registry data files from GitHub releases with version tracking and integrity verification.</p>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager-classes","title":"Classes","text":""},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager","title":"<code>DataManager</code>","text":"<p>Manages model registry data files with automatic updates and caching.</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>class DataManager:\n    \"\"\"Manages model registry data files with automatic updates and caching.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the data manager.\"\"\"\n        self._data_dir = self._get_data_directory()\n        self._ensure_data_directory()\n        self._version_lock = threading.Lock()\n\n    def _get_data_directory(self) -&gt; Path:\n        \"\"\"Get the data directory path, respecting OMR_DATA_DIR override.\"\"\"\n        custom_dir = os.getenv(ENV_DATA_DIR)\n        if custom_dir:\n            return Path(custom_dir)\n\n        return Path(user_data_dir(\"openai-model-registry\"))\n\n    def _ensure_data_directory(self) -&gt; None:\n        \"\"\"Ensure the data directory exists with proper permissions.\"\"\"\n        self._data_dir.mkdir(parents=True, exist_ok=True)\n\n        # Set secure permissions (readable/writable by owner only)\n        if hasattr(os, \"chmod\"):\n            os.chmod(self._data_dir, 0o700)  # Only owner can read/write/execute\n\n    def _get_github_api_url(self, endpoint: str, base_url: Optional[str] = None) -&gt; str:\n        \"\"\"Get GitHub API URL for the given endpoint.\"\"\"\n        base = base_url or GITHUB_API_BASE\n        return f\"{base}/repos/{GITHUB_REPO}/{endpoint}\"\n\n    def _fetch_latest_data_release(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Fetch information about the latest data release from GitHub API with fallback URLs.\"\"\"\n        if requests is None:\n            logger.error(\"requests module not available - cannot fetch data releases\")\n            return None\n\n        # Try each fallback base URL\n        for base_url in GITHUB_API_FALLBACK_BASES:\n            try:\n                url = self._get_github_api_url(\"releases\", base_url)\n                response = requests.get(url, timeout=30)\n                response.raise_for_status()\n\n                releases = response.json()\n\n                # Find the latest data release\n                for release in releases:\n                    tag_name = release.get(\"tag_name\", \"\")\n                    if tag_name.startswith(DATA_RELEASE_TAG_PREFIX):\n                        return cast(Dict[str, Any], release)\n\n                logger.warning(\"No data releases found in GitHub API response\")\n                return None\n\n            except requests.RequestException as e:\n                logger.warning(f\"Failed to fetch releases from {base_url}: {e}\")\n                continue  # Try next fallback URL\n            except (json.JSONDecodeError, KeyError) as e:\n                logger.warning(f\"Invalid response from {base_url}: {e}\")\n                continue  # Try next fallback URL\n\n        logger.error(\"Failed to fetch releases from all fallback URLs\")\n        return None\n\n    def _parse_version(self, version_str: str) -&gt; Tuple[int, int, int]:\n        \"\"\"Parse a semantic version string into components.\"\"\"\n        # Strip whitespace and validate input\n        version_str = version_str.strip()\n        if not version_str:\n            raise ValueError(\"Version string cannot be empty\")\n\n        # Check for reasonable string length (prevent excessive input)\n        if len(version_str) &gt; 50:\n            raise ValueError(f\"Version string too long: {len(version_str)} characters. Maximum allowed: 50\")\n\n        if version_str.startswith(DATA_RELEASE_TAG_PREFIX):\n            version_str = version_str[len(DATA_RELEASE_TAG_PREFIX) :]\n\n        try:\n            parts = version_str.split(\".\")\n            if len(parts) != 3:\n                raise ValueError(\"Invalid version format\")\n\n            return (int(parts[0]), int(parts[1]), int(parts[2]))\n        except (ValueError, IndexError) as e:\n            raise ValueError(f\"Invalid version format '{version_str}': {e}\")\n\n    def _compare_versions(self, version1: str, version2: str) -&gt; int:\n        \"\"\"Compare two version strings. Returns: -1 if v1 &lt; v2, 0 if equal, 1 if v1 &gt; v2.\"\"\"\n        try:\n            v1_parts = self._parse_version(version1)\n            v2_parts = self._parse_version(version2)\n\n            if v1_parts &lt; v2_parts:\n                return -1\n            elif v1_parts &gt; v2_parts:\n                return 1\n            else:\n                return 0\n        except ValueError as e:\n            logger.error(f\"Version comparison failed: {e}\")\n            return 0\n\n    def _get_current_version(self) -&gt; Optional[str]:\n        \"\"\"Get the currently installed data version.\"\"\"\n        version_file = self._data_dir / VERSION_INFO_JSON\n        if not version_file.exists():\n            return None\n\n        try:\n            with open(version_file, \"r\") as f:\n                version_info = json.load(f)\n            return cast(Optional[str], version_info.get(\"version\"))\n        except (json.JSONDecodeError, OSError) as e:\n            logger.warning(f\"Failed to read version info: {e}\")\n            return None\n\n    def _save_version_info(self, version: str, release_info: Dict[str, Any]) -&gt; None:\n        \"\"\"Save version information to local file with thread safety.\"\"\"\n        version_info = {\n            \"version\": version,\n            \"tag_name\": release_info.get(\"tag_name\"),\n            \"published_at\": release_info.get(\"published_at\"),\n            \"download_url\": release_info.get(\"html_url\"),\n        }\n\n        version_file = self._data_dir / VERSION_INFO_JSON\n        with self._version_lock:\n            try:\n                with open(version_file, \"w\") as f:\n                    json.dump(version_info, f, indent=2)\n\n                # Set secure permissions\n                if hasattr(os, \"chmod\"):\n                    os.chmod(version_file, 0o644)\n\n            except OSError as e:\n                logger.error(f\"Failed to save version info: {e}\")\n\n    def _download_file(self, url: str, target_path: Path) -&gt; bool:\n        \"\"\"Download a file from URL to target path.\"\"\"\n        if requests is None:\n            logger.error(\"requests module not available - cannot download files\")\n            return False\n\n        try:\n            response = requests.get(url, timeout=60, stream=True)\n            response.raise_for_status()\n\n            with open(target_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n\n            # Set secure permissions\n            if hasattr(os, \"chmod\"):\n                os.chmod(target_path, 0o644)\n\n            return True\n\n        except requests.RequestException as e:\n            logger.error(f\"Failed to download {url}: {e}\")\n            return False\n        except OSError as e:\n            logger.error(f\"Failed to write file {target_path}: {e}\")\n            return False\n\n    def _download_data_files(self, release_info: Dict[str, Any]) -&gt; bool:\n        \"\"\"Download data files from a GitHub release.\"\"\"\n        assets = release_info.get(\"assets\", [])\n\n        # Create temporary directory for downloads\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n\n            # Download all assets\n            for asset in assets:\n                asset_name = asset.get(\"name\", \"\")\n                download_url = asset.get(\"browser_download_url\")\n\n                if not download_url:\n                    continue\n\n                if asset_name in [MODELS_YAML, OVERRIDES_YAML]:\n                    target_path = temp_path / asset_name\n                    if not self._download_file(download_url, target_path):\n                        return False\n\n            # Move files to final location\n            for filename in [MODELS_YAML, OVERRIDES_YAML]:\n                temp_file = temp_path / filename\n                if temp_file.exists():\n                    final_path = self._data_dir / filename\n                    shutil.move(str(temp_file), str(final_path))\n\n            # Save version info\n            version = release_info.get(\"tag_name\", \"\").replace(DATA_RELEASE_TAG_PREFIX, \"\")\n            self._save_version_info(version, release_info)\n\n            return True\n\n    def should_update_data(self) -&gt; bool:\n        \"\"\"Check if data should be updated based on environment variables.\"\"\"\n        # Check if updates are disabled\n        if os.getenv(ENV_DISABLE_DATA_UPDATES, \"\").lower() in (\"1\", \"true\", \"yes\"):\n            logger.info(\"Data updates disabled by environment variable\")\n            return False\n\n        # Check if version is pinned\n        pinned_version = os.getenv(ENV_DATA_VERSION_PIN)\n        if pinned_version:\n            current_version = self._get_current_version()\n            if current_version == pinned_version:\n                logger.info(f\"Data version pinned to {pinned_version}, skipping update\")\n                return False\n\n        return True\n\n    def check_for_updates(self) -&gt; bool:\n        \"\"\"Check if there are newer data files available and update if needed.\"\"\"\n        if not self.should_update_data():\n            return False\n\n        # Check for pinned version\n        pinned_version = os.getenv(ENV_DATA_VERSION_PIN)\n        if pinned_version:\n            logger.info(f\"Checking for pinned version: {pinned_version}\")\n            # For pinned versions, we would need to fetch specific release\n            # This is a simplified implementation\n            return False\n\n        # Get latest release info\n        latest_release = self._fetch_latest_data_release()\n        if not latest_release:\n            logger.info(\"No data releases found, using bundled data\")\n            return False\n\n        latest_version = latest_release.get(\"tag_name\", \"\")\n        current_version = self._get_current_version()\n\n        if current_version and self._compare_versions(latest_version, current_version) &lt;= 0:\n            logger.info(f\"Current version {current_version} is up to date\")\n            return False\n\n        logger.info(f\"Updating data from {current_version or 'bundled'} to {latest_version}\")\n\n        # Download and install update\n        if self._download_data_files(latest_release):\n            logger.info(f\"Successfully updated to {latest_version}\")\n            return True\n        else:\n            logger.error(\"Failed to update data files\")\n            return False\n\n    def _get_bundled_data_content(self, filename: str) -&gt; Optional[str]:\n        \"\"\"Get bundled data file content as fallback.\"\"\"\n        try:\n            # Try to load from package data using importlib.resources\n            try:\n                # Use modern importlib.resources API (Python 3.9+)\n                data_package = resources.files(\"openai_model_registry.data\")\n                pkg_file = data_package / filename\n                if pkg_file.is_file():\n                    content = pkg_file.read_text()\n\n                    # Return content directly - validation handled elsewhere\n                    return content\n            except Exception:\n                # Ignore and fall through to filesystem fallback\n                pass\n\n            # Always try filesystem fallback regardless of importlib.resources result\n            bundled_path = Path(__file__).parent.parent.parent / \"data\" / filename\n            if bundled_path.exists():\n                # Return content directly - validation handled elsewhere\n                with open(bundled_path, \"r\") as f:\n                    return f.read()\n\n        except (OSError, IOError) as e:\n            logger.warning(f\"Failed to load bundled data {filename}: {e}\")\n        return None\n\n    def get_data_file_path(self, filename: str) -&gt; Optional[Path]:\n        \"\"\"Get the path to a data file, checking user directory first.\"\"\"\n        # Sanitize filename to prevent path traversal attacks\n        if not self._is_safe_filename(filename):\n            logger.warning(f\"Unsafe filename rejected: {filename}\")\n            return None\n\n        user_file = self._data_dir / filename\n\n        # Additional security check: ensure resolved path is within data directory\n        try:\n            resolved_path = user_file.resolve()\n            data_dir_resolved = self._data_dir.resolve()\n\n            # Check if the resolved path is within the data directory\n            if not str(resolved_path).startswith(str(data_dir_resolved)):\n                logger.warning(f\"Path traversal attempt blocked: {filename} -&gt; {resolved_path}\")\n                return None\n\n        except (OSError, RuntimeError) as e:\n            logger.warning(f\"Path resolution failed for {filename}: {e}\")\n            return None\n\n        if user_file.exists():\n            return user_file\n\n        return None\n\n    def _is_safe_filename(self, filename: str) -&gt; bool:\n        \"\"\"Check if a filename is safe (no path traversal attempts).\n\n        Args:\n            filename: The filename to validate\n\n        Returns:\n            True if filename is safe, False otherwise\n        \"\"\"\n        # Reject empty or None filenames\n        if not filename or not filename.strip():\n            return False\n\n        # Reject filenames with path separators or traversal attempts\n        dangerous_patterns = [\n            \"..\",  # Parent directory traversal\n            \"/\",  # Unix path separator\n            \"\\\\\",  # Windows path separator\n            \"\\0\",  # Null byte\n            \"\\n\",  # Newline\n            \"\\r\",  # Carriage return\n        ]\n\n        for pattern in dangerous_patterns:\n            if pattern in filename:\n                return False\n\n        # Reject filenames that are too long (potential buffer overflow)\n        if len(filename) &gt; 255:\n            return False\n\n        # Reject filenames starting with dot (hidden files)\n        if filename.startswith(\".\"):\n            return False\n\n        # Only allow alphanumeric characters, hyphens, underscores, and dots\n        import re\n\n        if not re.match(r\"^[a-zA-Z0-9._-]+$\", filename):\n            return False\n\n        return True\n\n    def get_data_file_content(self, filename: str) -&gt; Optional[str]:\n        \"\"\"Get data file content with fallback priority: (1) Env var, (2) User dir, (3) Bundled.\"\"\"\n        # Check for environment variable override (for tests)\n        if filename == \"models.yaml\":\n            env_path = os.getenv(\"OMR_MODEL_REGISTRY_PATH\")\n            if env_path and Path(env_path).exists():\n                try:\n                    with open(env_path, \"r\") as f:\n                        return f.read()\n                except (OSError, IOError) as e:\n                    logger.warning(f\"Failed to read env data file {env_path}: {e}\")\n\n        # First try user directory\n        user_file = self._data_dir / filename\n        if user_file.exists():\n            try:\n                with open(user_file, \"r\") as f:\n                    return f.read()\n            except (OSError, IOError) as e:\n                logger.warning(f\"Failed to read user data file {filename}: {e}\")\n\n        # Fallback to bundled data\n        logger.info(f\"Using bundled data for {filename}\")\n        return self._get_bundled_data_content(filename)\n\n    def force_update(self) -&gt; bool:\n        \"\"\"Force update data files regardless of current version.\"\"\"\n        latest_release = self._fetch_latest_data_release()\n        if not latest_release:\n            logger.error(\"No data releases found\")\n            return False\n\n        latest_version = latest_release.get(\"tag_name\", \"\")\n        logger.info(f\"Force updating to {latest_version}\")\n\n        return self._download_data_files(latest_release)\n\n    def _get_current_version_info(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get the current version information including date and metadata.\"\"\"\n        version_file = self._data_dir / VERSION_INFO_JSON\n        if not version_file.exists():\n            return None\n\n        try:\n            with open(version_file, \"r\") as f:\n                return cast(Dict[str, Any], json.load(f))\n        except (json.JSONDecodeError, OSError) as e:\n            logger.warning(f\"Failed to read version info: {e}\")\n            return None\n\n    def _fetch_all_data_releases(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Fetch all data releases from GitHub API.\"\"\"\n        if requests is None:\n            logger.error(\"requests module not available - cannot fetch data releases\")\n            return []\n\n        all_releases = []\n\n        # Try each fallback base URL\n        for base_url in GITHUB_API_FALLBACK_BASES:\n            try:\n                url = self._get_github_api_url(\"releases\", base_url)\n                response = requests.get(url, timeout=30)\n                response.raise_for_status()\n\n                releases = response.json()\n\n                # Filter for data releases only\n                for release in releases:\n                    tag_name = release.get(\"tag_name\", \"\")\n                    if tag_name.startswith(DATA_RELEASE_TAG_PREFIX):\n                        all_releases.append(release)\n\n                # Sort by version (newest first)\n                all_releases.sort(key=lambda x: x.get(\"tag_name\", \"\"), reverse=True)\n                return all_releases\n\n            except requests.RequestException as e:\n                logger.warning(f\"Failed to fetch releases from {base_url}: {e}\")\n                continue  # Try next fallback URL\n            except (json.JSONDecodeError, KeyError) as e:\n                logger.warning(f\"Invalid response from {base_url}: {e}\")\n                continue  # Try next fallback URL\n\n        logger.error(\"Failed to fetch releases from all fallback URLs\")\n        return []\n\n    def get_accumulated_changes(self, current_version: Optional[str], latest_version: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get accumulated changes between current and latest version.\n\n        Args:\n            current_version: Current version string (e.g., \"data-v1.0.0\")\n            latest_version: Latest version string (e.g., \"data-v1.2.0\")\n\n        Returns:\n            List of dictionaries containing change information for each version\n        \"\"\"\n        if not current_version:\n            # If no current version, return just the latest version info\n            latest_release = self._fetch_latest_data_release()\n            if latest_release:\n                return [\n                    {\n                        \"version\": latest_version,\n                        \"date\": latest_release.get(\"published_at\"),\n                        \"description\": latest_release.get(\"body\", \"No description available\"),\n                        \"url\": latest_release.get(\"html_url\"),\n                    }\n                ]\n            return []\n\n        # Fetch all releases to find changes between versions\n        all_releases = self._fetch_all_data_releases()\n        if not all_releases:\n            return []\n\n        changes = []\n        found_current = False\n\n        for release in all_releases:\n            release_version = release.get(\"tag_name\", \"\")\n\n            # Stop when we reach the current version\n            if release_version == current_version:\n                found_current = True\n                break\n\n            # Add releases newer than current version\n            if self._compare_versions(release_version, current_version) &gt; 0:\n                changes.append(\n                    {\n                        \"version\": release_version,\n                        \"date\": release.get(\"published_at\"),\n                        \"description\": self._extract_change_summary(release.get(\"body\", \"\")),\n                        \"url\": release.get(\"html_url\"),\n                    }\n                )\n\n        # If we didn't find the current version, include all releases up to latest\n        if not found_current:\n            # Find the latest release and include it\n            for release in all_releases:\n                if release.get(\"tag_name\") == latest_version:\n                    changes.append(\n                        {\n                            \"version\": latest_version,\n                            \"date\": release.get(\"published_at\"),\n                            \"description\": self._extract_change_summary(release.get(\"body\", \"\")),\n                            \"url\": release.get(\"html_url\"),\n                        }\n                    )\n                    break\n\n        # Sort by version (newest first)\n        changes.sort(key=lambda x: x[\"version\"], reverse=True)\n        return changes\n\n    def _extract_change_summary(self, release_body: str) -&gt; str:\n        \"\"\"Extract a one-sentence summary from release body.\n\n        Args:\n            release_body: The full release body text\n\n        Returns:\n            One-sentence summary of the changes\n        \"\"\"\n        if not release_body:\n            return \"No description available\"\n\n        # Clean up the release body\n        lines = release_body.strip().split(\"\\n\")\n\n        # Look for the first meaningful line that's not a header\n        for line in lines:\n            line = line.strip()\n            if line and not line.startswith(\"#\") and not line.startswith(\"**\") and len(line) &gt; 10:\n                # Take first sentence or first 100 characters\n                if \".\" in line:\n                    first_sentence = line.split(\".\")[0] + \".\"\n                    if len(first_sentence) &gt; 20:  # Make sure it's substantial\n                        return first_sentence\n\n                # Fallback to first 100 characters\n                if len(line) &gt; 100:\n                    return line[:97] + \"...\"\n                return line\n\n        # Fallback to first non-empty line\n        for line in lines:\n            line = line.strip()\n            if line:\n                if len(line) &gt; 100:\n                    return line[:97] + \"...\"\n                return line\n\n        return \"No description available\"\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager-functions","title":"Functions","text":""},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the data manager.</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the data manager.\"\"\"\n    self._data_dir = self._get_data_directory()\n    self._ensure_data_directory()\n    self._version_lock = threading.Lock()\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager.check_for_updates","title":"<code>check_for_updates()</code>","text":"<p>Check if there are newer data files available and update if needed.</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>def check_for_updates(self) -&gt; bool:\n    \"\"\"Check if there are newer data files available and update if needed.\"\"\"\n    if not self.should_update_data():\n        return False\n\n    # Check for pinned version\n    pinned_version = os.getenv(ENV_DATA_VERSION_PIN)\n    if pinned_version:\n        logger.info(f\"Checking for pinned version: {pinned_version}\")\n        # For pinned versions, we would need to fetch specific release\n        # This is a simplified implementation\n        return False\n\n    # Get latest release info\n    latest_release = self._fetch_latest_data_release()\n    if not latest_release:\n        logger.info(\"No data releases found, using bundled data\")\n        return False\n\n    latest_version = latest_release.get(\"tag_name\", \"\")\n    current_version = self._get_current_version()\n\n    if current_version and self._compare_versions(latest_version, current_version) &lt;= 0:\n        logger.info(f\"Current version {current_version} is up to date\")\n        return False\n\n    logger.info(f\"Updating data from {current_version or 'bundled'} to {latest_version}\")\n\n    # Download and install update\n    if self._download_data_files(latest_release):\n        logger.info(f\"Successfully updated to {latest_version}\")\n        return True\n    else:\n        logger.error(\"Failed to update data files\")\n        return False\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager.force_update","title":"<code>force_update()</code>","text":"<p>Force update data files regardless of current version.</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>def force_update(self) -&gt; bool:\n    \"\"\"Force update data files regardless of current version.\"\"\"\n    latest_release = self._fetch_latest_data_release()\n    if not latest_release:\n        logger.error(\"No data releases found\")\n        return False\n\n    latest_version = latest_release.get(\"tag_name\", \"\")\n    logger.info(f\"Force updating to {latest_version}\")\n\n    return self._download_data_files(latest_release)\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager.get_accumulated_changes","title":"<code>get_accumulated_changes(current_version, latest_version)</code>","text":"<p>Get accumulated changes between current and latest version.</p> <p>Parameters:</p> Name Type Description Default <code>current_version</code> <code>Optional[str]</code> <p>Current version string (e.g., \"data-v1.0.0\")</p> required <code>latest_version</code> <code>str</code> <p>Latest version string (e.g., \"data-v1.2.0\")</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dictionaries containing change information for each version</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>def get_accumulated_changes(self, current_version: Optional[str], latest_version: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get accumulated changes between current and latest version.\n\n    Args:\n        current_version: Current version string (e.g., \"data-v1.0.0\")\n        latest_version: Latest version string (e.g., \"data-v1.2.0\")\n\n    Returns:\n        List of dictionaries containing change information for each version\n    \"\"\"\n    if not current_version:\n        # If no current version, return just the latest version info\n        latest_release = self._fetch_latest_data_release()\n        if latest_release:\n            return [\n                {\n                    \"version\": latest_version,\n                    \"date\": latest_release.get(\"published_at\"),\n                    \"description\": latest_release.get(\"body\", \"No description available\"),\n                    \"url\": latest_release.get(\"html_url\"),\n                }\n            ]\n        return []\n\n    # Fetch all releases to find changes between versions\n    all_releases = self._fetch_all_data_releases()\n    if not all_releases:\n        return []\n\n    changes = []\n    found_current = False\n\n    for release in all_releases:\n        release_version = release.get(\"tag_name\", \"\")\n\n        # Stop when we reach the current version\n        if release_version == current_version:\n            found_current = True\n            break\n\n        # Add releases newer than current version\n        if self._compare_versions(release_version, current_version) &gt; 0:\n            changes.append(\n                {\n                    \"version\": release_version,\n                    \"date\": release.get(\"published_at\"),\n                    \"description\": self._extract_change_summary(release.get(\"body\", \"\")),\n                    \"url\": release.get(\"html_url\"),\n                }\n            )\n\n    # If we didn't find the current version, include all releases up to latest\n    if not found_current:\n        # Find the latest release and include it\n        for release in all_releases:\n            if release.get(\"tag_name\") == latest_version:\n                changes.append(\n                    {\n                        \"version\": latest_version,\n                        \"date\": release.get(\"published_at\"),\n                        \"description\": self._extract_change_summary(release.get(\"body\", \"\")),\n                        \"url\": release.get(\"html_url\"),\n                    }\n                )\n                break\n\n    # Sort by version (newest first)\n    changes.sort(key=lambda x: x[\"version\"], reverse=True)\n    return changes\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager.get_data_file_content","title":"<code>get_data_file_content(filename)</code>","text":"<p>Get data file content with fallback priority: (1) Env var, (2) User dir, (3) Bundled.</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>def get_data_file_content(self, filename: str) -&gt; Optional[str]:\n    \"\"\"Get data file content with fallback priority: (1) Env var, (2) User dir, (3) Bundled.\"\"\"\n    # Check for environment variable override (for tests)\n    if filename == \"models.yaml\":\n        env_path = os.getenv(\"OMR_MODEL_REGISTRY_PATH\")\n        if env_path and Path(env_path).exists():\n            try:\n                with open(env_path, \"r\") as f:\n                    return f.read()\n            except (OSError, IOError) as e:\n                logger.warning(f\"Failed to read env data file {env_path}: {e}\")\n\n    # First try user directory\n    user_file = self._data_dir / filename\n    if user_file.exists():\n        try:\n            with open(user_file, \"r\") as f:\n                return f.read()\n        except (OSError, IOError) as e:\n            logger.warning(f\"Failed to read user data file {filename}: {e}\")\n\n    # Fallback to bundled data\n    logger.info(f\"Using bundled data for {filename}\")\n    return self._get_bundled_data_content(filename)\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager.get_data_file_path","title":"<code>get_data_file_path(filename)</code>","text":"<p>Get the path to a data file, checking user directory first.</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>def get_data_file_path(self, filename: str) -&gt; Optional[Path]:\n    \"\"\"Get the path to a data file, checking user directory first.\"\"\"\n    # Sanitize filename to prevent path traversal attacks\n    if not self._is_safe_filename(filename):\n        logger.warning(f\"Unsafe filename rejected: {filename}\")\n        return None\n\n    user_file = self._data_dir / filename\n\n    # Additional security check: ensure resolved path is within data directory\n    try:\n        resolved_path = user_file.resolve()\n        data_dir_resolved = self._data_dir.resolve()\n\n        # Check if the resolved path is within the data directory\n        if not str(resolved_path).startswith(str(data_dir_resolved)):\n            logger.warning(f\"Path traversal attempt blocked: {filename} -&gt; {resolved_path}\")\n            return None\n\n    except (OSError, RuntimeError) as e:\n        logger.warning(f\"Path resolution failed for {filename}: {e}\")\n        return None\n\n    if user_file.exists():\n        return user_file\n\n    return None\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager.DataManager.should_update_data","title":"<code>should_update_data()</code>","text":"<p>Check if data should be updated based on environment variables.</p> Source code in <code>src/openai_model_registry/data_manager.py</code> <pre><code>def should_update_data(self) -&gt; bool:\n    \"\"\"Check if data should be updated based on environment variables.\"\"\"\n    # Check if updates are disabled\n    if os.getenv(ENV_DISABLE_DATA_UPDATES, \"\").lower() in (\"1\", \"true\", \"yes\"):\n        logger.info(\"Data updates disabled by environment variable\")\n        return False\n\n    # Check if version is pinned\n    pinned_version = os.getenv(ENV_DATA_VERSION_PIN)\n    if pinned_version:\n        current_version = self._get_current_version()\n        if current_version == pinned_version:\n            logger.info(f\"Data version pinned to {pinned_version}, skipping update\")\n            return False\n\n    return True\n</code></pre>"},{"location":"api_reference/data_manager/#openai_model_registry.data_manager-functions","title":"Functions","text":""},{"location":"api_reference/deprecation/","title":"Deprecation","text":""},{"location":"api_reference/deprecation/#openai_model_registry.deprecation","title":"<code>openai_model_registry.deprecation</code>","text":"<p>Deprecation support for model registry.</p> <p>This module provides deprecation metadata and validation for models.</p>"},{"location":"api_reference/deprecation/#openai_model_registry.deprecation-classes","title":"Classes","text":""},{"location":"api_reference/deprecation/#openai_model_registry.deprecation.DeprecationInfo","title":"<code>DeprecationInfo</code>  <code>dataclass</code>","text":"<p>Deprecation metadata for a model.</p> <p>All fields are mandatory in the current schema.</p> Source code in <code>src/openai_model_registry/deprecation.py</code> <pre><code>@dataclass(frozen=True)\nclass DeprecationInfo:\n    \"\"\"Deprecation metadata for a model.\n\n    All fields are mandatory in the current schema.\n    \"\"\"\n\n    status: Literal[\"active\", \"deprecated\", \"sunset\"]\n    deprecates_on: Optional[date]\n    sunsets_on: Optional[date]\n    replacement: Optional[str]\n    migration_guide: Optional[str]\n    reason: str\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate deprecation dates are properly ordered.\"\"\"\n        if self.deprecates_on is not None and self.sunsets_on is not None and self.deprecates_on &gt; self.sunsets_on:\n            raise ValueError(f\"deprecates_on ({self.deprecates_on}) must be &lt;= sunsets_on ({self.sunsets_on})\")\n</code></pre>"},{"location":"api_reference/deprecation/#openai_model_registry.deprecation.DeprecationInfo-functions","title":"Functions","text":""},{"location":"api_reference/deprecation/#openai_model_registry.deprecation.DeprecationInfo.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate deprecation dates are properly ordered.</p> Source code in <code>src/openai_model_registry/deprecation.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate deprecation dates are properly ordered.\"\"\"\n    if self.deprecates_on is not None and self.sunsets_on is not None and self.deprecates_on &gt; self.sunsets_on:\n        raise ValueError(f\"deprecates_on ({self.deprecates_on}) must be &lt;= sunsets_on ({self.sunsets_on})\")\n</code></pre>"},{"location":"api_reference/deprecation/#openai_model_registry.deprecation.InvalidSchemaVersionError","title":"<code>InvalidSchemaVersionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when the schema version is not supported.</p> Source code in <code>src/openai_model_registry/deprecation.py</code> <pre><code>class InvalidSchemaVersionError(Exception):\n    \"\"\"Raised when the schema version is not supported.\"\"\"\n\n    def __init__(self, found_version: Optional[str], expected_version: str = \"2\"):\n        self.found_version = found_version\n        self.expected_version = expected_version\n        super().__init__(f\"Invalid schema version: found {found_version}, expected {expected_version}\")\n</code></pre>"},{"location":"api_reference/deprecation/#openai_model_registry.deprecation.ModelSunsetError","title":"<code>ModelSunsetError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when attempting to access a sunset model.</p> Source code in <code>src/openai_model_registry/deprecation.py</code> <pre><code>class ModelSunsetError(Exception):\n    \"\"\"Raised when attempting to access a sunset model.\"\"\"\n\n    def __init__(self, model: str, sunset_date: date):\n        self.model = model\n        self.sunset_date = sunset_date\n        super().__init__(f\"Model '{model}' has been sunset as of {sunset_date}. It is no longer available for use.\")\n</code></pre>"},{"location":"api_reference/deprecation/#openai_model_registry.deprecation-functions","title":"Functions","text":""},{"location":"api_reference/deprecation/#openai_model_registry.deprecation.assert_model_active","title":"<code>assert_model_active(model, deprecation_info)</code>","text":"<p>Assert that a model is active and warn if deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <code>deprecation_info</code> <code>DeprecationInfo</code> <p>Deprecation metadata</p> required <p>Raises:</p> Type Description <code>ModelSunsetError</code> <p>If the model is sunset</p> <p>Warns:</p> Type Description <code>DeprecationWarning</code> <p>If the model is deprecated</p> Source code in <code>src/openai_model_registry/deprecation.py</code> <pre><code>def assert_model_active(model: str, deprecation_info: DeprecationInfo) -&gt; None:\n    \"\"\"Assert that a model is active and warn if deprecated.\n\n    Args:\n        model: Model name\n        deprecation_info: Deprecation metadata\n\n    Raises:\n        ModelSunsetError: If the model is sunset\n\n    Warns:\n        DeprecationWarning: If the model is deprecated\n    \"\"\"\n    if deprecation_info.status == \"sunset\":\n        if deprecation_info.sunsets_on is None:\n            raise ValueError(f\"Sunset model '{model}' missing sunset date\")\n        raise ModelSunsetError(model, deprecation_info.sunsets_on)\n\n    if deprecation_info.status == \"deprecated\":\n        sunset_date = (\n            deprecation_info.sunsets_on.isoformat() if deprecation_info.sunsets_on is not None else \"unknown date\"\n        )\n        warnings.warn(\n            f\"{model} is deprecated; will sunset {sunset_date}\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"api_reference/deprecation/#openai_model_registry.deprecation.sunset_headers","title":"<code>sunset_headers(deprecation_info)</code>","text":"<p>Generate RFC-compliant HTTP headers for deprecation status.</p> <p>Parameters:</p> Name Type Description Default <code>deprecation_info</code> <code>DeprecationInfo</code> <p>Deprecation metadata</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary of HTTP headers</p> Source code in <code>src/openai_model_registry/deprecation.py</code> <pre><code>def sunset_headers(deprecation_info: DeprecationInfo) -&gt; Dict[str, str]:\n    \"\"\"Generate RFC-compliant HTTP headers for deprecation status.\n\n    Args:\n        deprecation_info: Deprecation metadata\n\n    Returns:\n        Dictionary of HTTP headers\n    \"\"\"\n    if deprecation_info.status == \"active\":\n        return {}\n\n    headers: Dict[str, str] = {}\n\n    if deprecation_info.deprecates_on is not None:\n        headers[\"Deprecation\"] = deprecation_info.deprecates_on.isoformat()  # RFC 9745 \u00a73\n\n    if deprecation_info.sunsets_on is not None:\n        headers[\"Sunset\"] = deprecation_info.sunsets_on.isoformat()  # RFC 8594 \u00a72\n\n    if deprecation_info.migration_guide:\n        headers[\"Link\"] = f'&lt;{deprecation_info.migration_guide}&gt;; rel=\"deprecation\"'\n\n    return headers\n</code></pre>"},{"location":"api_reference/errors/","title":"Errors","text":""},{"location":"api_reference/errors/#openai_model_registry.errors","title":"<code>openai_model_registry.errors</code>","text":"<p>Error types for the OpenAI model registry.</p> <p>This module defines the error types used by the model registry for various validation and compatibility issues.</p>"},{"location":"api_reference/errors/#openai_model_registry.errors-classes","title":"Classes","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.ConfigFileNotFoundError","title":"<code>ConfigFileNotFoundError</code>","text":"<p>               Bases: <code>ConfigurationError</code></p> <p>Raised when a required configuration file is not found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     registry._load_config()\n... except ConfigFileNotFoundError as e:\n...     print(f\"Config file not found: {e.path}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ConfigFileNotFoundError(ConfigurationError):\n    \"\"\"Raised when a required configuration file is not found.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     registry._load_config()\n        ... except ConfigFileNotFoundError as e:\n        ...     print(f\"Config file not found: {e.path}\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ConfigurationError","title":"<code>ConfigurationError</code>","text":"<p>               Bases: <code>ModelRegistryError</code></p> <p>Base class for configuration-related errors.</p> <p>This is raised for errors related to configuration loading, parsing, or validation.</p> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ConfigurationError(ModelRegistryError):\n    \"\"\"Base class for configuration-related errors.\n\n    This is raised for errors related to configuration loading, parsing,\n    or validation.\n    \"\"\"\n\n    def __init__(self, message: str, path: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize configuration error.\n\n        Args:\n            message: Error message\n            path: Optional path to the configuration file that caused the error\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n        self.path = path\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ConfigurationError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.ConfigurationError.__init__","title":"<code>__init__(message, path=None)</code>","text":"<p>Initialize configuration error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>path</code> <code>Optional[str]</code> <p>Optional path to the configuration file that caused the error</p> <code>None</code> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(self, message: str, path: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize configuration error.\n\n    Args:\n        message: Error message\n        path: Optional path to the configuration file that caused the error\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.path = path\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ConstraintNotFoundError","title":"<code>ConstraintNotFoundError</code>","text":"<p>               Bases: <code>ModelRegistryError</code></p> <p>Raised when a constraint reference cannot be found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     registry.get_parameter_constraint(\"unknown.constraint\")\n... except ConstraintNotFoundError as e:\n...     print(f\"Constraint not found: {e.ref}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ConstraintNotFoundError(ModelRegistryError):\n    \"\"\"Raised when a constraint reference cannot be found.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     registry.get_parameter_constraint(\"unknown.constraint\")\n        ... except ConstraintNotFoundError as e:\n        ...     print(f\"Constraint not found: {e.ref}\")\n    \"\"\"\n\n    def __init__(self, message: str, ref: str) -&gt; None:\n        \"\"\"Initialize constraint not found error.\n\n        Args:\n            message: Error message\n            ref: The constraint reference that wasn't found\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n        self.ref = ref\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ConstraintNotFoundError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.ConstraintNotFoundError.__init__","title":"<code>__init__(message, ref)</code>","text":"<p>Initialize constraint not found error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>ref</code> <code>str</code> <p>The constraint reference that wasn't found</p> required Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(self, message: str, ref: str) -&gt; None:\n    \"\"\"Initialize constraint not found error.\n\n    Args:\n        message: Error message\n        ref: The constraint reference that wasn't found\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.ref = ref\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ConstraintViolation","title":"<code>ConstraintViolation</code>","text":"<p>               Bases: <code>ParameterValidationError</code></p> <p>Raised when a parameter value violates a constraint.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     registry.validate_parameter(\"temperature\", 3.0, \"gpt-4o\")\n... except ConstraintViolation as e:\n...     print(f\"Constraint violated: {e.param} must {e.rule}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ConstraintViolation(ParameterValidationError):\n    \"\"\"Raised when a parameter value violates a constraint.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     registry.validate_parameter(\"temperature\", 3.0, \"gpt-4o\")\n        ... except ConstraintViolation as e:\n        ...     print(f\"Constraint violated: {e.param} must {e.rule}\")\n    \"\"\"\n\n    def __init__(\n        self,\n        param: str,\n        value: Any,\n        rule: str,\n        provider: Optional[str] = None,\n        model: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize constraint violation error.\n\n        Args:\n            param: Parameter name that violated the constraint\n            value: The invalid value\n            rule: Description of the constraint rule\n            provider: Provider name (optional)\n            model: Model name (optional)\n        \"\"\"\n        provider_msg = f\" on provider '{provider}'\" if provider else \"\"\n        message = f\"{param} must {rule} (got {value}){provider_msg}\"\n        super().__init__(message, param, value, model)\n        self.param = param\n        self.value = value\n        self.rule = rule\n        self.provider = provider\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ConstraintViolation-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.ConstraintViolation.__init__","title":"<code>__init__(param, value, rule, provider=None, model=None)</code>","text":"<p>Initialize constraint violation error.</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>str</code> <p>Parameter name that violated the constraint</p> required <code>value</code> <code>Any</code> <p>The invalid value</p> required <code>rule</code> <code>str</code> <p>Description of the constraint rule</p> required <code>provider</code> <code>Optional[str]</code> <p>Provider name (optional)</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model name (optional)</p> <code>None</code> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(\n    self,\n    param: str,\n    value: Any,\n    rule: str,\n    provider: Optional[str] = None,\n    model: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize constraint violation error.\n\n    Args:\n        param: Parameter name that violated the constraint\n        value: The invalid value\n        rule: Description of the constraint rule\n        provider: Provider name (optional)\n        model: Model name (optional)\n    \"\"\"\n    provider_msg = f\" on provider '{provider}'\" if provider else \"\"\n    message = f\"{param} must {rule} (got {value}){provider_msg}\"\n    super().__init__(message, param, value, model)\n    self.param = param\n    self.value = value\n    self.rule = rule\n    self.provider = provider\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.InvalidConfigFormatError","title":"<code>InvalidConfigFormatError</code>","text":"<p>               Bases: <code>ConfigurationError</code></p> <p>Raised when a configuration file has an invalid format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     registry._load_config()\n... except InvalidConfigFormatError as e:\n...     print(f\"Invalid config format: {e}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class InvalidConfigFormatError(ConfigurationError):\n    \"\"\"Raised when a configuration file has an invalid format.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     registry._load_config()\n        ... except InvalidConfigFormatError as e:\n        ...     print(f\"Invalid config format: {e}\")\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        path: Optional[str] = None,\n        expected_type: str = \"dict\",\n    ) -&gt; None:\n        \"\"\"Initialize invalid format error.\n\n        Args:\n            message: Error message\n            path: Optional path to the configuration file\n            expected_type: Expected type of the configuration\n        \"\"\"\n        super().__init__(message, path)\n        self.expected_type = expected_type\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.InvalidConfigFormatError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.InvalidConfigFormatError.__init__","title":"<code>__init__(message, path=None, expected_type='dict')</code>","text":"<p>Initialize invalid format error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>path</code> <code>Optional[str]</code> <p>Optional path to the configuration file</p> <code>None</code> <code>expected_type</code> <code>str</code> <p>Expected type of the configuration</p> <code>'dict'</code> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    path: Optional[str] = None,\n    expected_type: str = \"dict\",\n) -&gt; None:\n    \"\"\"Initialize invalid format error.\n\n    Args:\n        message: Error message\n        path: Optional path to the configuration file\n        expected_type: Expected type of the configuration\n    \"\"\"\n    super().__init__(message, path)\n    self.expected_type = expected_type\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.InvalidDateError","title":"<code>InvalidDateError</code>","text":"<p>               Bases: <code>ModelVersionError</code></p> <p>Raised when a model version has an invalid date format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     ModelVersion.from_string(\"2024-13-01\")\n... except InvalidDateError as e:\n...     print(f\"Date format error: {e}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class InvalidDateError(ModelVersionError):\n    \"\"\"Raised when a model version has an invalid date format.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     ModelVersion.from_string(\"2024-13-01\")\n        ... except InvalidDateError as e:\n        ...     print(f\"Date format error: {e}\")\n    \"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        \"\"\"Initialize invalid date error.\n\n        Args:\n            message: Error message explaining the date format issue\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.InvalidDateError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.InvalidDateError.__init__","title":"<code>__init__(message)</code>","text":"<p>Initialize invalid date error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message explaining the date format issue</p> required Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(self, message: str) -&gt; None:\n    \"\"\"Initialize invalid date error.\n\n    Args:\n        message: Error message explaining the date format issue\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ModelFormatError","title":"<code>ModelFormatError</code>","text":"<p>               Bases: <code>ModelRegistryError</code></p> <p>Raised when a model name has an invalid format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     ModelVersion.parse_from_model(\"invalid-model-name\")\n... except ModelFormatError as e:\n...     print(f\"Invalid model format: {e.model}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ModelFormatError(ModelRegistryError):\n    \"\"\"Raised when a model name has an invalid format.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     ModelVersion.parse_from_model(\"invalid-model-name\")\n        ... except ModelFormatError as e:\n        ...     print(f\"Invalid model format: {e.model}\")\n    \"\"\"\n\n    def __init__(self, message: str, model: str) -&gt; None:\n        \"\"\"Initialize model format error.\n\n        Args:\n            message: Error message\n            model: The invalid model name\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n        self.model = model\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ModelFormatError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.ModelFormatError.__init__","title":"<code>__init__(message, model)</code>","text":"<p>Initialize model format error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>model</code> <code>str</code> <p>The invalid model name</p> required Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(self, message: str, model: str) -&gt; None:\n    \"\"\"Initialize model format error.\n\n    Args:\n        message: Error message\n        model: The invalid model name\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.model = model\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ModelNotSupportedError","title":"<code>ModelNotSupportedError</code>","text":"<p>               Bases: <code>ModelRegistryError</code></p> <p>Raised when a model is not supported by the registry.</p> <p>This error indicates that the requested model is not in the registry of supported models. This is different from version-related errors, which indicate that the model exists but the specific version is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     registry.get_capabilities(\"unsupported-model\")\n... except ModelNotSupportedError as e:\n...     print(f\"Model {e.model} is not supported\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ModelNotSupportedError(ModelRegistryError):\n    \"\"\"Raised when a model is not supported by the registry.\n\n    This error indicates that the requested model is not in the registry of\n    supported models. This is different from version-related errors, which\n    indicate that the model exists but the specific version is invalid.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     registry.get_capabilities(\"unsupported-model\")\n        ... except ModelNotSupportedError as e:\n        ...     print(f\"Model {e.model} is not supported\")\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        model: Optional[str] = None,\n        available_models: Optional[Union[List[str], Set[str], Dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize model not supported error.\n\n        Args:\n            message: Error message\n            model: The unsupported model name\n            available_models: List of available models (optional)\n        \"\"\"\n        super().__init__(message)\n        self.model = model\n        self.message = message\n        # Convert other collection types to list for consistency\n        if available_models is not None:\n            if isinstance(available_models, dict):\n                self.available_models: Optional[List[str]] = list(available_models.keys())\n            elif isinstance(available_models, set):\n                self.available_models = list(available_models)\n            else:\n                self.available_models = available_models\n        else:\n            self.available_models = None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return string representation of the error.\n\n        Returns:\n            Error message\n        \"\"\"\n        return self.message\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ModelNotSupportedError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.ModelNotSupportedError.__init__","title":"<code>__init__(message, model=None, available_models=None)</code>","text":"<p>Initialize model not supported error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>model</code> <code>Optional[str]</code> <p>The unsupported model name</p> <code>None</code> <code>available_models</code> <code>Optional[Union[List[str], Set[str], Dict[str, Any]]]</code> <p>List of available models (optional)</p> <code>None</code> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    model: Optional[str] = None,\n    available_models: Optional[Union[List[str], Set[str], Dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"Initialize model not supported error.\n\n    Args:\n        message: Error message\n        model: The unsupported model name\n        available_models: List of available models (optional)\n    \"\"\"\n    super().__init__(message)\n    self.model = model\n    self.message = message\n    # Convert other collection types to list for consistency\n    if available_models is not None:\n        if isinstance(available_models, dict):\n            self.available_models: Optional[List[str]] = list(available_models.keys())\n        elif isinstance(available_models, set):\n            self.available_models = list(available_models)\n        else:\n            self.available_models = available_models\n    else:\n        self.available_models = None\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ModelNotSupportedError.__str__","title":"<code>__str__()</code>","text":"<p>Return string representation of the error.</p> <p>Returns:</p> Type Description <code>str</code> <p>Error message</p> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return string representation of the error.\n\n    Returns:\n        Error message\n    \"\"\"\n    return self.message\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ModelRegistryError","title":"<code>ModelRegistryError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for all registry-related errors.</p> <p>This is the parent class for all registry-specific exceptions.</p> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ModelRegistryError(Exception):\n    \"\"\"Base class for all registry-related errors.\n\n    This is the parent class for all registry-specific exceptions.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ModelVersionError","title":"<code>ModelVersionError</code>","text":"<p>               Bases: <code>ModelRegistryError</code></p> <p>Base class for version-related errors.</p> <p>This is raised for any errors related to model versions.</p> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ModelVersionError(ModelRegistryError):\n    \"\"\"Base class for version-related errors.\n\n    This is raised for any errors related to model versions.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.NetworkError","title":"<code>NetworkError</code>","text":"<p>               Bases: <code>ModelRegistryError</code></p> <p>Raised when a network operation fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     registry.refresh_from_remote()\n... except NetworkError as e:\n...     print(f\"Network error: {e}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class NetworkError(ModelRegistryError):\n    \"\"\"Raised when a network operation fails.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     registry.refresh_from_remote()\n        ... except NetworkError as e:\n        ...     print(f\"Network error: {e}\")\n    \"\"\"\n\n    def __init__(self, message: str, url: Optional[str] = None) -&gt; None:\n        \"\"\"Initialize network error.\n\n        Args:\n            message: Error message\n            url: Optional URL that was being accessed\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n        self.url = url\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.NetworkError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.NetworkError.__init__","title":"<code>__init__(message, url=None)</code>","text":"<p>Initialize network error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>url</code> <code>Optional[str]</code> <p>Optional URL that was being accessed</p> <code>None</code> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(self, message: str, url: Optional[str] = None) -&gt; None:\n    \"\"\"Initialize network error.\n\n    Args:\n        message: Error message\n        url: Optional URL that was being accessed\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.url = url\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ParameterNotSupportedError","title":"<code>ParameterNotSupportedError</code>","text":"<p>               Bases: <code>ParameterValidationError</code></p> <p>Raised when a parameter is not supported for a model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     capabilities.validate_parameter(\"unknown_param\", \"value\")\n... except ParameterNotSupportedError as e:\n...     print(f\"Parameter {e.param_name} not supported for {e.model}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ParameterNotSupportedError(ParameterValidationError):\n    \"\"\"Raised when a parameter is not supported for a model.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     capabilities.validate_parameter(\"unknown_param\", \"value\")\n        ... except ParameterNotSupportedError as e:\n        ...     print(f\"Parameter {e.param_name} not supported for {e.model}\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ParameterValidationError","title":"<code>ParameterValidationError</code>","text":"<p>               Bases: <code>ModelRegistryError</code></p> <p>Base class for parameter validation errors.</p> <p>This is raised for errors related to parameter validation.</p> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class ParameterValidationError(ModelRegistryError):\n    \"\"\"Base class for parameter validation errors.\n\n    This is raised for errors related to parameter validation.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        param_name: str,\n        value: Any,\n        model: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize parameter validation error.\n\n        Args:\n            message: Error message\n            param_name: The name of the parameter being validated\n            value: The value that failed validation\n            model: Optional model name for context\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n        self.param_name = param_name\n        self.value = value\n        self.model = model\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.ParameterValidationError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.ParameterValidationError.__init__","title":"<code>__init__(message, param_name, value, model=None)</code>","text":"<p>Initialize parameter validation error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>param_name</code> <code>str</code> <p>The name of the parameter being validated</p> required <code>value</code> <code>Any</code> <p>The value that failed validation</p> required <code>model</code> <code>Optional[str]</code> <p>Optional model name for context</p> <code>None</code> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    param_name: str,\n    value: Any,\n    model: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize parameter validation error.\n\n    Args:\n        message: Error message\n        param_name: The name of the parameter being validated\n        value: The value that failed validation\n        model: Optional model name for context\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.param_name = param_name\n    self.value = value\n    self.model = model\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.TokenParameterError","title":"<code>TokenParameterError</code>","text":"<p>               Bases: <code>ParameterValidationError</code></p> <p>Raised when there's an issue with token-related parameters.</p> <p>This error is used for issues with max_tokens, completion_tokens, etc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     capabilities.validate_parameter(\"max_completion_tokens\", 100000)\n... except TokenParameterError as e:\n...     print(f\"Token error: {e}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class TokenParameterError(ParameterValidationError):\n    \"\"\"Raised when there's an issue with token-related parameters.\n\n    This error is used for issues with max_tokens, completion_tokens, etc.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     capabilities.validate_parameter(\"max_completion_tokens\", 100000)\n        ... except TokenParameterError as e:\n        ...     print(f\"Token error: {e}\")\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.VersionTooOldError","title":"<code>VersionTooOldError</code>","text":"<p>               Bases: <code>ModelVersionError</code></p> <p>Raised when a model version is older than the minimum supported version.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; try:\n...     registry.get_capabilities(\"gpt-4o-2024-07-01\")\n... except VersionTooOldError as e:\n...     print(f\"Version error: {e}\")\n...     print(f\"Try using the alias: {e.alias}\")\n</code></pre> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>class VersionTooOldError(ModelVersionError):\n    \"\"\"Raised when a model version is older than the minimum supported version.\n\n    Examples:\n        &gt;&gt;&gt; try:\n        ...     registry.get_capabilities(\"gpt-4o-2024-07-01\")\n        ... except VersionTooOldError as e:\n        ...     print(f\"Version error: {e}\")\n        ...     print(f\"Try using the alias: {e.alias}\")\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        model: str,\n        min_version: str,\n        alias: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Initialize version too old error.\n\n        Args:\n            message: Error message\n            model: The model that has a version too old\n            min_version: The minimum supported version\n            alias: Suggested alias to use instead (if available)\n        \"\"\"\n        super().__init__(message)\n        self.message = message\n        self.model = model\n        self.min_version = min_version\n        self.alias = alias\n</code></pre>"},{"location":"api_reference/errors/#openai_model_registry.errors.VersionTooOldError-functions","title":"Functions","text":""},{"location":"api_reference/errors/#openai_model_registry.errors.VersionTooOldError.__init__","title":"<code>__init__(message, model, min_version, alias=None)</code>","text":"<p>Initialize version too old error.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>Error message</p> required <code>model</code> <code>str</code> <p>The model that has a version too old</p> required <code>min_version</code> <code>str</code> <p>The minimum supported version</p> required <code>alias</code> <code>Optional[str]</code> <p>Suggested alias to use instead (if available)</p> <code>None</code> Source code in <code>src/openai_model_registry/errors.py</code> <pre><code>def __init__(\n    self,\n    message: str,\n    model: str,\n    min_version: str,\n    alias: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Initialize version too old error.\n\n    Args:\n        message: Error message\n        model: The model that has a version too old\n        min_version: The minimum supported version\n        alias: Suggested alias to use instead (if available)\n    \"\"\"\n    super().__init__(message)\n    self.message = message\n    self.model = model\n    self.min_version = min_version\n    self.alias = alias\n</code></pre>"},{"location":"api_reference/logging/","title":"Logging","text":""},{"location":"api_reference/logging/#openai_model_registry.logging","title":"<code>openai_model_registry.logging</code>","text":"<p>Logging utilities for the model registry.</p> <p>This module provides standardized logging functionality for registry operations.</p>"},{"location":"api_reference/logging/#openai_model_registry.logging-classes","title":"Classes","text":""},{"location":"api_reference/logging/#openai_model_registry.logging.LogEvent","title":"<code>LogEvent</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Event types for registry logging.</p> Source code in <code>src/openai_model_registry/logging.py</code> <pre><code>class LogEvent(str, Enum):\n    \"\"\"Event types for registry logging.\"\"\"\n\n    MODEL_REGISTRY = \"model_registry\"\n    MODEL_CAPABILITIES = \"model_capabilities\"\n    PARAMETER_VALIDATION = \"parameter_validation\"\n    VERSION_VALIDATION = \"version_validation\"\n    REGISTRY_UPDATE = \"registry_update\"\n</code></pre>"},{"location":"api_reference/logging/#openai_model_registry.logging-functions","title":"Functions","text":""},{"location":"api_reference/logging/#openai_model_registry.logging.get_logger","title":"<code>get_logger(name=None)</code>","text":"<p>Get a logger for a module.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Optional name for the logger. If None, returns the root package logger.</p> <code>None</code> <p>Returns:</p> Type Description <code>Logger</code> <p>A configured logger instance</p> Source code in <code>src/openai_model_registry/logging.py</code> <pre><code>def get_logger(name: Optional[str] = None) -&gt; logging.Logger:\n    \"\"\"Get a logger for a module.\n\n    Args:\n        name: Optional name for the logger. If None, returns the root package logger.\n\n    Returns:\n        A configured logger instance\n    \"\"\"\n    if name:\n        return logging.getLogger(f\"openai_model_registry.{name}\")\n    return logger\n</code></pre>"},{"location":"api_reference/logging/#openai_model_registry.logging.log_critical","title":"<code>log_critical(event, message, **kwargs)</code>","text":"<p>Log a critical message.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LogEvent</code> <p>The event type</p> required <code>message</code> <code>str</code> <p>The message to log</p> required <code>**kwargs</code> <code>Any</code> <p>Additional data to include in the log</p> <code>{}</code> Source code in <code>src/openai_model_registry/logging.py</code> <pre><code>def log_critical(event: LogEvent, message: str, **kwargs: Any) -&gt; None:\n    \"\"\"Log a critical message.\n\n    Args:\n        event: The event type\n        message: The message to log\n        **kwargs: Additional data to include in the log\n    \"\"\"\n    extra = {\"event\": str(event), **kwargs}\n    logger.critical(f\"{message}\", extra=extra)\n</code></pre>"},{"location":"api_reference/logging/#openai_model_registry.logging.log_debug","title":"<code>log_debug(event, message, **kwargs)</code>","text":"<p>Log a debug message.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LogEvent</code> <p>The event type</p> required <code>message</code> <code>str</code> <p>The message to log</p> required <code>**kwargs</code> <code>Any</code> <p>Additional data to include in the log</p> <code>{}</code> Source code in <code>src/openai_model_registry/logging.py</code> <pre><code>def log_debug(event: LogEvent, message: str, **kwargs: Any) -&gt; None:\n    \"\"\"Log a debug message.\n\n    Args:\n        event: The event type\n        message: The message to log\n        **kwargs: Additional data to include in the log\n    \"\"\"\n    extra = {\"event\": str(event), **kwargs}\n    logger.debug(f\"{message}\", extra=extra)\n</code></pre>"},{"location":"api_reference/logging/#openai_model_registry.logging.log_error","title":"<code>log_error(event, message, **kwargs)</code>","text":"<p>Log an error message.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LogEvent</code> <p>The event type</p> required <code>message</code> <code>str</code> <p>The message to log</p> required <code>**kwargs</code> <code>Any</code> <p>Additional data to include in the log</p> <code>{}</code> Source code in <code>src/openai_model_registry/logging.py</code> <pre><code>def log_error(event: LogEvent, message: str, **kwargs: Any) -&gt; None:\n    \"\"\"Log an error message.\n\n    Args:\n        event: The event type\n        message: The message to log\n        **kwargs: Additional data to include in the log\n    \"\"\"\n    extra = {\"event\": str(event), **kwargs}\n    logger.error(f\"{message}\", extra=extra)\n</code></pre>"},{"location":"api_reference/logging/#openai_model_registry.logging.log_info","title":"<code>log_info(event, message, **kwargs)</code>","text":"<p>Log an info message.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LogEvent</code> <p>The event type</p> required <code>message</code> <code>str</code> <p>The message to log</p> required <code>**kwargs</code> <code>Any</code> <p>Additional data to include in the log</p> <code>{}</code> Source code in <code>src/openai_model_registry/logging.py</code> <pre><code>def log_info(event: LogEvent, message: str, **kwargs: Any) -&gt; None:\n    \"\"\"Log an info message.\n\n    Args:\n        event: The event type\n        message: The message to log\n        **kwargs: Additional data to include in the log\n    \"\"\"\n    extra = {\"event\": str(event), **kwargs}\n    logger.info(f\"{message}\", extra=extra)\n</code></pre>"},{"location":"api_reference/logging/#openai_model_registry.logging.log_warning","title":"<code>log_warning(event, message, **kwargs)</code>","text":"<p>Log a warning message.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LogEvent</code> <p>The event type</p> required <code>message</code> <code>str</code> <p>The message to log</p> required <code>**kwargs</code> <code>Any</code> <p>Additional data to include in the log</p> <code>{}</code> Source code in <code>src/openai_model_registry/logging.py</code> <pre><code>def log_warning(event: LogEvent, message: str, **kwargs: Any) -&gt; None:\n    \"\"\"Log a warning message.\n\n    Args:\n        event: The event type\n        message: The message to log\n        **kwargs: Additional data to include in the log\n    \"\"\"\n    extra = {\"event\": str(event), **kwargs}\n    logger.warning(f\"{message}\", extra=extra)\n</code></pre>"},{"location":"api_reference/model_version/","title":"Model version","text":""},{"location":"api_reference/model_version/#openai_model_registry.model_version","title":"<code>openai_model_registry.model_version</code>","text":"<p>Model version handling for OpenAI models.</p> <p>This module provides utilities for parsing and comparing model versions in the format YYYY-MM-DD.</p>"},{"location":"api_reference/model_version/#openai_model_registry.model_version-classes","title":"Classes","text":""},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion","title":"<code>ModelVersion</code>","text":"<p>Represents a model version in YYYY-MM-DD format.</p> <p>This class handles parsing, validation, and comparison of model version dates.</p> <p>Attributes:</p> Name Type Description <code>year</code> <p>The year component of the version</p> <code>month</code> <p>The month component of the version</p> <code>day</code> <p>The day component of the version</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>class ModelVersion:\n    \"\"\"Represents a model version in YYYY-MM-DD format.\n\n    This class handles parsing, validation, and comparison of model version dates.\n\n    Attributes:\n        year: The year component of the version\n        month: The month component of the version\n        day: The day component of the version\n    \"\"\"\n\n    def __init__(self, year: int, month: int, day: int) -&gt; None:\n        \"\"\"Initialize a model version.\n\n        Args:\n            year: The year component (e.g., 2024)\n            month: The month component (1-12)\n            day: The day component (1-31)\n        \"\"\"\n        self.year = year\n        self.month = month\n        self.day = day\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"Check if two versions are equal.\n\n        Args:\n            other: The other version to compare with\n\n        Returns:\n            bool: True if versions are equal, False otherwise\n        \"\"\"\n        if not isinstance(other, ModelVersion):\n            return NotImplemented\n        return self.year == other.year and self.month == other.month and self.day == other.day\n\n    def __lt__(self, other: \"ModelVersion\") -&gt; bool:\n        \"\"\"Check if this version is earlier than another.\n\n        Args:\n            other: The other version to compare with\n\n        Returns:\n            bool: True if this version is earlier, False otherwise\n        \"\"\"\n        if self.year != other.year:\n            return self.year &lt; other.year\n        if self.month != other.month:\n            return self.month &lt; other.month\n        return self.day &lt; other.day\n\n    def __le__(self, other: \"ModelVersion\") -&gt; bool:\n        \"\"\"Check if this version is earlier than or equal to another.\n\n        Args:\n            other: The other version to compare with\n\n        Returns:\n            bool: True if this version is earlier or equal, False otherwise\n        \"\"\"\n        return self &lt; other or self == other\n\n    def __gt__(self, other: \"ModelVersion\") -&gt; bool:\n        \"\"\"Check if this version is later than another.\n\n        Args:\n            other: The other version to compare with\n\n        Returns:\n            bool: True if this version is later, False otherwise\n        \"\"\"\n        return not (self &lt;= other)\n\n    def __ge__(self, other: \"ModelVersion\") -&gt; bool:\n        \"\"\"Check if this version is later than or equal to another.\n\n        Args:\n            other: The other version to compare with\n\n        Returns:\n            bool: True if this version is later or equal, False otherwise\n        \"\"\"\n        return not (self &lt; other)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Get string representation of the version.\n\n        Returns:\n            str: Version in YYYY-MM-DD format\n        \"\"\"\n        return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n\n    @classmethod\n    def from_string(cls, version_str: str) -&gt; \"ModelVersion\":\n        \"\"\"Create a version from a string in YYYY-MM-DD format.\n\n        Args:\n            version_str: The version string to parse\n\n        Returns:\n            A new ModelVersion instance\n\n        Raises:\n            InvalidDateError: If the string is not a valid version\n        \"\"\"\n        # Strip whitespace and validate input\n        version_str = version_str.strip()\n        if not version_str:\n            raise InvalidDateError(\"Version string cannot be empty\")\n\n        # Check for reasonable string length (prevent excessive input)\n        if len(version_str) &gt; 50:\n            raise InvalidDateError(f\"Version string too long: {len(version_str)} characters. Maximum allowed: 50\")\n\n        parts = version_str.split(\"-\")\n        if len(parts) != 3:\n            raise InvalidDateError(f\"Invalid version format: {version_str}. \" f\"Expected YYYY-MM-DD.\")\n\n        try:\n            year = int(parts[0])\n            month = int(parts[1])\n            day = int(parts[2])\n        except ValueError:\n            raise InvalidDateError(\n                f\"Invalid version components in {version_str}. \" f\"Year, month, and day must be integers.\"\n            )\n\n        # Basic validation\n        if not (1000 &lt;= year &lt;= 9999):\n            raise InvalidDateError(f\"Invalid year: {year}. Must be 1000-9999.\")\n        if not (1 &lt;= month &lt;= 12):\n            raise InvalidDateError(f\"Invalid month: {month}. Must be 1-12.\")\n        if not (1 &lt;= day &lt;= 31):\n            raise InvalidDateError(f\"Invalid day: {day}. Must be 1-31.\")\n\n        # Calendar validation\n        try:\n            date(year, month, day)\n        except ValueError as e:\n            raise InvalidDateError(f\"Invalid date: {version_str}. {str(e)}\")\n\n        return cls(year, month, day)\n\n    @staticmethod\n    def parse_from_model(model: str) -&gt; Tuple[str, \"ModelVersion\"]:\n        \"\"\"Parse a model name into base name and version.\n\n        Args:\n            model: Full model name with version (e.g., \"gpt-4o-2024-08-06\")\n\n        Returns:\n            Tuple of (base_name, version)\n            Example: (\"gpt-4o\", ModelVersion(2024, 8, 6))\n\n        Raises:\n            ModelFormatError: If the model name does not follow the expected format\n            InvalidDateError: If the date part of the model name is invalid\n        \"\"\"\n        # Format: \"{base_model}-{YYYY}-{MM}-{DD}\"\n        pattern = re.compile(r\"^([\\w-]+?)-(\\d{4}-\\d{2}-\\d{2})$\")\n        match = pattern.match(model)\n\n        if not match:\n            raise ModelFormatError(\n                f\"Invalid model format: {model}. Expected format: \" f\"base-name-YYYY-MM-DD (e.g., gpt-4o-2024-08-06)\",\n                model=model,\n            )\n\n        base_model = match.group(1)\n        version_str = match.group(2)\n\n        try:\n            version = ModelVersion.from_string(version_str)\n            return base_model, version\n        except InvalidDateError as e:\n            raise ModelFormatError(\n                f\"Invalid version in model name {model}: {e}\",\n                model=model,\n            ) from e\n\n    @staticmethod\n    def is_dated_model(model_name: str) -&gt; bool:\n        \"\"\"Check if a model name follows the dated model format.\n\n        Args:\n            model_name: The model name to check\n\n        Returns:\n            True if the model name follows the dated format (with YYYY-MM-DD suffix)\n        \"\"\"\n        return bool(re.match(r\"^.*-\\d{4}-\\d{2}-\\d{2}$\", model_name))\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion-functions","title":"Functions","text":""},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check if two versions are equal.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>The other version to compare with</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if versions are equal, False otherwise</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"Check if two versions are equal.\n\n    Args:\n        other: The other version to compare with\n\n    Returns:\n        bool: True if versions are equal, False otherwise\n    \"\"\"\n    if not isinstance(other, ModelVersion):\n        return NotImplemented\n    return self.year == other.year and self.month == other.month and self.day == other.day\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.__ge__","title":"<code>__ge__(other)</code>","text":"<p>Check if this version is later than or equal to another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ModelVersion</code> <p>The other version to compare with</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if this version is later or equal, False otherwise</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>def __ge__(self, other: \"ModelVersion\") -&gt; bool:\n    \"\"\"Check if this version is later than or equal to another.\n\n    Args:\n        other: The other version to compare with\n\n    Returns:\n        bool: True if this version is later or equal, False otherwise\n    \"\"\"\n    return not (self &lt; other)\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.__gt__","title":"<code>__gt__(other)</code>","text":"<p>Check if this version is later than another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ModelVersion</code> <p>The other version to compare with</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if this version is later, False otherwise</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>def __gt__(self, other: \"ModelVersion\") -&gt; bool:\n    \"\"\"Check if this version is later than another.\n\n    Args:\n        other: The other version to compare with\n\n    Returns:\n        bool: True if this version is later, False otherwise\n    \"\"\"\n    return not (self &lt;= other)\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.__init__","title":"<code>__init__(year, month, day)</code>","text":"<p>Initialize a model version.</p> <p>Parameters:</p> Name Type Description Default <code>year</code> <code>int</code> <p>The year component (e.g., 2024)</p> required <code>month</code> <code>int</code> <p>The month component (1-12)</p> required <code>day</code> <code>int</code> <p>The day component (1-31)</p> required Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>def __init__(self, year: int, month: int, day: int) -&gt; None:\n    \"\"\"Initialize a model version.\n\n    Args:\n        year: The year component (e.g., 2024)\n        month: The month component (1-12)\n        day: The day component (1-31)\n    \"\"\"\n    self.year = year\n    self.month = month\n    self.day = day\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.__le__","title":"<code>__le__(other)</code>","text":"<p>Check if this version is earlier than or equal to another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ModelVersion</code> <p>The other version to compare with</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if this version is earlier or equal, False otherwise</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>def __le__(self, other: \"ModelVersion\") -&gt; bool:\n    \"\"\"Check if this version is earlier than or equal to another.\n\n    Args:\n        other: The other version to compare with\n\n    Returns:\n        bool: True if this version is earlier or equal, False otherwise\n    \"\"\"\n    return self &lt; other or self == other\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Check if this version is earlier than another.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>ModelVersion</code> <p>The other version to compare with</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if this version is earlier, False otherwise</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>def __lt__(self, other: \"ModelVersion\") -&gt; bool:\n    \"\"\"Check if this version is earlier than another.\n\n    Args:\n        other: The other version to compare with\n\n    Returns:\n        bool: True if this version is earlier, False otherwise\n    \"\"\"\n    if self.year != other.year:\n        return self.year &lt; other.year\n    if self.month != other.month:\n        return self.month &lt; other.month\n    return self.day &lt; other.day\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.__repr__","title":"<code>__repr__()</code>","text":"<p>Get string representation of the version.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Version in YYYY-MM-DD format</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Get string representation of the version.\n\n    Returns:\n        str: Version in YYYY-MM-DD format\n    \"\"\"\n    return f\"{self.year:04d}-{self.month:02d}-{self.day:02d}\"\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.from_string","title":"<code>from_string(version_str)</code>  <code>classmethod</code>","text":"<p>Create a version from a string in YYYY-MM-DD format.</p> <p>Parameters:</p> Name Type Description Default <code>version_str</code> <code>str</code> <p>The version string to parse</p> required <p>Returns:</p> Type Description <code>ModelVersion</code> <p>A new ModelVersion instance</p> <p>Raises:</p> Type Description <code>InvalidDateError</code> <p>If the string is not a valid version</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>@classmethod\ndef from_string(cls, version_str: str) -&gt; \"ModelVersion\":\n    \"\"\"Create a version from a string in YYYY-MM-DD format.\n\n    Args:\n        version_str: The version string to parse\n\n    Returns:\n        A new ModelVersion instance\n\n    Raises:\n        InvalidDateError: If the string is not a valid version\n    \"\"\"\n    # Strip whitespace and validate input\n    version_str = version_str.strip()\n    if not version_str:\n        raise InvalidDateError(\"Version string cannot be empty\")\n\n    # Check for reasonable string length (prevent excessive input)\n    if len(version_str) &gt; 50:\n        raise InvalidDateError(f\"Version string too long: {len(version_str)} characters. Maximum allowed: 50\")\n\n    parts = version_str.split(\"-\")\n    if len(parts) != 3:\n        raise InvalidDateError(f\"Invalid version format: {version_str}. \" f\"Expected YYYY-MM-DD.\")\n\n    try:\n        year = int(parts[0])\n        month = int(parts[1])\n        day = int(parts[2])\n    except ValueError:\n        raise InvalidDateError(\n            f\"Invalid version components in {version_str}. \" f\"Year, month, and day must be integers.\"\n        )\n\n    # Basic validation\n    if not (1000 &lt;= year &lt;= 9999):\n        raise InvalidDateError(f\"Invalid year: {year}. Must be 1000-9999.\")\n    if not (1 &lt;= month &lt;= 12):\n        raise InvalidDateError(f\"Invalid month: {month}. Must be 1-12.\")\n    if not (1 &lt;= day &lt;= 31):\n        raise InvalidDateError(f\"Invalid day: {day}. Must be 1-31.\")\n\n    # Calendar validation\n    try:\n        date(year, month, day)\n    except ValueError as e:\n        raise InvalidDateError(f\"Invalid date: {version_str}. {str(e)}\")\n\n    return cls(year, month, day)\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.is_dated_model","title":"<code>is_dated_model(model_name)</code>  <code>staticmethod</code>","text":"<p>Check if a model name follows the dated model format.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model name to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the model name follows the dated format (with YYYY-MM-DD suffix)</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>@staticmethod\ndef is_dated_model(model_name: str) -&gt; bool:\n    \"\"\"Check if a model name follows the dated model format.\n\n    Args:\n        model_name: The model name to check\n\n    Returns:\n        True if the model name follows the dated format (with YYYY-MM-DD suffix)\n    \"\"\"\n    return bool(re.match(r\"^.*-\\d{4}-\\d{2}-\\d{2}$\", model_name))\n</code></pre>"},{"location":"api_reference/model_version/#openai_model_registry.model_version.ModelVersion.parse_from_model","title":"<code>parse_from_model(model)</code>  <code>staticmethod</code>","text":"<p>Parse a model name into base name and version.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Full model name with version (e.g., \"gpt-4o-2024-08-06\")</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>Tuple of (base_name, version)</p> <code>Example</code> <code>ModelVersion</code> <p>(\"gpt-4o\", ModelVersion(2024, 8, 6))</p> <p>Raises:</p> Type Description <code>ModelFormatError</code> <p>If the model name does not follow the expected format</p> <code>InvalidDateError</code> <p>If the date part of the model name is invalid</p> Source code in <code>src/openai_model_registry/model_version.py</code> <pre><code>@staticmethod\ndef parse_from_model(model: str) -&gt; Tuple[str, \"ModelVersion\"]:\n    \"\"\"Parse a model name into base name and version.\n\n    Args:\n        model: Full model name with version (e.g., \"gpt-4o-2024-08-06\")\n\n    Returns:\n        Tuple of (base_name, version)\n        Example: (\"gpt-4o\", ModelVersion(2024, 8, 6))\n\n    Raises:\n        ModelFormatError: If the model name does not follow the expected format\n        InvalidDateError: If the date part of the model name is invalid\n    \"\"\"\n    # Format: \"{base_model}-{YYYY}-{MM}-{DD}\"\n    pattern = re.compile(r\"^([\\w-]+?)-(\\d{4}-\\d{2}-\\d{2})$\")\n    match = pattern.match(model)\n\n    if not match:\n        raise ModelFormatError(\n            f\"Invalid model format: {model}. Expected format: \" f\"base-name-YYYY-MM-DD (e.g., gpt-4o-2024-08-06)\",\n            model=model,\n        )\n\n    base_model = match.group(1)\n    version_str = match.group(2)\n\n    try:\n        version = ModelVersion.from_string(version_str)\n        return base_model, version\n    except InvalidDateError as e:\n        raise ModelFormatError(\n            f\"Invalid version in model name {model}: {e}\",\n            model=model,\n        ) from e\n</code></pre>"},{"location":"api_reference/pricing/","title":"Pricing","text":""},{"location":"api_reference/pricing/#openai_model_registry.pricing","title":"<code>openai_model_registry.pricing</code>","text":"<p>Pricing data structures for model registry.</p>"},{"location":"api_reference/pricing/#openai_model_registry.pricing-classes","title":"Classes","text":""},{"location":"api_reference/pricing/#openai_model_registry.pricing.PricingInfo","title":"<code>PricingInfo</code>  <code>dataclass</code>","text":"<p>Unified pricing information for a model.</p> <p>This unified schema supports both token-based and non-token pricing.</p> <ul> <li>scheme: pricing method</li> <li>unit: display/normalization unit</li> <li>input_cost_per_unit / output_cost_per_unit: numeric non-negative costs</li> <li>currency: ISO currency code (default: USD)</li> </ul> Source code in <code>src/openai_model_registry/pricing.py</code> <pre><code>@dataclass(frozen=True)\nclass PricingInfo:\n    \"\"\"Unified pricing information for a model.\n\n    This unified schema supports both token-based and non-token pricing.\n\n    - scheme: pricing method\n    - unit: display/normalization unit\n    - input_cost_per_unit / output_cost_per_unit: numeric non-negative costs\n    - currency: ISO currency code (default: USD)\n    \"\"\"\n\n    scheme: Literal[\"per_token\", \"per_minute\", \"per_image\", \"per_request\"]\n    unit: Literal[\"million_tokens\", \"minute\", \"image\", \"request\"]\n    input_cost_per_unit: float\n    output_cost_per_unit: float\n    currency: str = \"USD\"\n    # Optional tiers for per_image (and future schemes):\n    # [ { quality: str, sizes: [ { size: str, cost_per_image: float } ] }, ... ]\n    tiers: Optional[List[Dict[str, Any]]] = None\n\n    def __post_init__(self) -&gt; None:  # noqa: D401\n        \"\"\"Basic validation ensuring non-negative costs.\"\"\"\n        if self.input_cost_per_unit &lt; 0:\n            raise ValueError(\"Input cost must be non-negative\")\n        if self.output_cost_per_unit &lt; 0:\n            raise ValueError(\"Output cost must be non-negative\")\n        # Basic structure validation for tiers when present\n        if self.tiers is not None:\n            if not isinstance(self.tiers, list):\n                raise ValueError(\"tiers must be a list if provided\")\n</code></pre>"},{"location":"api_reference/pricing/#openai_model_registry.pricing.PricingInfo-functions","title":"Functions","text":""},{"location":"api_reference/pricing/#openai_model_registry.pricing.PricingInfo.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Basic validation ensuring non-negative costs.</p> Source code in <code>src/openai_model_registry/pricing.py</code> <pre><code>def __post_init__(self) -&gt; None:  # noqa: D401\n    \"\"\"Basic validation ensuring non-negative costs.\"\"\"\n    if self.input_cost_per_unit &lt; 0:\n        raise ValueError(\"Input cost must be non-negative\")\n    if self.output_cost_per_unit &lt; 0:\n        raise ValueError(\"Output cost must be non-negative\")\n    # Basic structure validation for tiers when present\n    if self.tiers is not None:\n        if not isinstance(self.tiers, list):\n            raise ValueError(\"tiers must be a list if provided\")\n</code></pre>"},{"location":"api_reference/registry/","title":"Registry","text":""},{"location":"api_reference/registry/#openai_model_registry.registry","title":"<code>openai_model_registry.registry</code>","text":"<p>Core registry functionality for managing OpenAI model capabilities.</p> <p>This module provides the ModelRegistry class, which is the central component for managing model capabilities, version validation, and parameter constraints.</p> <p>This module is the canonical implementation (v1.0 and later). Typical usage:</p> <pre><code>from openai_model_registry import get_registry  # singleton helper\n\n# or, for a custom configuration\nfrom openai_model_registry import ModelRegistry\n</code></pre> <p>Backward-compatibility shims from &lt; v1.0 have been removed.</p>"},{"location":"api_reference/registry/#openai_model_registry.registry-classes","title":"Classes","text":""},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities","title":"<code>ModelCapabilities</code>","text":"<p>Represents the capabilities of a model.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class ModelCapabilities:\n    \"\"\"Represents the capabilities of a model.\"\"\"\n\n    def __init__(\n        self,\n        model_name: str,\n        openai_model_name: str,\n        context_window: int,\n        max_output_tokens: int,\n        deprecation: DeprecationInfo,\n        supports_vision: bool = False,\n        supports_functions: bool = False,\n        supports_streaming: bool = False,\n        supports_structured: bool = False,\n        supports_web_search: bool = False,\n        supports_audio: bool = False,\n        supports_json_mode: bool = False,\n        pricing: Optional[\"PricingInfo\"] = None,\n        input_modalities: Optional[List[str]] = None,\n        output_modalities: Optional[List[str]] = None,\n        min_version: Optional[ModelVersion] = None,\n        aliases: Optional[List[str]] = None,\n        supported_parameters: Optional[List[ParameterReference]] = None,\n        constraints: Optional[Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]]] = None,\n        inline_parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n        web_search_billing: Optional[\"WebSearchBilling\"] = None,\n    ):\n        \"\"\"Initialize model capabilities.\n\n        Args:\n            model_name: The model identifier in the registry\n            openai_model_name: The model name to use with OpenAI API\n            context_window: Maximum context window size in tokens\n            max_output_tokens: Maximum output tokens\n            deprecation: Deprecation metadata (mandatory in current schema)\n            supports_vision: Whether the model supports vision inputs\n            supports_functions: Whether the model supports function calling\n            supports_streaming: Whether the model supports streaming\n            supports_structured: Whether the model supports structured output\n            supports_web_search: Whether the model supports web search\n                (Chat API search-preview models or Responses API tool)\n            supports_audio: Whether the model supports audio inputs\n            supports_json_mode: Whether the model supports JSON mode\n            pricing: Pricing information for the model\n            input_modalities: List of supported input modalities (e.g., [\"text\", \"image\"]).\n            output_modalities: List of supported output modalities (e.g., [\"text\", \"image\"]).\n            min_version: Minimum version for dated model variants\n            aliases: List of aliases for this model\n            supported_parameters: List of parameter references supported by this model\n            constraints: Dictionary of constraints for validation\n            inline_parameters: Dictionary of inline parameter configurations from schema\n            web_search_billing: Optional web-search billing policy and rates for the model\n        \"\"\"\n        self.model_name = model_name\n        self.openai_model_name = openai_model_name\n        self.context_window = context_window\n        self.max_output_tokens = max_output_tokens\n        self.deprecation = deprecation\n        self.supports_vision = supports_vision\n        self.supports_functions = supports_functions\n        self.supports_streaming = supports_streaming\n        self.supports_structured = supports_structured\n        self.supports_web_search = supports_web_search\n        self.supports_audio = supports_audio\n        self.supports_json_mode = supports_json_mode\n        self.pricing = pricing\n        self.input_modalities = input_modalities or []\n        self.output_modalities = output_modalities or []\n        self.min_version = min_version\n        self.aliases = aliases or []\n        self.supported_parameters = supported_parameters or []\n        self._constraints = constraints or {}\n        self._inline_parameters = inline_parameters or {}\n        self.web_search_billing = web_search_billing\n\n    @property\n    def inline_parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Inline parameter definitions for this model (if any).\"\"\"\n        return self._inline_parameters\n\n    @property\n    def is_sunset(self) -&gt; bool:\n        \"\"\"Check if the model is sunset.\"\"\"\n        return self.deprecation.status == \"sunset\"\n\n    @property\n    def is_deprecated(self) -&gt; bool:\n        \"\"\"Check if the model is deprecated or sunset.\"\"\"\n        return self.deprecation.status in [\"deprecated\", \"sunset\"]\n\n    def get_constraint(self, ref: str) -&gt; Optional[Union[NumericConstraint, EnumConstraint, ObjectConstraint]]:\n        \"\"\"Get a constraint by reference.\n\n        Args:\n            ref: Constraint reference (key in constraints dict)\n\n        Returns:\n            The constraint or None if not found\n        \"\"\"\n        return self._constraints.get(ref)\n\n    def validate_parameter(self, name: str, value: Any, used_params: Optional[Set[str]] = None) -&gt; None:\n        \"\"\"Validate a parameter against constraints.\n\n        Args:\n            name: Parameter name\n            value: Parameter value to validate\n            used_params: Optional set to track used parameters\n\n        Raises:\n            ParameterNotSupportedError: If the parameter is not supported\n            ConstraintNotFoundError: If a constraint reference is invalid\n            ModelRegistryError: If validation fails for other reasons\n        \"\"\"\n        # Track used parameters if requested\n        if used_params is not None:\n            used_params.add(name)\n\n        # Check if we have inline parameter constraints\n        if name in self._inline_parameters:\n            self._validate_inline_parameter(name, value)\n            return\n\n        # Find matching parameter reference\n        param_ref = next(\n            (p for p in self.supported_parameters if p.ref == name or p.ref.split(\".\")[-1] == name),\n            None,\n        )\n\n        if not param_ref:\n            # If we're validating a parameter explicitly, it should be supported\n            raise ParameterNotSupportedError(\n                f\"Parameter '{name}' is not supported for model '{self.model_name}'\",\n                param_name=name,\n                value=value,\n                model=self.model_name,\n            )\n\n        constraint = self.get_constraint(param_ref.ref)\n        if not constraint:\n            # If a parameter references a constraint, the constraint should exist\n            raise ConstraintNotFoundError(\n                f\"Constraint reference '{param_ref.ref}' not found for parameter '{name}'\",\n                ref=param_ref.ref,\n            )\n\n        # Validate based on constraint type\n        if isinstance(constraint, NumericConstraint):\n            constraint.validate(name=name, value=value)\n        elif isinstance(constraint, EnumConstraint):\n            constraint.validate(name=name, value=value)\n        elif isinstance(constraint, ObjectConstraint):\n            constraint.validate(name=name, value=value)\n        else:\n            # This shouldn't happen with proper type checking, but just in case\n            raise TypeError(f\"Unknown constraint type for '{name}': {type(constraint).__name__}\")\n\n    def validate_parameters(self, params: Dict[str, Any], used_params: Optional[Set[str]] = None) -&gt; None:\n        \"\"\"Validate multiple parameters against constraints.\n\n        Args:\n            params: Dictionary of parameter names and values to validate\n            used_params: Optional set to track used parameters\n\n        Raises:\n            ModelRegistryError: If validation fails for any parameter\n        \"\"\"\n        for name, value in params.items():\n            self.validate_parameter(name, value, used_params)\n\n    def _validate_inline_parameter(self, name: str, value: Any) -&gt; None:\n        \"\"\"Validate a parameter using inline parameter constraints.\n\n        Args:\n            name: Parameter name\n            value: Parameter value to validate\n\n        Raises:\n            ValidationError: If validation fails\n        \"\"\"\n        from .errors import ParameterValidationError\n\n        param_config = self._inline_parameters[name]\n        param_type = param_config.get(\"type\")\n\n        # Handle numeric parameters (temperature, top_p, etc.)\n        if param_type == \"number\":\n            if not isinstance(value, (int, float)):\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' expects a numeric value\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n            min_val = param_config.get(\"min\")\n            max_val = param_config.get(\"max\")\n\n            if min_val is not None and value &lt; min_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is below minimum {min_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            if max_val is not None and value &gt; max_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is above maximum {max_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n        # Handle canonical numeric schema (min_value/max_value)\n        elif param_type == \"numeric\":\n            allow_float = param_config.get(\"allow_float\", True)\n            allow_int = param_config.get(\"allow_int\", True)\n\n            if not isinstance(value, (int, float)):\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' expects a numeric value\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            # Enforce numeric subtype rules when provided\n            if isinstance(value, float) and not allow_float:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' does not allow float values\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n            if isinstance(value, int) and not allow_int:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' does not allow integer values\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            min_val = param_config.get(\"min_value\")\n            max_val = param_config.get(\"max_value\")\n\n            if min_val is not None and value &lt; min_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is below minimum {min_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            if max_val is not None and value &gt; max_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is above maximum {max_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n        # Handle integer parameters (max_tokens, etc.)\n        elif param_type == \"integer\":\n            if not isinstance(value, int):\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' expects an integer value\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n            # Support both min/max (models.yaml) and min_value/max_value (constraints)\n            min_val = param_config.get(\"min\") or param_config.get(\"min_value\")\n            max_val = param_config.get(\"max\") or param_config.get(\"max_value\")\n\n            if min_val is not None and value &lt; min_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is below minimum {min_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n\n            if max_val is not None and value &gt; max_val:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value {value} is above maximum {max_val}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n        # Handle enum parameters declared inline\n        elif param_type == \"enum\":\n            allowed_values = param_config.get(\"enum\", [])\n            if value not in allowed_values:\n                raise ParameterValidationError(\n                    f\"Parameter '{name}' value '{value}' is not one of: {', '.join(map(str, allowed_values))}\",\n                    param_name=name,\n                    value=value,\n                    model=self.model_name,\n                )\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities-attributes","title":"Attributes","text":""},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities.inline_parameters","title":"<code>inline_parameters</code>  <code>property</code>","text":"<p>Inline parameter definitions for this model (if any).</p>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities.is_deprecated","title":"<code>is_deprecated</code>  <code>property</code>","text":"<p>Check if the model is deprecated or sunset.</p>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities.is_sunset","title":"<code>is_sunset</code>  <code>property</code>","text":"<p>Check if the model is sunset.</p>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities-functions","title":"Functions","text":""},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities.__init__","title":"<code>__init__(model_name, openai_model_name, context_window, max_output_tokens, deprecation, supports_vision=False, supports_functions=False, supports_streaming=False, supports_structured=False, supports_web_search=False, supports_audio=False, supports_json_mode=False, pricing=None, input_modalities=None, output_modalities=None, min_version=None, aliases=None, supported_parameters=None, constraints=None, inline_parameters=None, web_search_billing=None)</code>","text":"<p>Initialize model capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The model identifier in the registry</p> required <code>openai_model_name</code> <code>str</code> <p>The model name to use with OpenAI API</p> required <code>context_window</code> <code>int</code> <p>Maximum context window size in tokens</p> required <code>max_output_tokens</code> <code>int</code> <p>Maximum output tokens</p> required <code>deprecation</code> <code>DeprecationInfo</code> <p>Deprecation metadata (mandatory in current schema)</p> required <code>supports_vision</code> <code>bool</code> <p>Whether the model supports vision inputs</p> <code>False</code> <code>supports_functions</code> <code>bool</code> <p>Whether the model supports function calling</p> <code>False</code> <code>supports_streaming</code> <code>bool</code> <p>Whether the model supports streaming</p> <code>False</code> <code>supports_structured</code> <code>bool</code> <p>Whether the model supports structured output</p> <code>False</code> <code>supports_web_search</code> <code>bool</code> <p>Whether the model supports web search (Chat API search-preview models or Responses API tool)</p> <code>False</code> <code>supports_audio</code> <code>bool</code> <p>Whether the model supports audio inputs</p> <code>False</code> <code>supports_json_mode</code> <code>bool</code> <p>Whether the model supports JSON mode</p> <code>False</code> <code>pricing</code> <code>Optional[PricingInfo]</code> <p>Pricing information for the model</p> <code>None</code> <code>input_modalities</code> <code>Optional[List[str]]</code> <p>List of supported input modalities (e.g., [\"text\", \"image\"]).</p> <code>None</code> <code>output_modalities</code> <code>Optional[List[str]]</code> <p>List of supported output modalities (e.g., [\"text\", \"image\"]).</p> <code>None</code> <code>min_version</code> <code>Optional[ModelVersion]</code> <p>Minimum version for dated model variants</p> <code>None</code> <code>aliases</code> <code>Optional[List[str]]</code> <p>List of aliases for this model</p> <code>None</code> <code>supported_parameters</code> <code>Optional[List[ParameterReference]]</code> <p>List of parameter references supported by this model</p> <code>None</code> <code>constraints</code> <code>Optional[Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]]]</code> <p>Dictionary of constraints for validation</p> <code>None</code> <code>inline_parameters</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Dictionary of inline parameter configurations from schema</p> <code>None</code> <code>web_search_billing</code> <code>Optional[WebSearchBilling]</code> <p>Optional web-search billing policy and rates for the model</p> <code>None</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    openai_model_name: str,\n    context_window: int,\n    max_output_tokens: int,\n    deprecation: DeprecationInfo,\n    supports_vision: bool = False,\n    supports_functions: bool = False,\n    supports_streaming: bool = False,\n    supports_structured: bool = False,\n    supports_web_search: bool = False,\n    supports_audio: bool = False,\n    supports_json_mode: bool = False,\n    pricing: Optional[\"PricingInfo\"] = None,\n    input_modalities: Optional[List[str]] = None,\n    output_modalities: Optional[List[str]] = None,\n    min_version: Optional[ModelVersion] = None,\n    aliases: Optional[List[str]] = None,\n    supported_parameters: Optional[List[ParameterReference]] = None,\n    constraints: Optional[Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]]] = None,\n    inline_parameters: Optional[Dict[str, Dict[str, Any]]] = None,\n    web_search_billing: Optional[\"WebSearchBilling\"] = None,\n):\n    \"\"\"Initialize model capabilities.\n\n    Args:\n        model_name: The model identifier in the registry\n        openai_model_name: The model name to use with OpenAI API\n        context_window: Maximum context window size in tokens\n        max_output_tokens: Maximum output tokens\n        deprecation: Deprecation metadata (mandatory in current schema)\n        supports_vision: Whether the model supports vision inputs\n        supports_functions: Whether the model supports function calling\n        supports_streaming: Whether the model supports streaming\n        supports_structured: Whether the model supports structured output\n        supports_web_search: Whether the model supports web search\n            (Chat API search-preview models or Responses API tool)\n        supports_audio: Whether the model supports audio inputs\n        supports_json_mode: Whether the model supports JSON mode\n        pricing: Pricing information for the model\n        input_modalities: List of supported input modalities (e.g., [\"text\", \"image\"]).\n        output_modalities: List of supported output modalities (e.g., [\"text\", \"image\"]).\n        min_version: Minimum version for dated model variants\n        aliases: List of aliases for this model\n        supported_parameters: List of parameter references supported by this model\n        constraints: Dictionary of constraints for validation\n        inline_parameters: Dictionary of inline parameter configurations from schema\n        web_search_billing: Optional web-search billing policy and rates for the model\n    \"\"\"\n    self.model_name = model_name\n    self.openai_model_name = openai_model_name\n    self.context_window = context_window\n    self.max_output_tokens = max_output_tokens\n    self.deprecation = deprecation\n    self.supports_vision = supports_vision\n    self.supports_functions = supports_functions\n    self.supports_streaming = supports_streaming\n    self.supports_structured = supports_structured\n    self.supports_web_search = supports_web_search\n    self.supports_audio = supports_audio\n    self.supports_json_mode = supports_json_mode\n    self.pricing = pricing\n    self.input_modalities = input_modalities or []\n    self.output_modalities = output_modalities or []\n    self.min_version = min_version\n    self.aliases = aliases or []\n    self.supported_parameters = supported_parameters or []\n    self._constraints = constraints or {}\n    self._inline_parameters = inline_parameters or {}\n    self.web_search_billing = web_search_billing\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities.get_constraint","title":"<code>get_constraint(ref)</code>","text":"<p>Get a constraint by reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>Constraint reference (key in constraints dict)</p> required <p>Returns:</p> Type Description <code>Optional[Union[NumericConstraint, EnumConstraint, ObjectConstraint]]</code> <p>The constraint or None if not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_constraint(self, ref: str) -&gt; Optional[Union[NumericConstraint, EnumConstraint, ObjectConstraint]]:\n    \"\"\"Get a constraint by reference.\n\n    Args:\n        ref: Constraint reference (key in constraints dict)\n\n    Returns:\n        The constraint or None if not found\n    \"\"\"\n    return self._constraints.get(ref)\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities.validate_parameter","title":"<code>validate_parameter(name, value, used_params=None)</code>","text":"<p>Validate a parameter against constraints.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>Any</code> <p>Parameter value to validate</p> required <code>used_params</code> <code>Optional[Set[str]]</code> <p>Optional set to track used parameters</p> <code>None</code> <p>Raises:</p> Type Description <code>ParameterNotSupportedError</code> <p>If the parameter is not supported</p> <code>ConstraintNotFoundError</code> <p>If a constraint reference is invalid</p> <code>ModelRegistryError</code> <p>If validation fails for other reasons</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def validate_parameter(self, name: str, value: Any, used_params: Optional[Set[str]] = None) -&gt; None:\n    \"\"\"Validate a parameter against constraints.\n\n    Args:\n        name: Parameter name\n        value: Parameter value to validate\n        used_params: Optional set to track used parameters\n\n    Raises:\n        ParameterNotSupportedError: If the parameter is not supported\n        ConstraintNotFoundError: If a constraint reference is invalid\n        ModelRegistryError: If validation fails for other reasons\n    \"\"\"\n    # Track used parameters if requested\n    if used_params is not None:\n        used_params.add(name)\n\n    # Check if we have inline parameter constraints\n    if name in self._inline_parameters:\n        self._validate_inline_parameter(name, value)\n        return\n\n    # Find matching parameter reference\n    param_ref = next(\n        (p for p in self.supported_parameters if p.ref == name or p.ref.split(\".\")[-1] == name),\n        None,\n    )\n\n    if not param_ref:\n        # If we're validating a parameter explicitly, it should be supported\n        raise ParameterNotSupportedError(\n            f\"Parameter '{name}' is not supported for model '{self.model_name}'\",\n            param_name=name,\n            value=value,\n            model=self.model_name,\n        )\n\n    constraint = self.get_constraint(param_ref.ref)\n    if not constraint:\n        # If a parameter references a constraint, the constraint should exist\n        raise ConstraintNotFoundError(\n            f\"Constraint reference '{param_ref.ref}' not found for parameter '{name}'\",\n            ref=param_ref.ref,\n        )\n\n    # Validate based on constraint type\n    if isinstance(constraint, NumericConstraint):\n        constraint.validate(name=name, value=value)\n    elif isinstance(constraint, EnumConstraint):\n        constraint.validate(name=name, value=value)\n    elif isinstance(constraint, ObjectConstraint):\n        constraint.validate(name=name, value=value)\n    else:\n        # This shouldn't happen with proper type checking, but just in case\n        raise TypeError(f\"Unknown constraint type for '{name}': {type(constraint).__name__}\")\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelCapabilities.validate_parameters","title":"<code>validate_parameters(params, used_params=None)</code>","text":"<p>Validate multiple parameters against constraints.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>Dictionary of parameter names and values to validate</p> required <code>used_params</code> <code>Optional[Set[str]]</code> <p>Optional set to track used parameters</p> <code>None</code> <p>Raises:</p> Type Description <code>ModelRegistryError</code> <p>If validation fails for any parameter</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def validate_parameters(self, params: Dict[str, Any], used_params: Optional[Set[str]] = None) -&gt; None:\n    \"\"\"Validate multiple parameters against constraints.\n\n    Args:\n        params: Dictionary of parameter names and values to validate\n        used_params: Optional set to track used parameters\n\n    Raises:\n        ModelRegistryError: If validation fails for any parameter\n    \"\"\"\n    for name, value in params.items():\n        self.validate_parameter(name, value, used_params)\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry","title":"<code>ModelRegistry</code>","text":"<p>Registry for model capabilities and validation.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class ModelRegistry:\n    \"\"\"Registry for model capabilities and validation.\"\"\"\n\n    _default_instance: Optional[\"ModelRegistry\"] = None\n    # Pre-compile regex patterns for improved performance\n    _DATE_PATTERN = re.compile(r\"^(.*)-(\\d{4}-\\d{2}-\\d{2})$\")\n    _IS_DATED_MODEL_PATTERN = re.compile(r\".*-\\d{4}-\\d{2}-\\d{2}$\")\n    _instance_lock = threading.RLock()\n\n    @classmethod\n    def get_instance(cls) -&gt; \"ModelRegistry\":\n        \"\"\"Get the default registry instance.\n\n        Prefer :py:meth:`get_default` for clarity; this alias remains for\n        brevity and historical usage but is *not* a separate code path.\n\n        Returns:\n            The singleton :class:`ModelRegistry` instance.\n        \"\"\"\n        return cls.get_default()\n\n    @classmethod\n    def get_default(cls) -&gt; \"ModelRegistry\":\n        \"\"\"Get the default registry instance with standard configuration.\n\n        Returns:\n            The default ModelRegistry instance\n        \"\"\"\n        with cls._instance_lock:\n            if cls._default_instance is None:\n                cls._default_instance = cls()\n            return cls._default_instance\n\n    def __init__(self, config: Optional[RegistryConfig] = None):\n        \"\"\"Initialize a new registry instance.\n\n        Args:\n            config: Configuration for this registry instance. If None, default\n                   configuration is used.\n        \"\"\"\n        self.config = config or RegistryConfig()\n        self._capabilities: Dict[str, ModelCapabilities] = {}\n        self._constraints: Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]] = {}\n        self._capabilities_lock = threading.RLock()\n        # Stats for last load/dump operations (for observability)\n        self._last_load_stats: Dict[str, Any] = {}\n\n        # Initialize DataManager for model and overrides data\n        self._data_manager = DataManager()\n\n        # Set up caching for get_capabilities\n        self.get_capabilities = functools.lru_cache(maxsize=self.config.cache_size)(self._get_capabilities_impl)\n\n        # Auto-copy default constraint files to user directory if they don't exist\n        if not config or not config.constraints_path:\n            try:\n                copy_default_to_user_config(PARAM_CONSTRAINTS_FILENAME)\n            except OSError as e:\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to copy default constraint config: {e}\",\n                    error=str(e),\n                )\n\n        # Check for data updates if auto_update is enabled\n        if self.config.auto_update and self._data_manager.should_update_data():\n            try:\n                success = self._data_manager.check_for_updates()\n                if success:\n                    log_info(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Auto-update completed successfully\",\n                    )\n                    # Reload capabilities after successful auto-update\n                    self._load_capabilities()\n            except Exception as e:\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to auto-update data: {e}\",\n                    error=str(e),\n                )\n\n        self._load_constraints()\n        self._load_capabilities()\n\n    def _load_config(self) -&gt; ConfigResult:\n        \"\"\"Load model configuration from file using DataManager.\n\n        Returns:\n            ConfigResult: Result of the configuration loading operation\n        \"\"\"\n        try:\n            # Use DataManager to get models.yaml content\n            content = self._data_manager.get_data_file_content(\"models.yaml\")\n            if content is None:\n                error_msg = \"Could not load models.yaml from DataManager\"\n                log_error(LogEvent.MODEL_REGISTRY, error_msg)\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            # Validate YAML content before parsing\n            if not content.strip():\n                error_msg = \"models.yaml file is empty\"\n                log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            # Check for obvious corruption patterns (heuristic)\n            if \"&amp;\" in content and \"*\" in content:\n                try:\n                    import re\n\n                    anchor_pattern = r\"&amp;(\\w+)\"\n                    reference_pattern = r\"\\*(\\w+)\"\n                    anchors = set(re.findall(anchor_pattern, content))\n                    references = set(re.findall(reference_pattern, content))\n\n                    for anchor in anchors:\n                        if anchor in references:\n                            circular_pattern = rf\"&amp;{anchor}.*?\\*{anchor}\"\n                            if re.search(circular_pattern, content, re.DOTALL):\n                                error_msg = f\"Detected circular reference in YAML with anchor '{anchor}'\"\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    error_msg,\n                                    path=\"models.yaml\",\n                                )\n                                return ConfigResult(\n                                    success=False,\n                                    error=error_msg,\n                                    path=\"models.yaml\",\n                                )\n                except Exception:\n                    # If our heuristic check fails, continue with normal parsing\n                    pass\n\n            data = yaml.safe_load(content)\n\n            # Additional validation after YAML parsing\n            if data is None:\n                error_msg = \"YAML parsing resulted in None - file may be corrupted\"\n                log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            if not isinstance(data, dict):\n                error_msg = (\n                    f\"Invalid configuration format in models.yaml: expected dictionary, got {type(data).__name__}\"\n                )\n                log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n                return ConfigResult(success=False, error=error_msg, path=\"models.yaml\")\n\n            # Load and apply provider overrides\n            try:\n                overrides_data = self._load_overrides()\n                if overrides_data:\n                    data = self._apply_overrides(data, overrides_data)\n            except Exception as e:\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to load or apply overrides: {e}\",\n                    error=str(e),\n                )\n\n            # Schema version is declared inside the YAML itself; the loader\n            # supports schema v1.x with top-level ``models`` mapping.\n            return ConfigResult(success=True, data=data, path=\"models.yaml\")\n        except yaml.YAMLError as e:\n            error_msg = f\"YAML parsing error in models.yaml: {e}\"\n            log_error(LogEvent.MODEL_REGISTRY, error_msg, path=\"models.yaml\")\n            return ConfigResult(\n                success=False,\n                error=error_msg,\n                exception=e,\n                path=\"models.yaml\",\n            )\n        except Exception as e:\n            error_msg = f\"Error loading model registry config: {e}\"\n            log_warning(LogEvent.MODEL_REGISTRY, error_msg)\n            return ConfigResult(\n                success=False,\n                error=error_msg,\n                exception=e,\n                path=\"models.yaml\",\n            )\n\n    def _load_overrides(self) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Load provider overrides from overrides.yaml.\n\n        Returns:\n            Dictionary containing overrides data, or None if not available\n        \"\"\"\n        try:\n            content = self._data_manager.get_data_file_content(\"overrides.yaml\")\n            if content is None:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"No overrides.yaml file available\",\n                )\n                return None\n\n            if not content.strip():\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"overrides.yaml file is empty\",\n                )\n                return None\n\n            data = yaml.safe_load(content)\n            if not isinstance(data, dict):\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Invalid overrides format: expected dictionary, got {type(data).__name__}\",\n                )\n                return None\n\n            return data\n        except yaml.YAMLError as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"YAML parsing error in overrides.yaml: {e}\",\n            )\n            return None\n        except Exception as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Error loading overrides.yaml: {e}\",\n            )\n            return None\n\n    def _apply_overrides(self, base_data: Dict[str, Any], overrides_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Apply provider overrides to base model data.\n\n        Args:\n            base_data: Base model configuration data\n            overrides_data: Provider overrides data\n\n        Returns:\n            Updated model configuration with overrides applied\n        \"\"\"\n        if \"overrides\" not in overrides_data:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"No 'overrides' key found in overrides.yaml\",\n            )\n            return base_data\n\n        # Get provider from environment variable, default to OpenAI\n        import os\n\n        provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n\n        # Validate provider\n        if provider not in [\"openai\", \"azure\"]:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Invalid provider '{provider}', defaulting to 'openai'\",\n            )\n            provider = \"openai\"\n        provider_overrides = overrides_data[\"overrides\"].get(provider)\n\n        if not provider_overrides:\n            if provider != \"openai\":\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"No overrides found for provider '{provider}', using base OpenAI data\",\n                )\n            # For OpenAI provider, no overrides is expected - return base data\n            return base_data\n\n        if \"models\" not in provider_overrides:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"No 'models' section found in {provider} overrides\",\n            )\n            return base_data\n\n        # Deep copy base data to avoid modifying original\n        import copy\n\n        result_data = copy.deepcopy(base_data)\n\n        # Apply model-specific overrides\n        for model_name, override_config in provider_overrides[\"models\"].items():\n            if model_name in result_data.get(\"models\", {}):\n                self._merge_model_override(result_data[\"models\"][model_name], override_config)\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Applied {provider} overrides to model '{model_name}'\",\n                )\n\n        return result_data\n\n    def _merge_model_override(self, base_model: Dict[str, Any], override_config: Dict[str, Any]) -&gt; None:\n        \"\"\"Merge override configuration into base model configuration.\n\n        Args:\n            base_model: Base model configuration (modified in place)\n            override_config: Override configuration to merge\n        \"\"\"\n        for key, value in override_config.items():\n            if key == \"pricing\" and isinstance(value, dict) and isinstance(base_model.get(\"pricing\"), dict):\n                # Merge pricing information\n                base_model[\"pricing\"].update(value)\n            elif key == \"capabilities\" and isinstance(value, dict):\n                # Replace or merge capabilities\n                if \"capabilities\" not in base_model:\n                    base_model[\"capabilities\"] = {}\n                base_model[\"capabilities\"].update(value)\n            elif key == \"parameters\" and isinstance(value, dict) and isinstance(base_model.get(\"parameters\"), dict):\n                # Merge parameters\n                base_model[\"parameters\"].update(value)\n            else:\n                # For other keys, replace entirely\n                base_model[key] = value\n\n    def _load_constraints(self) -&gt; None:\n        \"\"\"Load parameter constraints from file.\"\"\"\n        try:\n            with open(self.config.constraints_path, \"r\") as f:\n                data = yaml.safe_load(f)\n                if not isinstance(data, dict):\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Constraints file must contain a dictionary\",\n                    )\n                    return\n\n                # Handle nested structure: numeric_constraints and enum_constraints\n                for category_name, category_data in data.items():\n                    if not isinstance(category_data, dict):\n                        log_error(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Constraint category '{category_name}' must be a dictionary\",\n                            category=category_data,\n                        )\n                        continue\n\n                    # Process each constraint in the category\n                    for constraint_name, constraint in category_data.items():\n                        if not isinstance(constraint, dict):\n                            log_error(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Constraint '{constraint_name}' must be a dictionary\",\n                                constraint=constraint,\n                            )\n                            continue\n\n                        constraint_type = constraint.get(\"type\", \"\")\n                        if not constraint_type:\n                            log_error(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Constraint '{constraint_name}' missing required 'type' field\",\n                                constraint=constraint,\n                            )\n                            continue\n\n                        # Create full reference name (e.g., \"numeric_constraints.temperature\")\n                        full_ref = f\"{category_name}.{constraint_name}\"\n\n                        if constraint_type == \"numeric\":\n                            min_value = constraint.get(\"min_value\")\n                            max_value = constraint.get(\"max_value\")\n                            allow_float = constraint.get(\"allow_float\", True)\n                            allow_int = constraint.get(\"allow_int\", True)\n                            description = constraint.get(\"description\", \"\")\n\n                            # Type validation\n                            if min_value is not None and not isinstance(min_value, (int, float)):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-numeric 'min_value' value\",\n                                    min_value=min_value,\n                                )\n                                continue\n\n                            if max_value is not None and not isinstance(max_value, (int, float)):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-numeric 'max_value' value\",\n                                    max_value=max_value,\n                                )\n                                continue\n\n                            if not isinstance(allow_float, bool) or not isinstance(allow_int, bool):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-boolean 'allow_float' or 'allow_int'\",\n                                    allow_float=allow_float,\n                                    allow_int=allow_int,\n                                )\n                                continue\n\n                            # Create constraint\n                            self._constraints[full_ref] = NumericConstraint(\n                                min_value=min_value if min_value is not None else 0.0,\n                                max_value=max_value,\n                                allow_float=allow_float,\n                                allow_int=allow_int,\n                                description=description,\n                            )\n                        elif constraint_type == \"enum\":\n                            allowed_values = constraint.get(\"allowed_values\")\n                            description = constraint.get(\"description\", \"\")\n\n                            # Required field validation\n                            if allowed_values is None:\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' missing required 'allowed_values' field\",\n                                    constraint=constraint,\n                                )\n                                continue\n\n                            # Type validation\n                            if not isinstance(allowed_values, list):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-list 'allowed_values' field\",\n                                    allowed_values=allowed_values,\n                                )\n                                continue\n\n                            # Validate all values are strings\n                            if not all(isinstance(val, str) for val in allowed_values):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-string values in 'allowed_values' list\",\n                                    allowed_values=allowed_values,\n                                )\n                                continue\n\n                            # Create constraint\n                            self._constraints[full_ref] = EnumConstraint(\n                                allowed_values=allowed_values,\n                                description=description,\n                            )\n                        elif constraint_type == \"object\":\n                            # Implementation for object constraint type\n                            description = constraint.get(\"description\", \"\")\n                            required_keys = constraint.get(\"required_keys\", [])\n                            allowed_keys = constraint.get(\"allowed_keys\")\n\n                            # Type validation\n                            if not isinstance(required_keys, list):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-list 'required_keys' field\",\n                                    required_keys=required_keys,\n                                )\n                                continue\n\n                            if allowed_keys is not None and not isinstance(allowed_keys, list):\n                                log_error(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Constraint '{constraint_name}' has non-list 'allowed_keys' field\",\n                                    allowed_keys=allowed_keys,\n                                )\n                                continue\n\n                            # Create constraint\n                            self._constraints[full_ref] = ObjectConstraint(\n                                description=description,\n                                required_keys=required_keys,\n                                allowed_keys=allowed_keys,\n                            )\n                        else:\n                            log_error(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Unknown constraint type '{constraint_type}' for '{constraint_name}'\",\n                                constraint=constraint,\n                            )\n\n        except FileNotFoundError:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"Parameter constraints file not found\",\n                path=self.config.constraints_path,\n            )\n        except Exception as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to load parameter constraints\",\n                path=self.config.constraints_path,\n                error=str(e),\n            )\n\n    def _load_capabilities(self) -&gt; None:\n        \"\"\"Load model capabilities from config.\"\"\"\n        config_result = self._load_config()\n        # Abort if configuration failed to load\n        if not config_result.success or config_result.data is None:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to load registry configuration\",\n                path=self.config.registry_path,\n                error=getattr(config_result, \"error\", None),\n            )\n            return\n\n        # -------------------------------\n        # Schema version detection\n        # -------------------------------\n        from .schema_version import SchemaVersionValidator\n\n        try:\n            # Get and validate schema version\n            schema_version = SchemaVersionValidator.get_schema_version(config_result.data)\n\n            # Log schema version detection\n            from .logging import log_info\n\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                \"Schema version detected\",\n                version=schema_version,\n                compatible_range=SchemaVersionValidator.get_compatible_range(schema_version),\n                path=config_result.path,\n            )\n\n            # Check compatibility\n            if not SchemaVersionValidator.is_compatible_schema(schema_version):\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Unsupported schema version\",\n                    version=schema_version,\n                    supported_ranges=list(SchemaVersionValidator.SUPPORTED_SCHEMA_VERSIONS.values()),\n                    path=config_result.path,\n                )\n                return\n\n            # Validate structure matches version\n            if not SchemaVersionValidator.validate_schema_structure(config_result.data, schema_version):\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Schema structure validation failed\",\n                    version=schema_version,\n                    path=config_result.path,\n                )\n                return\n\n            # Route to appropriate loader\n            loader_method = SchemaVersionValidator.get_loader_method_name(schema_version)\n            if loader_method and hasattr(self, loader_method):\n                getattr(self, loader_method)(config_result.data.get(\"models\", {}))\n                return\n            else:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"No loader available for schema version\",\n                    version=schema_version,\n                    path=config_result.path,\n                )\n                return\n\n        except ValueError as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Schema version validation failed\",\n                error=str(e),\n                path=config_result.path,\n            )\n            return\n\n    def _load_capabilities_modern(self, models_data: Dict[str, Any]) -&gt; None:\n        \"\"\"Load capabilities from modern schema (1.x) where models are top-level.\n\n        The modern schema (1.0.0+) places every model \u2013 base or dated \u2013 as a\n        direct child of the ``models`` mapping and groups feature flags beneath\n        a ``capabilities`` key. This helper converts the structure into\n        the ``ModelCapabilities`` dataclass so the public API remains\n        unchanged.\n        \"\"\"\n        from datetime import date, datetime\n\n        loaded_count: int = 0\n        skipped_count: int = 0\n        first_error: Optional[str] = None\n\n        for model_name, model_config in models_data.items():\n            try:\n                # -------------------\n                # Context window size\n                # -------------------\n                context_window_raw = model_config.get(\"context_window\", 0)\n                if isinstance(context_window_raw, dict):\n                    context_window = int(context_window_raw.get(\"total\", 0) or 0)\n                    output_tokens_raw = context_window_raw.get(\"output\") or model_config.get(\"max_output_tokens\", 0)\n                    max_output_tokens = int(output_tokens_raw or 0)\n                else:\n                    context_window = int(context_window_raw or 0)\n                    max_output_tokens = int(model_config.get(\"max_output_tokens\", 0) or 0)\n\n                # -------------\n                # Capabilities\n                # -------------\n                caps_block: Dict[str, Any] = model_config.get(\"capabilities\", {})\n\n                supports_vision = bool(\n                    caps_block.get(\n                        \"supports_vision\",\n                        model_config.get(\"supports_vision\", False),\n                    )\n                )\n                supports_functions = bool(\n                    caps_block.get(\n                        \"supports_function_calling\",\n                        model_config.get(\"supports_functions\", False),\n                    )\n                )\n                supports_streaming = bool(\n                    caps_block.get(\n                        \"supports_streaming\",\n                        model_config.get(\"supports_streaming\", False),\n                    )\n                )\n                supports_structured = bool(\n                    caps_block.get(\n                        \"supports_structured_output\",\n                        model_config.get(\"supports_structured\", False),\n                    )\n                )\n                supports_web_search = bool(\n                    caps_block.get(\n                        \"supports_web_search\",\n                        model_config.get(\"supports_web_search\", False),\n                    )\n                )\n                supports_audio = bool(\n                    caps_block.get(\n                        \"supports_audio\",\n                        model_config.get(\"supports_audio\", False),\n                    )\n                )\n                supports_json_mode = bool(\n                    caps_block.get(\n                        \"supports_json_mode\",\n                        model_config.get(\"supports_json_mode\", False),\n                    )\n                )\n\n                # -------------\n                # Deprecation\n                # -------------\n                dep_block: Dict[str, Any] = model_config.get(\"deprecation\", {})\n                dep_status = dep_block.get(\"status\", \"active\")\n\n                def _parse_date(val: Any) -&gt; Optional[date]:\n                    if val in (None, \"\", \"null\"):\n                        return None\n                    try:\n                        return datetime.fromisoformat(str(val)).date()\n                    except Exception:\n                        return None\n\n                deprecates_on = _parse_date(dep_block.get(\"deprecates_on\"))\n                sunsets_on = _parse_date(dep_block.get(\"sunsets_on\")) or _parse_date(dep_block.get(\"sunset_date\"))\n\n                deprecation = DeprecationInfo(\n                    status=dep_status,\n                    deprecates_on=deprecates_on,\n                    sunsets_on=sunsets_on,\n                    replacement=dep_block.get(\"replacement\"),\n                    migration_guide=dep_block.get(\"migration_guide\"),\n                    reason=dep_block.get(\"reason\", dep_status),\n                )\n\n                # -------------\n                # Min version\n                # -------------\n                min_version_data = model_config.get(\"min_version\")\n                min_version: Optional[ModelVersion] = None\n                if min_version_data:\n                    try:\n                        if isinstance(min_version_data, dict):\n                            year = min_version_data.get(\"year\")\n                            month = min_version_data.get(\"month\")\n                            day = min_version_data.get(\"day\")\n                            if year and month and day:\n                                min_version = ModelVersion(year=year, month=month, day=day)\n                        else:\n                            min_version = ModelVersion.from_string(str(min_version_data))\n                    except (ValueError, TypeError):\n                        # Ignore bad min_version values\n                        min_version = None\n\n                # ----------------------\n                # Supported parameters\n                # ----------------------\n                param_refs: List[ParameterReference] = []\n\n                # Extract parameters from parameters block\n                parameters_block = model_config.get(\"parameters\", {})\n                if parameters_block and isinstance(parameters_block, dict):\n                    for param_name, param_config in parameters_block.items():\n                        if isinstance(param_config, dict):\n                            # Create parameter reference\n                            param_refs.append(\n                                ParameterReference(\n                                    ref=param_name,\n                                    description=f\"Parameter {param_name}\",\n                                )\n                            )\n                    # If we collected inline parameters but there were no explicit supported_parameters,\n                    # use the inline list as supported parameters to allow validation.\n                    if not param_refs:\n                        for param_name in parameters_block.keys():\n                            param_refs.append(ParameterReference(ref=param_name, description=f\"Parameter {param_name}\"))\n\n                # Note: legacy 'supported_parameters' is intentionally not supported.\n\n                # -------------\n                # Pricing block\n                # -------------\n                pricing_block = model_config.get(\"pricing\")\n                pricing_obj: Optional[PricingInfo] = None\n                if isinstance(pricing_block, dict):\n                    try:\n                        # Support both unified pricing (scheme/unit) and legacy per-million-token keys\n                        if \"scheme\" in pricing_block and \"unit\" in pricing_block:\n                            pricing_obj = PricingInfo(\n                                scheme=typing.cast(\n                                    typing.Literal[\n                                        \"per_token\",\n                                        \"per_minute\",\n                                        \"per_image\",\n                                        \"per_request\",\n                                    ],\n                                    str(pricing_block.get(\"scheme\")),\n                                ),\n                                unit=typing.cast(\n                                    typing.Literal[\n                                        \"million_tokens\",\n                                        \"minute\",\n                                        \"image\",\n                                        \"request\",\n                                    ],\n                                    str(pricing_block.get(\"unit\")),\n                                ),\n                                input_cost_per_unit=float(pricing_block.get(\"input_cost_per_unit\", 0.0)),\n                                output_cost_per_unit=float(pricing_block.get(\"output_cost_per_unit\", 0.0)),\n                                currency=str(pricing_block.get(\"currency\", \"USD\")),\n                                tiers=typing.cast(\n                                    typing.Optional[typing.List[typing.Dict[str, typing.Any]]],\n                                    pricing_block.get(\"tiers\"),\n                                ),\n                            )\n                        else:\n                            # Legacy support (pre-unified): interpret as per_token/million_tokens\n                            pricing_obj = PricingInfo(\n                                scheme=\"per_token\",\n                                unit=\"million_tokens\",\n                                input_cost_per_unit=float(pricing_block.get(\"input_cost_per_million_tokens\", 0.0)),\n                                output_cost_per_unit=float(pricing_block.get(\"output_cost_per_million_tokens\", 0.0)),\n                                currency=str(pricing_block.get(\"currency\", \"USD\")),\n                            )\n                    except Exception as e:  # pragma: no cover\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            \"Invalid pricing block ignored\",\n                            model=model_name,\n                            error=str(e),\n                        )\n\n                # -------------\n                # Web search billing block (optional)\n                # -------------\n                web_search_billing: Optional[WebSearchBilling] = None\n                billing_block = model_config.get(\"billing\")\n                if isinstance(billing_block, dict):\n                    ws = billing_block.get(\"web_search\")\n                    if isinstance(ws, dict):\n                        try:\n                            policy = str(ws.get(\"content_token_policy\", \"\")).strip()\n                            if policy in {\"included_in_call_fee\", \"billed_at_model_rate\"}:\n                                web_search_billing = WebSearchBilling(\n                                    call_fee_per_1000=float(ws.get(\"call_fee_per_1000\", 0.0)),\n                                    content_token_policy=policy,  # type: ignore[arg-type]\n                                    currency=str(ws.get(\"currency\", \"USD\")),\n                                    notes=str(ws.get(\"notes\")) if \"notes\" in ws else None,\n                                )\n                        except Exception:\n                            web_search_billing = None\n\n                # -------------\n                # Build object\n                # -------------\n                capabilities = ModelCapabilities(\n                    model_name=model_name,\n                    openai_model_name=model_config.get(\"openai_name\", model_name),\n                    context_window=context_window,\n                    max_output_tokens=max_output_tokens,\n                    deprecation=deprecation,\n                    supports_vision=supports_vision,\n                    supports_functions=supports_functions,\n                    supports_streaming=supports_streaming,\n                    supports_structured=supports_structured,\n                    supports_web_search=supports_web_search,\n                    supports_audio=supports_audio,\n                    supports_json_mode=supports_json_mode,\n                    pricing=pricing_obj,\n                    input_modalities=model_config.get(\"input_modalities\"),\n                    output_modalities=model_config.get(\"output_modalities\"),\n                    min_version=min_version,\n                    aliases=[],\n                    supported_parameters=param_refs,\n                    constraints=copy.deepcopy(self._constraints),\n                    inline_parameters=parameters_block,\n                    web_search_billing=web_search_billing,\n                )\n\n                with self._capabilities_lock:\n                    self._capabilities[model_name] = capabilities\n                loaded_count += 1\n            except Exception as e:  # pragma: no cover \u2013 best-effort parsing\n                if first_error is None:\n                    first_error = f\"{type(e).__name__}: {e}\"\n                skipped_count += 1\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Failed to load model capabilities\",\n                    model=model_name,\n                    error=str(e),\n                )\n\n        # Bookkeep and log summary for observability\n        try:\n            self._last_load_stats = {\n                \"total\": len(models_data),\n                \"loaded\": loaded_count,\n                \"skipped\": skipped_count,\n                \"first_error\": first_error,\n            }\n        except Exception:\n            pass\n\n        log_info(\n            LogEvent.MODEL_REGISTRY,\n            \"Model load summary\",\n            total=len(models_data),\n            loaded=loaded_count,\n            skipped=skipped_count,\n        )\n\n    def _get_capabilities_impl(self, model: str) -&gt; ModelCapabilities:\n        \"\"\"Implementation of get_capabilities without caching.\n\n        Args:\n            model: Model name, which can be:\n                  - Dated model (e.g. \"gpt-4o-2024-08-06\")\n                  - Alias (e.g. \"gpt-4o\")\n                  - Versioned model (e.g. \"gpt-4o-2024-09-01\")\n\n        Returns:\n            ModelCapabilities for the requested model\n\n        Raises:\n            ModelNotSupportedError: If the model is not supported\n            InvalidDateError: If the date components are invalid\n            VersionTooOldError: If the version is older than minimum supported\n        \"\"\"\n        # First check for exact match (dated model or alias)\n        with self._capabilities_lock:\n            if model in self._capabilities:\n                return self._capabilities[model]\n\n        # Check if this is a versioned model\n        version_match = self._DATE_PATTERN.match(model)\n        if version_match:\n            base_name = version_match.group(1)\n            version_str = version_match.group(2)\n\n            # Find all capabilities for this base model\n            with self._capabilities_lock:\n                model_versions = [(k, v) for k, v in self._capabilities.items() if k.startswith(f\"{base_name}-\")]\n\n            if not model_versions:\n                # No versions found for this base model\n                # Find aliases that might provide a valid alternative\n                with self._capabilities_lock:\n                    aliases = [\n                        name for name in self._capabilities.keys() if not self._IS_DATED_MODEL_PATTERN.match(name)\n                    ]\n\n                # Find if any alias might match the base model\n                matching_aliases = [alias for alias in aliases if alias == base_name]\n\n                if matching_aliases:\n                    raise ModelNotSupportedError(\n                        f\"Model '{model}' not found.\",\n                        model=model,\n                        available_models=matching_aliases,\n                    )\n                else:\n                    # No matching aliases either\n                    with self._capabilities_lock:\n                        available_base_models: set[str] = set(\n                            k for k in self._capabilities.keys() if not self._IS_DATED_MODEL_PATTERN.match(k)\n                        )\n                    raise ModelNotSupportedError(\n                        f\"Model '{model}' not found. Available base models: {', '.join(sorted(available_base_models))}\",\n                        model=model,\n                        available_models=list(available_base_models),\n                    )\n\n            try:\n                # Parse version\n                requested_version = ModelVersion.from_string(version_str)\n            except ValueError as e:\n                raise InvalidDateError(str(e))\n\n            # Find the model with the minimum version\n            for _unused, caps in model_versions:\n                if caps.min_version and requested_version &lt; caps.min_version:\n                    raise VersionTooOldError(\n                        f\"Model version '{model}' is older than the minimum supported \"\n                        f\"version {caps.min_version} for {base_name}.\",\n                        model=model,\n                        min_version=str(caps.min_version),\n                        alias=None,\n                    )\n\n            # Find the best matching model\n            base_model_caps = None\n            for _dated_model, caps in model_versions:\n                if base_model_caps is None or (\n                    caps.min_version\n                    and caps.min_version &lt;= requested_version\n                    and (not base_model_caps.min_version or caps.min_version &gt; base_model_caps.min_version)\n                ):\n                    base_model_caps = caps\n\n            if base_model_caps:\n                # Create a copy with the requested model name\n                new_caps = ModelCapabilities(\n                    model_name=base_model_caps.model_name,\n                    openai_model_name=model,\n                    context_window=base_model_caps.context_window,\n                    max_output_tokens=base_model_caps.max_output_tokens,\n                    deprecation=base_model_caps.deprecation,\n                    supports_vision=base_model_caps.supports_vision,\n                    supports_functions=base_model_caps.supports_functions,\n                    supports_streaming=base_model_caps.supports_streaming,\n                    supports_structured=base_model_caps.supports_structured,\n                    supports_web_search=base_model_caps.supports_web_search,\n                    supports_audio=base_model_caps.supports_audio,\n                    supports_json_mode=base_model_caps.supports_json_mode,\n                    pricing=base_model_caps.pricing,\n                    input_modalities=base_model_caps.input_modalities,\n                    output_modalities=base_model_caps.output_modalities,\n                    min_version=base_model_caps.min_version,\n                    aliases=base_model_caps.aliases,\n                    supported_parameters=base_model_caps.supported_parameters,\n                    constraints=base_model_caps._constraints,\n                )\n                return new_caps\n\n        # If we get here, the model is not supported\n        with self._capabilities_lock:\n            available_models: set[str] = set(\n                k for k in self._capabilities.keys() if not self._IS_DATED_MODEL_PATTERN.match(k)\n            )\n        raise ModelNotSupportedError(\n            f\"Model '{model}' not found. Available base models: {', '.join(sorted(available_models))}\",\n            model=model,\n            available_models=list(available_models),\n        )\n\n    def get_parameter_constraint(self, ref: str) -&gt; Union[NumericConstraint, EnumConstraint, ObjectConstraint]:\n        \"\"\"Get a parameter constraint by reference.\n\n        Args:\n            ref: Reference string (e.g., \"numeric_constraints.temperature\")\n\n        Returns:\n            The constraint object (NumericConstraint or EnumConstraint or ObjectConstraint)\n\n        Raises:\n            ConstraintNotFoundError: If the constraint is not found\n        \"\"\"\n        if ref not in self._constraints:\n            raise ConstraintNotFoundError(\n                f\"Constraint reference '{ref}' not found in registry\",\n                ref=ref,\n            )\n        return self._constraints[ref]\n\n    def assert_model_active(self, model: str) -&gt; None:\n        \"\"\"Assert that a model is active and warn if deprecated.\n\n        Args:\n            model: Model name to check\n\n        Raises:\n            ModelSunsetError: If the model is sunset\n            ModelNotSupportedError: If the model is not found\n\n        Warns:\n            DeprecationWarning: If the model is deprecated\n        \"\"\"\n        capabilities = self.get_capabilities(model)\n        assert_model_active(model, capabilities.deprecation)\n\n    def get_sunset_headers(self, model: str) -&gt; dict[str, str]:\n        \"\"\"Get RFC-compliant HTTP headers for model deprecation status.\n\n        Args:\n            model: Model name\n\n        Returns:\n            Dictionary of HTTP headers\n\n        Raises:\n            ModelNotSupportedError: If the model is not found\n        \"\"\"\n        capabilities = self.get_capabilities(model)\n        return sunset_headers(capabilities.deprecation)\n\n    def _get_conditional_headers(self, force: bool = False) -&gt; Dict[str, str]:\n        \"\"\"Get conditional headers for HTTP requests.\n\n        Args:\n            force: If True, bypass conditional headers\n\n        Returns:\n            Dictionary of HTTP headers\n        \"\"\"\n        if force:\n            return {}\n\n        headers = {}\n        meta_path = self._get_metadata_path()\n        if meta_path and os.path.exists(meta_path):\n            try:\n                with open(meta_path, \"r\") as f:\n                    metadata = yaml.safe_load(f)\n                    if metadata and isinstance(metadata, dict):\n                        if \"etag\" in metadata:\n                            headers[\"If-None-Match\"] = metadata[\"etag\"]\n                        if \"last_modified\" in metadata:\n                            headers[\"If-Modified-Since\"] = metadata[\"last_modified\"]\n            except Exception as e:\n                log_debug(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Could not load cache metadata, skipping conditional headers\",\n                    error=str(e),\n                )\n        return headers\n\n    def _get_metadata_path(self) -&gt; Optional[str]:\n        \"\"\"Get the path to the cache metadata file.\n\n        Returns:\n            Optional[str]: Path to the metadata file, or None if config_path is not set\n        \"\"\"\n        if not self.config.registry_path:\n            return None\n        return f\"{self.config.registry_path}.meta\"\n\n    def _save_cache_metadata(self, metadata: Dict[str, str]) -&gt; None:\n        \"\"\"Save cache metadata to file.\n\n        Args:\n            metadata: Dictionary of metadata to save\n        \"\"\"\n        meta_path = self._get_metadata_path()\n        if not meta_path:\n            return\n\n        try:\n            with open(meta_path, \"w\") as f:\n                yaml.safe_dump(metadata, f)\n        except Exception as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"Could not save cache metadata\",\n                error=str(e),\n                path=str(meta_path),  # Convert to string in case meta_path is None\n            )\n\n    def _fetch_remote_config(self, url: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Fetch the remote configuration from the specified URL.\n\n        Args:\n            url: URL to fetch the configuration from\n\n        Returns:\n            Parsed configuration dictionary or None if fetch failed\n        \"\"\"\n        try:\n            import requests\n        except ImportError:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Could not import requests module\",\n            )\n            return None\n\n        try:\n            # Add a timeout of 10 seconds to prevent indefinite hanging\n            response = requests.get(url, timeout=10)\n            try:\n                if response.status_code != 200:\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"HTTP error {response.status_code}\",\n                        url=url,\n                    )\n                    return None\n\n                # Parse the YAML content\n                config = yaml.safe_load(response.text)\n                if not isinstance(config, dict):\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Remote config is not a dictionary\",\n                        url=url,\n                    )\n                    return None\n\n                return config\n            finally:\n                # Ensure response is closed to prevent resource leaks\n                response.close()\n        except (requests.RequestException, yaml.YAMLError) as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to fetch or parse remote config: {str(e)}\",\n                url=url,\n            )\n            return None\n\n    def _validate_remote_config(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Validate the remote configuration before applying it.\n\n        Args:\n            config: Configuration dictionary to validate\n\n        Raises:\n            ValueError: If the configuration is invalid\n        \"\"\"\n        # Check version\n        if \"version\" not in config:\n            raise ValueError(\"Remote configuration missing version field\")\n\n        # Check required sections for both schema versions\n        if \"models\" in config:\n            # New schema \u2013 nothing else to validate here for presence\n            pass\n        else:\n            raise ValueError(\"Remote configuration missing 'models' section\")\n\n    def refresh_from_remote(\n        self,\n        url: Optional[str] = None,\n        force: bool = False,\n        validate_only: bool = False,\n    ) -&gt; RefreshResult:\n        \"\"\"Refresh the registry configuration from remote source.\n\n        Args:\n            url: Optional custom URL to fetch registry from\n            force: Force refresh even if version is current\n            validate_only: Only validate remote config without updating\n\n        Returns:\n            Result of the refresh operation\n        \"\"\"\n        try:\n            # Get remote config\n            config_url = url or (\n                \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n            )\n            remote_config = self._fetch_remote_config(config_url)\n            if not remote_config:\n                raise ValueError(\"Failed to fetch remote configuration\")\n\n            # Validate the remote config\n            self._validate_remote_config(remote_config)\n\n            if validate_only:\n                # Only validation was requested\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.VALIDATED,\n                    message=\"Remote registry configuration validated successfully\",\n                )\n\n            # Check for updates only if not forcing and not validating\n            if not force:\n                result = self.check_for_updates(url=url)\n                if result.status == RefreshStatus.ALREADY_CURRENT:\n                    return RefreshResult(\n                        success=True,\n                        status=RefreshStatus.ALREADY_CURRENT,\n                        message=\"Registry is already up to date\",\n                    )\n\n            # Use DataManager to handle the update\n            try:\n                # Force update through DataManager\n                if self._data_manager.force_update():\n                    log_info(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Successfully updated registry data via DataManager\",\n                    )\n                else:\n                    # Fallback to manual update if DataManager fails\n                    # Note: This fallback downloads models.yaml and attempts overrides.yaml\n                    log_warning(\n                        LogEvent.MODEL_REGISTRY,\n                        \"DataManager update failed, using limited fallback (models.yaml only)\",\n                    )\n                    target_path = get_user_data_dir() / \"models.yaml\"\n                    with open(target_path, \"w\") as f:\n                        yaml.safe_dump(remote_config, f)\n\n                    # Try to download overrides.yaml if possible\n                    try:\n                        overrides_url = \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/overrides.yaml\"\n\n                        # Simple fallback downloads\n                        try:\n                            import requests\n                        except ImportError:\n                            requests = None  # type: ignore\n\n                        if requests is not None:\n                            try:\n                                # Download overrides.yaml\n                                overrides_resp = requests.get(overrides_url, timeout=30)\n                                if overrides_resp.status_code == 200:\n                                    overrides_content = overrides_resp.text\n                                    overrides_path = get_user_data_dir() / \"overrides.yaml\"\n                                    with open(overrides_path, \"w\") as f:\n                                        f.write(overrides_content)\n                                    log_info(\n                                        LogEvent.MODEL_REGISTRY,\n                                        \"Downloaded overrides.yaml in fallback\",\n                                    )\n\n                            except requests.RequestException as e:\n                                log_warning(\n                                    LogEvent.MODEL_REGISTRY,\n                                    f\"Failed to download additional files in fallback: {e}\",\n                                )\n                    except Exception as e:\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Error in fallback additional file download: {e}\",\n                        )\n            except PermissionError as e:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Permission denied when writing registry configuration\",\n                    path=str(target_path),\n                    error=str(e),\n                )\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"Permission denied when writing to {target_path}\",\n                )\n            except OSError as e:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"File system error when writing registry configuration\",\n                    path=str(target_path),\n                    error=str(e),\n                )\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"Error writing to {target_path}: {str(e)}\",\n                )\n\n            # Reload the registry with new configuration\n            self._load_constraints()\n            self._load_capabilities()\n\n            # Verify that the reload was successful\n            if not self._capabilities:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Failed to reload registry after update\",\n                    path=str(target_path),\n                )\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=\"Registry update failed: could not load capabilities after update\",\n                )\n\n            # Log success\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                \"Registry updated from remote\",\n                version=remote_config.get(\"version\", \"unknown\"),\n            )\n\n            return RefreshResult(\n                success=True,\n                status=RefreshStatus.UPDATED,\n                message=\"Registry updated successfully\",\n            )\n\n        except Exception as e:\n            error_msg = f\"Error refreshing registry: {str(e)}\"\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                error_msg,\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=error_msg,\n            )\n\n    def check_for_updates(self, url: Optional[str] = None) -&gt; RefreshResult:\n        \"\"\"Check if updates are available for the model registry.\n\n        Args:\n            url: Optional custom URL to check for updates\n\n        Returns:\n            Result of the update check\n        \"\"\"\n        try:\n            import requests\n        except ImportError:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=\"Could not import requests module\",\n            )\n\n        # Set up the URL with fallback handling\n        primary_url = url or (\n            \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n        )\n\n        # Define fallback URLs in case primary fails\n        fallback_urls = [\n            \"https://github.com/yaniv-golan/openai-model-registry/raw/main/data/models.yaml\",\n        ]\n\n        urls_to_try = [primary_url] + fallback_urls\n\n        try:\n            # Use a lock when checking and comparing versions to prevent race conditions\n            with self.__class__._instance_lock:\n                # First check with DataManager\n                if self._data_manager.should_update_data():\n                    latest_release = self._data_manager._fetch_latest_data_release()\n                    if latest_release:\n                        latest_version = latest_release.get(\"tag_name\", \"\")\n                        current_version = self._data_manager._get_current_version()\n                        if (\n                            current_version\n                            and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0\n                        ):\n                            return RefreshResult(\n                                success=True,\n                                status=RefreshStatus.ALREADY_CURRENT,\n                                message=f\"Registry is up to date (version {current_version})\",\n                            )\n                        else:\n                            return RefreshResult(\n                                success=True,\n                                status=RefreshStatus.UPDATE_AVAILABLE,\n                                message=f\"Update available: {current_version or 'bundled'} -&gt; {latest_version}\",\n                            )\n\n                # Fallback to original HTTP check with URL fallback\n                remote_config = None\n                for config_url in urls_to_try:\n                    try:\n                        response = requests.get(config_url, timeout=10)\n                        response.raise_for_status()\n\n                        # Parse the remote config\n                        remote_config = yaml.safe_load(response.text)\n                        if isinstance(remote_config, dict):\n                            break\n                        else:\n                            log_warning(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Remote config from {config_url} is not a valid dictionary\",\n                            )\n                    except (requests.RequestException, yaml.YAMLError) as e:\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Failed to fetch from {config_url}: {e}\",\n                        )\n                        continue\n\n                if remote_config is None:\n                    return RefreshResult(\n                        success=False,\n                        status=RefreshStatus.ERROR,\n                        message=\"Could not fetch remote config from any URL\",\n                    )\n\n                # Get local config for comparison\n                local_config = self._load_config()\n                if not local_config.success:\n                    return RefreshResult(\n                        success=False,\n                        status=RefreshStatus.ERROR,\n                        message=f\"Could not load local config: {local_config.error}\",\n                    )\n\n                # Compare versions (simplified comparison)\n                remote_version = remote_config.get(\"version\", \"unknown\")\n                local_version = local_config.data.get(\"version\", \"unknown\") if local_config.data else \"unknown\"\n\n                if remote_version == local_version:\n                    return RefreshResult(\n                        success=True,\n                        status=RefreshStatus.ALREADY_CURRENT,\n                        message=f\"Registry is up to date (version {local_version})\",\n                    )\n                else:\n                    return RefreshResult(\n                        success=True,\n                        status=RefreshStatus.UPDATE_AVAILABLE,\n                        message=f\"Update available: {local_version} -&gt; {remote_version}\",\n                    )\n\n        except requests.HTTPError as e:\n            if e.response.status_code == 404:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=\"Registry not found at any of the configured URLs\",\n                )\n            else:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"HTTP error {e.response.status_code}: {e}\",\n                )\n        except requests.RequestException as e:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Network error: {e}\",\n            )\n        except Exception as e:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Unexpected error: {e}\",\n            )\n\n    def check_data_updates(self) -&gt; bool:\n        \"\"\"Check if data updates are available using DataManager.\n\n        Returns:\n            True if updates are available, False otherwise\n        \"\"\"\n        try:\n            if not self._data_manager.should_update_data():\n                return False\n\n            latest_release = self._data_manager._fetch_latest_data_release()\n            if not latest_release:\n                return False\n\n            latest_version = latest_release.get(\"tag_name\", \"\")\n            current_version = self._data_manager._get_current_version()\n\n            return not (current_version and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0)\n        except Exception:\n            return False\n\n    def get_update_info(self) -&gt; UpdateInfo:\n        \"\"\"Get detailed information about available updates.\n\n        Returns:\n            UpdateInfo object containing update details\n        \"\"\"\n        try:\n            if not self._data_manager.should_update_data():\n                return UpdateInfo(\n                    update_available=False,\n                    current_version=self._data_manager._get_current_version(),\n                    current_version_date=None,\n                    latest_version=None,\n                    latest_version_date=None,\n                    download_url=None,\n                    update_size_estimate=None,\n                    latest_version_description=None,\n                    accumulated_changes=[],\n                    error_message=\"Updates are disabled via environment variable\",\n                )\n\n            latest_release = self._data_manager._fetch_latest_data_release()\n            if not latest_release:\n                return UpdateInfo(\n                    update_available=False,\n                    current_version=self._data_manager._get_current_version(),\n                    current_version_date=None,\n                    latest_version=None,\n                    latest_version_date=None,\n                    download_url=None,\n                    update_size_estimate=None,\n                    latest_version_description=None,\n                    accumulated_changes=[],\n                    error_message=\"No releases found on GitHub\",\n                )\n\n            latest_version_raw = latest_release.get(\"tag_name\", \"\")\n            current_version = self._data_manager._get_current_version()\n\n            # Clean the latest version to match current version format (remove data-v prefix)\n            latest_version = latest_version_raw\n            if latest_version_raw.startswith(\"data-v\"):\n                latest_version = latest_version_raw[len(\"data-v\") :]\n\n            # Get current version info with date\n            current_version_info = self._data_manager._get_current_version_info()\n            current_version_date = current_version_info.get(\"published_at\") if current_version_info else None\n\n            update_available = not (\n                current_version and self._data_manager._compare_versions(latest_version_raw, current_version) &lt;= 0\n            )\n\n            # Get accumulated changes between current and latest version\n            accumulated_changes = []\n            if update_available:\n                accumulated_changes = self._data_manager.get_accumulated_changes(current_version, latest_version_raw)\n\n            # Estimate update size based on assets\n            update_size_estimate = None\n            total_size = 0\n            assets = latest_release.get(\"assets\", [])\n            for asset in assets:\n                if asset.get(\"name\") in [\"models.yaml\", \"overrides.yaml\"]:\n                    total_size += asset.get(\"size\", 0)\n\n            if total_size &gt; 0:\n                if total_size &lt; 1024:\n                    update_size_estimate = f\"{total_size} bytes\"\n                elif total_size &lt; 1024 * 1024:\n                    update_size_estimate = f\"{total_size / 1024:.1f} KB\"\n                else:\n                    update_size_estimate = f\"{total_size / (1024 * 1024):.1f} MB\"\n\n            # Extract one-sentence description from latest release body\n            latest_version_description = None\n            if latest_release.get(\"body\"):\n                latest_version_description = self._data_manager._extract_change_summary(latest_release.get(\"body\", \"\"))\n\n            return UpdateInfo(\n                update_available=update_available,\n                current_version=current_version,\n                current_version_date=current_version_date,\n                latest_version=latest_version,\n                latest_version_date=latest_release.get(\"published_at\"),\n                download_url=latest_release.get(\"html_url\"),\n                update_size_estimate=update_size_estimate,\n                latest_version_description=latest_version_description,\n                accumulated_changes=accumulated_changes,\n                error_message=None,\n            )\n\n        except Exception as e:\n            return UpdateInfo(\n                update_available=False,\n                current_version=self._data_manager._get_current_version(),\n                current_version_date=None,\n                latest_version=None,\n                latest_version_date=None,\n                download_url=None,\n                update_size_estimate=None,\n                latest_version_description=None,\n                accumulated_changes=[],\n                error_message=str(e),\n            )\n\n    def update_data(self, force: bool = False) -&gt; bool:\n        \"\"\"Update model registry data using DataManager.\n\n        Args:\n            force: If True, force update regardless of current version\n\n        Returns:\n            True if update was successful, False otherwise\n        \"\"\"\n        try:\n            if force:\n                success = self._data_manager.force_update()\n            else:\n                success = self._data_manager.check_for_updates()\n\n            if success:\n                # Reload capabilities after successful update\n                self._load_capabilities()\n\n            return success\n        except Exception:\n            return False\n\n    def manual_update_workflow(self, prompt_user_func: Optional[Callable[[UpdateInfo], bool]] = None) -&gt; bool:\n        \"\"\"Manual update workflow with user approval.\n\n        Args:\n            prompt_user_func: Optional function to prompt user for approval.\n                            Should take UpdateInfo as parameter and return bool.\n                            If None, uses a default console prompt.\n\n        Returns:\n            True if update was performed, False otherwise\n        \"\"\"\n        try:\n            # Get update information\n            update_info = self.get_update_info()\n\n            if update_info.error_message:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Failed to check for updates: {update_info.error_message}\",\n                )\n                return False\n\n            if not update_info.update_available:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Registry is up to date (version {update_info.current_version or 'bundled'})\",\n                )\n                return False\n\n            # Use custom prompt function or default\n            if prompt_user_func is None:\n                prompt_user_func = self._default_update_prompt\n\n            # Ask user for approval\n            if prompt_user_func(update_info):\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"User approved update from {update_info.current_version or 'bundled'} to {update_info.latest_version}\",\n                )\n\n                # Perform the update\n                success = self.update_data()\n\n                if success:\n                    log_info(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"Successfully updated to {update_info.latest_version}\",\n                    )\n                else:\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Update failed\",\n                    )\n\n                return success\n            else:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"User declined update\",\n                )\n                return False\n\n        except Exception as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                f\"Manual update workflow failed: {e}\",\n            )\n            return False\n\n    def _default_update_prompt(self, update_info: UpdateInfo) -&gt; bool:\n        \"\"\"Default console prompt for update approval.\n\n        Args:\n            update_info: Information about the available update\n\n        Returns:\n            True if user approves, False otherwise\n        \"\"\"\n        print(\"\\n\ud83d\udd04 OpenAI Model Registry Update Available\")\n        print(f\"   Current version: {update_info.current_version or 'bundled'}\")\n        print(f\"   Latest version:  {update_info.latest_version}\")\n\n        if update_info.current_version_date:\n            print(f\"   Current date:    {update_info.current_version_date}\")\n        if update_info.latest_version_date:\n            print(f\"   Latest date:     {update_info.latest_version_date}\")\n        if update_info.update_size_estimate:\n            print(f\"   Download size:   {update_info.update_size_estimate}\")\n        if update_info.latest_version_description:\n            print(f\"   Description:     {update_info.latest_version_description}\")\n\n        # Show accumulated changes\n        if update_info.accumulated_changes:\n            print(\"\\n\ud83d\udcdd Changes since your last update:\")\n            for change in update_info.accumulated_changes:\n                print(f\"   \u2022 {change['version']} ({change['date'][:10] if change['date'] else 'Unknown date'})\")\n                print(f\"     {change['description']}\")\n\n        print(f\"\\n\ud83d\udd17 Release info: {update_info.download_url}\")\n\n        try:\n            response = input(\"\\nWould you like to update now? [y/N]: \").strip().lower()\n            return response in (\"y\", \"yes\")\n        except (KeyboardInterrupt, EOFError):\n            return False\n\n    def get_data_version(self) -&gt; Optional[str]:\n        \"\"\"Get the current data version.\n\n        Returns:\n            Current data version string, or None if using bundled data\n        \"\"\"\n        try:\n            return self._data_manager._get_current_version()\n        except Exception:\n            return None\n\n    def get_data_info(self) -&gt; Dict[str, Any]:\n        \"\"\"Get information about data configuration and status.\n\n        Returns:\n            Dictionary containing data configuration information\n        \"\"\"\n        try:\n            info: Dict[str, Any] = {\n                \"data_directory\": str(self._data_manager._data_dir),\n                \"current_version\": self._data_manager._get_current_version(),\n                \"updates_enabled\": self._data_manager.should_update_data(),\n                \"environment_variables\": {\n                    \"OMR_DISABLE_DATA_UPDATES\": os.getenv(\"OMR_DISABLE_DATA_UPDATES\"),\n                    \"OMR_DATA_VERSION_PIN\": os.getenv(\"OMR_DATA_VERSION_PIN\"),\n                    \"OMR_DATA_DIR\": os.getenv(\"OMR_DATA_DIR\"),\n                },\n                \"data_files\": {},\n            }\n\n            # Check data file status\n            for filename in [\"models.yaml\", \"overrides.yaml\"]:\n                file_path = self._data_manager.get_data_file_path(filename)\n                info[\"data_files\"][filename] = {\n                    \"path\": str(file_path) if file_path else None,\n                    \"exists\": file_path is not None,\n                    \"using_bundled\": file_path is None,\n                }\n\n            return info\n        except Exception as e:\n            return {\"error\": str(e)}\n\n    @staticmethod\n    def cleanup() -&gt; None:\n        \"\"\"Clean up the registry instance.\"\"\"\n        with ModelRegistry._instance_lock:\n            ModelRegistry._default_instance = None\n\n    def list_providers(self) -&gt; List[str]:\n        \"\"\"List all providers available in the overrides configuration.\n\n        Returns:\n            List of provider names found in overrides data\n        \"\"\"\n        import os\n\n        providers = set()\n\n        # Add the current provider\n        current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n        providers.add(current_provider)\n\n        # Add providers from overrides\n        if hasattr(self, \"_overrides\") and self._overrides:\n            overrides_data = self._overrides.get(\"overrides\", {})\n            for provider_name in overrides_data.keys():\n                providers.add(provider_name.lower())\n\n        return sorted(list(providers))\n\n    def dump_effective(self) -&gt; Dict[str, Any]:\n        \"\"\"Return the fully merged provider-adjusted dataset for the current provider.\n\n        Returns:\n            Dictionary containing the effective model capabilities after provider overrides\n        \"\"\"\n        import os\n        from datetime import datetime\n\n        current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n        effective_data: Dict[str, Any] = {}\n        total_models = 0\n        serialized = 0\n        skipped_for_dump = 0\n\n        for model_name in self.models:\n            total_models += 1\n            try:\n                capabilities = self.get_capabilities(model_name)\n                effective_data[model_name] = {\n                    \"context_window\": {\n                        \"total\": capabilities.context_window,\n                        \"input\": getattr(capabilities, \"input_context_window\", None),\n                        \"output\": capabilities.max_output_tokens,\n                    },\n                    \"pricing\": (\n                        {\n                            \"scheme\": getattr(capabilities.pricing, \"scheme\", \"per_token\"),\n                            \"unit\": getattr(capabilities.pricing, \"unit\", \"million_tokens\"),\n                            \"input_cost_per_unit\": getattr(capabilities.pricing, \"input_cost_per_unit\", 0.0),\n                            \"output_cost_per_unit\": getattr(capabilities.pricing, \"output_cost_per_unit\", 0.0),\n                            \"currency\": getattr(capabilities.pricing, \"currency\", \"USD\"),\n                            \"tiers\": getattr(capabilities.pricing, \"tiers\", None),\n                        }\n                        if getattr(capabilities, \"pricing\", None)\n                        else {\n                            \"scheme\": \"per_token\",\n                            \"unit\": \"million_tokens\",\n                            \"input_cost_per_unit\": 0.0,\n                            \"output_cost_per_unit\": 0.0,\n                            \"currency\": \"USD\",\n                            \"tiers\": None,\n                        }\n                    ),\n                    \"supports_vision\": capabilities.supports_vision,\n                    \"supports_function_calling\": getattr(capabilities, \"supports_functions\", False),\n                    \"supports_streaming\": capabilities.supports_streaming,\n                    \"supports_structured_output\": getattr(capabilities, \"supports_structured\", False),\n                    \"supports_json_mode\": getattr(capabilities, \"supports_json_mode\", False),\n                    \"supports_web_search\": getattr(capabilities, \"supports_web_search\", False),\n                    \"supports_audio\": getattr(capabilities, \"supports_audio\", False),\n                    \"billing\": (\n                        {\"web_search\": asdict(cast(WebSearchBilling, capabilities.web_search_billing))}\n                        if getattr(capabilities, \"web_search_billing\", None)\n                        else None\n                    ),\n                    \"provider\": current_provider,\n                    \"parameters\": getattr(capabilities, \"parameters\", {}),\n                    \"input_modalities\": getattr(\n                        capabilities, \"input_modalities\", getattr(capabilities, \"modalities\", [])\n                    ),\n                    \"output_modalities\": getattr(capabilities, \"output_modalities\", []),\n                    \"deprecation\": {\n                        \"status\": capabilities.deprecation.status,\n                        \"deprecates_on\": getattr(capabilities.deprecation, \"deprecates_on\", None),\n                        \"sunsets_on\": getattr(capabilities.deprecation, \"sunsets_on\", None),\n                        \"replacement\": getattr(capabilities.deprecation, \"replacement\", None),\n                        \"reason\": getattr(capabilities.deprecation, \"reason\", None),\n                        \"migration_guide\": getattr(capabilities.deprecation, \"migration_guide\", None),\n                    },\n                }\n                serialized += 1\n            except Exception as e:\n                skipped_for_dump += 1\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Failed to serialize model for dump_effective\",\n                    model=model_name,\n                    error=str(e),\n                )\n\n        return {\n            \"provider\": current_provider,\n            \"models\": effective_data,\n            \"metadata\": {\n                \"schema_version\": \"1.0.0\",\n                \"generated_at\": str(datetime.now().isoformat()),\n                \"data_sources\": self.get_data_info(),\n                \"summary\": {\n                    \"total\": total_models,\n                    \"serialized\": serialized,\n                    \"skipped\": skipped_for_dump,\n                    \"load_stats\": getattr(self, \"_last_load_stats\", None),\n                },\n            },\n        }\n\n    def get_raw_data_paths(self) -&gt; Dict[str, Optional[str]]:\n        \"\"\"Return canonical paths for raw data files (models.yaml and overrides.yaml).\n\n        Returns:\n            Dictionary with 'models' and 'overrides' keys containing file paths or None if bundled\n        \"\"\"\n        import os\n        from pathlib import Path\n\n        paths: Dict[str, Optional[str]] = {}\n\n        # Get models.yaml path\n        if hasattr(self, \"_data_manager\"):\n            # Try to get the actual file path from data manager\n            user_data_dir = get_user_data_dir()\n            models_path = user_data_dir / \"models.yaml\"\n            if models_path.exists():\n                paths[\"models\"] = str(models_path)\n            else:\n                # Check for environment override\n                env_path = os.getenv(\"OMR_MODEL_REGISTRY_PATH\")\n                if env_path and Path(env_path).exists():\n                    paths[\"models\"] = env_path\n                else:\n                    paths[\"models\"] = None  # Bundled\n\n            # Get overrides.yaml path\n            overrides_path = user_data_dir / \"overrides.yaml\"\n            if overrides_path.exists():\n                paths[\"overrides\"] = str(overrides_path)\n            else:\n                paths[\"overrides\"] = None  # Bundled\n        else:\n            paths[\"models\"] = None\n            paths[\"overrides\"] = None\n\n        return paths\n\n    def clear_cache(self, files: Optional[List[str]] = None) -&gt; None:\n        \"\"\"Delete cached data files in the user data directory.\n\n        Args:\n            files: Optional list of specific files to clear. If None, clears all known cache files.\n        \"\"\"\n        user_data_dir = get_user_data_dir()\n\n        # Default files to clear if none specified\n        if files is None:\n            files = [\"models.yaml\", \"overrides.yaml\"]\n\n        cleared_files = []\n        for filename in files:\n            file_path = user_data_dir / filename\n            try:\n                if file_path.exists():\n                    file_path.unlink()\n                    cleared_files.append(str(file_path))\n            except (OSError, PermissionError) as e:\n                log_warning(LogEvent.MODEL_REGISTRY, f\"Failed to clear cache file {file_path}: {e}\")\n\n        if cleared_files:\n            log_info(LogEvent.MODEL_REGISTRY, f\"Cleared {len(cleared_files)} cache files: {', '.join(cleared_files)}\")\n\n    def get_bundled_data_content(self, filename: str) -&gt; Optional[str]:\n        \"\"\"Get bundled data file content using public APIs.\n\n        Args:\n            filename: Name of the data file (e.g., 'models.yaml', 'overrides.yaml')\n\n        Returns:\n            File content as string, or None if not available\n        \"\"\"\n        if hasattr(self, \"_data_manager\"):\n            return self._data_manager._get_bundled_data_content(filename)\n        return None\n\n    def get_raw_model_data(self, model_name: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get raw model data without provider overrides.\n\n        Args:\n            model_name: Name of the model to get raw data for\n\n        Returns:\n            Raw model data dictionary, or None if not found\n        \"\"\"\n        try:\n            # Get raw models.yaml content\n            raw_paths = self.get_raw_data_paths()\n            models_path = raw_paths.get(\"models\")\n\n            from pathlib import Path\n\n            if models_path and Path(models_path).exists():\n                # Load from user data file\n                with open(models_path, \"r\") as f:\n                    import yaml\n\n                    raw_data = yaml.safe_load(f)\n            else:\n                # Load from bundled data\n                content = self.get_bundled_data_content(\"models.yaml\")\n                if content:\n                    import yaml\n\n                    raw_data = yaml.safe_load(content)\n                else:\n                    return None\n\n            # Extract the specific model from raw data\n            if isinstance(raw_data, dict) and \"models\" in raw_data:\n                models = typing.cast(Dict[str, Any], raw_data[\"models\"])  # ensure typed\n                if model_name in models:\n                    base_obj = models[model_name]\n                    model_data: Dict[str, Any] = dict(base_obj) if isinstance(base_obj, dict) else {}\n                    model_data[\"name\"] = model_name\n                    model_data[\"metadata\"] = {\"source\": \"raw\", \"provider_applied\": None}\n                    return model_data\n\n            return None\n\n        except Exception:\n            return None\n\n    @property\n    def models(self) -&gt; Dict[str, ModelCapabilities]:\n        \"\"\"Get a read-only view of registered models.\"\"\"\n        with self._capabilities_lock:\n            return dict(self._capabilities)\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry-attributes","title":"Attributes","text":""},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.models","title":"<code>models</code>  <code>property</code>","text":"<p>Get a read-only view of registered models.</p>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry-functions","title":"Functions","text":""},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.__init__","title":"<code>__init__(config=None)</code>","text":"<p>Initialize a new registry instance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[RegistryConfig]</code> <p>Configuration for this registry instance. If None, default    configuration is used.</p> <code>None</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def __init__(self, config: Optional[RegistryConfig] = None):\n    \"\"\"Initialize a new registry instance.\n\n    Args:\n        config: Configuration for this registry instance. If None, default\n               configuration is used.\n    \"\"\"\n    self.config = config or RegistryConfig()\n    self._capabilities: Dict[str, ModelCapabilities] = {}\n    self._constraints: Dict[str, Union[NumericConstraint, EnumConstraint, ObjectConstraint]] = {}\n    self._capabilities_lock = threading.RLock()\n    # Stats for last load/dump operations (for observability)\n    self._last_load_stats: Dict[str, Any] = {}\n\n    # Initialize DataManager for model and overrides data\n    self._data_manager = DataManager()\n\n    # Set up caching for get_capabilities\n    self.get_capabilities = functools.lru_cache(maxsize=self.config.cache_size)(self._get_capabilities_impl)\n\n    # Auto-copy default constraint files to user directory if they don't exist\n    if not config or not config.constraints_path:\n        try:\n            copy_default_to_user_config(PARAM_CONSTRAINTS_FILENAME)\n        except OSError as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to copy default constraint config: {e}\",\n                error=str(e),\n            )\n\n    # Check for data updates if auto_update is enabled\n    if self.config.auto_update and self._data_manager.should_update_data():\n        try:\n            success = self._data_manager.check_for_updates()\n            if success:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Auto-update completed successfully\",\n                )\n                # Reload capabilities after successful auto-update\n                self._load_capabilities()\n        except Exception as e:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to auto-update data: {e}\",\n                error=str(e),\n            )\n\n    self._load_constraints()\n    self._load_capabilities()\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.assert_model_active","title":"<code>assert_model_active(model)</code>","text":"<p>Assert that a model is active and warn if deprecated.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name to check</p> required <p>Raises:</p> Type Description <code>ModelSunsetError</code> <p>If the model is sunset</p> <code>ModelNotSupportedError</code> <p>If the model is not found</p> <p>Warns:</p> Type Description <code>DeprecationWarning</code> <p>If the model is deprecated</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def assert_model_active(self, model: str) -&gt; None:\n    \"\"\"Assert that a model is active and warn if deprecated.\n\n    Args:\n        model: Model name to check\n\n    Raises:\n        ModelSunsetError: If the model is sunset\n        ModelNotSupportedError: If the model is not found\n\n    Warns:\n        DeprecationWarning: If the model is deprecated\n    \"\"\"\n    capabilities = self.get_capabilities(model)\n    assert_model_active(model, capabilities.deprecation)\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.check_data_updates","title":"<code>check_data_updates()</code>","text":"<p>Check if data updates are available using DataManager.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if updates are available, False otherwise</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def check_data_updates(self) -&gt; bool:\n    \"\"\"Check if data updates are available using DataManager.\n\n    Returns:\n        True if updates are available, False otherwise\n    \"\"\"\n    try:\n        if not self._data_manager.should_update_data():\n            return False\n\n        latest_release = self._data_manager._fetch_latest_data_release()\n        if not latest_release:\n            return False\n\n        latest_version = latest_release.get(\"tag_name\", \"\")\n        current_version = self._data_manager._get_current_version()\n\n        return not (current_version and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0)\n    except Exception:\n        return False\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.check_for_updates","title":"<code>check_for_updates(url=None)</code>","text":"<p>Check if updates are available for the model registry.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Optional[str]</code> <p>Optional custom URL to check for updates</p> <code>None</code> <p>Returns:</p> Type Description <code>RefreshResult</code> <p>Result of the update check</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def check_for_updates(self, url: Optional[str] = None) -&gt; RefreshResult:\n    \"\"\"Check if updates are available for the model registry.\n\n    Args:\n        url: Optional custom URL to check for updates\n\n    Returns:\n        Result of the update check\n    \"\"\"\n    try:\n        import requests\n    except ImportError:\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=\"Could not import requests module\",\n        )\n\n    # Set up the URL with fallback handling\n    primary_url = url or (\n        \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n    )\n\n    # Define fallback URLs in case primary fails\n    fallback_urls = [\n        \"https://github.com/yaniv-golan/openai-model-registry/raw/main/data/models.yaml\",\n    ]\n\n    urls_to_try = [primary_url] + fallback_urls\n\n    try:\n        # Use a lock when checking and comparing versions to prevent race conditions\n        with self.__class__._instance_lock:\n            # First check with DataManager\n            if self._data_manager.should_update_data():\n                latest_release = self._data_manager._fetch_latest_data_release()\n                if latest_release:\n                    latest_version = latest_release.get(\"tag_name\", \"\")\n                    current_version = self._data_manager._get_current_version()\n                    if (\n                        current_version\n                        and self._data_manager._compare_versions(latest_version, current_version) &lt;= 0\n                    ):\n                        return RefreshResult(\n                            success=True,\n                            status=RefreshStatus.ALREADY_CURRENT,\n                            message=f\"Registry is up to date (version {current_version})\",\n                        )\n                    else:\n                        return RefreshResult(\n                            success=True,\n                            status=RefreshStatus.UPDATE_AVAILABLE,\n                            message=f\"Update available: {current_version or 'bundled'} -&gt; {latest_version}\",\n                        )\n\n            # Fallback to original HTTP check with URL fallback\n            remote_config = None\n            for config_url in urls_to_try:\n                try:\n                    response = requests.get(config_url, timeout=10)\n                    response.raise_for_status()\n\n                    # Parse the remote config\n                    remote_config = yaml.safe_load(response.text)\n                    if isinstance(remote_config, dict):\n                        break\n                    else:\n                        log_warning(\n                            LogEvent.MODEL_REGISTRY,\n                            f\"Remote config from {config_url} is not a valid dictionary\",\n                        )\n                except (requests.RequestException, yaml.YAMLError) as e:\n                    log_warning(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"Failed to fetch from {config_url}: {e}\",\n                    )\n                    continue\n\n            if remote_config is None:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=\"Could not fetch remote config from any URL\",\n                )\n\n            # Get local config for comparison\n            local_config = self._load_config()\n            if not local_config.success:\n                return RefreshResult(\n                    success=False,\n                    status=RefreshStatus.ERROR,\n                    message=f\"Could not load local config: {local_config.error}\",\n                )\n\n            # Compare versions (simplified comparison)\n            remote_version = remote_config.get(\"version\", \"unknown\")\n            local_version = local_config.data.get(\"version\", \"unknown\") if local_config.data else \"unknown\"\n\n            if remote_version == local_version:\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.ALREADY_CURRENT,\n                    message=f\"Registry is up to date (version {local_version})\",\n                )\n            else:\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.UPDATE_AVAILABLE,\n                    message=f\"Update available: {local_version} -&gt; {remote_version}\",\n                )\n\n    except requests.HTTPError as e:\n        if e.response.status_code == 404:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=\"Registry not found at any of the configured URLs\",\n            )\n        else:\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"HTTP error {e.response.status_code}: {e}\",\n            )\n    except requests.RequestException as e:\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=f\"Network error: {e}\",\n        )\n    except Exception as e:\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=f\"Unexpected error: {e}\",\n        )\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.cleanup","title":"<code>cleanup()</code>  <code>staticmethod</code>","text":"<p>Clean up the registry instance.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@staticmethod\ndef cleanup() -&gt; None:\n    \"\"\"Clean up the registry instance.\"\"\"\n    with ModelRegistry._instance_lock:\n        ModelRegistry._default_instance = None\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.clear_cache","title":"<code>clear_cache(files=None)</code>","text":"<p>Delete cached data files in the user data directory.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>Optional[List[str]]</code> <p>Optional list of specific files to clear. If None, clears all known cache files.</p> <code>None</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def clear_cache(self, files: Optional[List[str]] = None) -&gt; None:\n    \"\"\"Delete cached data files in the user data directory.\n\n    Args:\n        files: Optional list of specific files to clear. If None, clears all known cache files.\n    \"\"\"\n    user_data_dir = get_user_data_dir()\n\n    # Default files to clear if none specified\n    if files is None:\n        files = [\"models.yaml\", \"overrides.yaml\"]\n\n    cleared_files = []\n    for filename in files:\n        file_path = user_data_dir / filename\n        try:\n            if file_path.exists():\n                file_path.unlink()\n                cleared_files.append(str(file_path))\n        except (OSError, PermissionError) as e:\n            log_warning(LogEvent.MODEL_REGISTRY, f\"Failed to clear cache file {file_path}: {e}\")\n\n    if cleared_files:\n        log_info(LogEvent.MODEL_REGISTRY, f\"Cleared {len(cleared_files)} cache files: {', '.join(cleared_files)}\")\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.dump_effective","title":"<code>dump_effective()</code>","text":"<p>Return the fully merged provider-adjusted dataset for the current provider.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing the effective model capabilities after provider overrides</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def dump_effective(self) -&gt; Dict[str, Any]:\n    \"\"\"Return the fully merged provider-adjusted dataset for the current provider.\n\n    Returns:\n        Dictionary containing the effective model capabilities after provider overrides\n    \"\"\"\n    import os\n    from datetime import datetime\n\n    current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n    effective_data: Dict[str, Any] = {}\n    total_models = 0\n    serialized = 0\n    skipped_for_dump = 0\n\n    for model_name in self.models:\n        total_models += 1\n        try:\n            capabilities = self.get_capabilities(model_name)\n            effective_data[model_name] = {\n                \"context_window\": {\n                    \"total\": capabilities.context_window,\n                    \"input\": getattr(capabilities, \"input_context_window\", None),\n                    \"output\": capabilities.max_output_tokens,\n                },\n                \"pricing\": (\n                    {\n                        \"scheme\": getattr(capabilities.pricing, \"scheme\", \"per_token\"),\n                        \"unit\": getattr(capabilities.pricing, \"unit\", \"million_tokens\"),\n                        \"input_cost_per_unit\": getattr(capabilities.pricing, \"input_cost_per_unit\", 0.0),\n                        \"output_cost_per_unit\": getattr(capabilities.pricing, \"output_cost_per_unit\", 0.0),\n                        \"currency\": getattr(capabilities.pricing, \"currency\", \"USD\"),\n                        \"tiers\": getattr(capabilities.pricing, \"tiers\", None),\n                    }\n                    if getattr(capabilities, \"pricing\", None)\n                    else {\n                        \"scheme\": \"per_token\",\n                        \"unit\": \"million_tokens\",\n                        \"input_cost_per_unit\": 0.0,\n                        \"output_cost_per_unit\": 0.0,\n                        \"currency\": \"USD\",\n                        \"tiers\": None,\n                    }\n                ),\n                \"supports_vision\": capabilities.supports_vision,\n                \"supports_function_calling\": getattr(capabilities, \"supports_functions\", False),\n                \"supports_streaming\": capabilities.supports_streaming,\n                \"supports_structured_output\": getattr(capabilities, \"supports_structured\", False),\n                \"supports_json_mode\": getattr(capabilities, \"supports_json_mode\", False),\n                \"supports_web_search\": getattr(capabilities, \"supports_web_search\", False),\n                \"supports_audio\": getattr(capabilities, \"supports_audio\", False),\n                \"billing\": (\n                    {\"web_search\": asdict(cast(WebSearchBilling, capabilities.web_search_billing))}\n                    if getattr(capabilities, \"web_search_billing\", None)\n                    else None\n                ),\n                \"provider\": current_provider,\n                \"parameters\": getattr(capabilities, \"parameters\", {}),\n                \"input_modalities\": getattr(\n                    capabilities, \"input_modalities\", getattr(capabilities, \"modalities\", [])\n                ),\n                \"output_modalities\": getattr(capabilities, \"output_modalities\", []),\n                \"deprecation\": {\n                    \"status\": capabilities.deprecation.status,\n                    \"deprecates_on\": getattr(capabilities.deprecation, \"deprecates_on\", None),\n                    \"sunsets_on\": getattr(capabilities.deprecation, \"sunsets_on\", None),\n                    \"replacement\": getattr(capabilities.deprecation, \"replacement\", None),\n                    \"reason\": getattr(capabilities.deprecation, \"reason\", None),\n                    \"migration_guide\": getattr(capabilities.deprecation, \"migration_guide\", None),\n                },\n            }\n            serialized += 1\n        except Exception as e:\n            skipped_for_dump += 1\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to serialize model for dump_effective\",\n                model=model_name,\n                error=str(e),\n            )\n\n    return {\n        \"provider\": current_provider,\n        \"models\": effective_data,\n        \"metadata\": {\n            \"schema_version\": \"1.0.0\",\n            \"generated_at\": str(datetime.now().isoformat()),\n            \"data_sources\": self.get_data_info(),\n            \"summary\": {\n                \"total\": total_models,\n                \"serialized\": serialized,\n                \"skipped\": skipped_for_dump,\n                \"load_stats\": getattr(self, \"_last_load_stats\", None),\n            },\n        },\n    }\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_bundled_data_content","title":"<code>get_bundled_data_content(filename)</code>","text":"<p>Get bundled data file content using public APIs.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the data file (e.g., 'models.yaml', 'overrides.yaml')</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>File content as string, or None if not available</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_bundled_data_content(self, filename: str) -&gt; Optional[str]:\n    \"\"\"Get bundled data file content using public APIs.\n\n    Args:\n        filename: Name of the data file (e.g., 'models.yaml', 'overrides.yaml')\n\n    Returns:\n        File content as string, or None if not available\n    \"\"\"\n    if hasattr(self, \"_data_manager\"):\n        return self._data_manager._get_bundled_data_content(filename)\n    return None\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_data_info","title":"<code>get_data_info()</code>","text":"<p>Get information about data configuration and status.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing data configuration information</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_data_info(self) -&gt; Dict[str, Any]:\n    \"\"\"Get information about data configuration and status.\n\n    Returns:\n        Dictionary containing data configuration information\n    \"\"\"\n    try:\n        info: Dict[str, Any] = {\n            \"data_directory\": str(self._data_manager._data_dir),\n            \"current_version\": self._data_manager._get_current_version(),\n            \"updates_enabled\": self._data_manager.should_update_data(),\n            \"environment_variables\": {\n                \"OMR_DISABLE_DATA_UPDATES\": os.getenv(\"OMR_DISABLE_DATA_UPDATES\"),\n                \"OMR_DATA_VERSION_PIN\": os.getenv(\"OMR_DATA_VERSION_PIN\"),\n                \"OMR_DATA_DIR\": os.getenv(\"OMR_DATA_DIR\"),\n            },\n            \"data_files\": {},\n        }\n\n        # Check data file status\n        for filename in [\"models.yaml\", \"overrides.yaml\"]:\n            file_path = self._data_manager.get_data_file_path(filename)\n            info[\"data_files\"][filename] = {\n                \"path\": str(file_path) if file_path else None,\n                \"exists\": file_path is not None,\n                \"using_bundled\": file_path is None,\n            }\n\n        return info\n    except Exception as e:\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_data_version","title":"<code>get_data_version()</code>","text":"<p>Get the current data version.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Current data version string, or None if using bundled data</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_data_version(self) -&gt; Optional[str]:\n    \"\"\"Get the current data version.\n\n    Returns:\n        Current data version string, or None if using bundled data\n    \"\"\"\n    try:\n        return self._data_manager._get_current_version()\n    except Exception:\n        return None\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_default","title":"<code>get_default()</code>  <code>classmethod</code>","text":"<p>Get the default registry instance with standard configuration.</p> <p>Returns:</p> Type Description <code>ModelRegistry</code> <p>The default ModelRegistry instance</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@classmethod\ndef get_default(cls) -&gt; \"ModelRegistry\":\n    \"\"\"Get the default registry instance with standard configuration.\n\n    Returns:\n        The default ModelRegistry instance\n    \"\"\"\n    with cls._instance_lock:\n        if cls._default_instance is None:\n            cls._default_instance = cls()\n        return cls._default_instance\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_instance","title":"<code>get_instance()</code>  <code>classmethod</code>","text":"<p>Get the default registry instance.</p> <p>Prefer :py:meth:<code>get_default</code> for clarity; this alias remains for brevity and historical usage but is not a separate code path.</p> <p>Returns:</p> Type Description <code>ModelRegistry</code> <p>The singleton :class:<code>ModelRegistry</code> instance.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@classmethod\ndef get_instance(cls) -&gt; \"ModelRegistry\":\n    \"\"\"Get the default registry instance.\n\n    Prefer :py:meth:`get_default` for clarity; this alias remains for\n    brevity and historical usage but is *not* a separate code path.\n\n    Returns:\n        The singleton :class:`ModelRegistry` instance.\n    \"\"\"\n    return cls.get_default()\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_parameter_constraint","title":"<code>get_parameter_constraint(ref)</code>","text":"<p>Get a parameter constraint by reference.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>str</code> <p>Reference string (e.g., \"numeric_constraints.temperature\")</p> required <p>Returns:</p> Type Description <code>Union[NumericConstraint, EnumConstraint, ObjectConstraint]</code> <p>The constraint object (NumericConstraint or EnumConstraint or ObjectConstraint)</p> <p>Raises:</p> Type Description <code>ConstraintNotFoundError</code> <p>If the constraint is not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_parameter_constraint(self, ref: str) -&gt; Union[NumericConstraint, EnumConstraint, ObjectConstraint]:\n    \"\"\"Get a parameter constraint by reference.\n\n    Args:\n        ref: Reference string (e.g., \"numeric_constraints.temperature\")\n\n    Returns:\n        The constraint object (NumericConstraint or EnumConstraint or ObjectConstraint)\n\n    Raises:\n        ConstraintNotFoundError: If the constraint is not found\n    \"\"\"\n    if ref not in self._constraints:\n        raise ConstraintNotFoundError(\n            f\"Constraint reference '{ref}' not found in registry\",\n            ref=ref,\n        )\n    return self._constraints[ref]\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_raw_data_paths","title":"<code>get_raw_data_paths()</code>","text":"<p>Return canonical paths for raw data files (models.yaml and overrides.yaml).</p> <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dictionary with 'models' and 'overrides' keys containing file paths or None if bundled</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_raw_data_paths(self) -&gt; Dict[str, Optional[str]]:\n    \"\"\"Return canonical paths for raw data files (models.yaml and overrides.yaml).\n\n    Returns:\n        Dictionary with 'models' and 'overrides' keys containing file paths or None if bundled\n    \"\"\"\n    import os\n    from pathlib import Path\n\n    paths: Dict[str, Optional[str]] = {}\n\n    # Get models.yaml path\n    if hasattr(self, \"_data_manager\"):\n        # Try to get the actual file path from data manager\n        user_data_dir = get_user_data_dir()\n        models_path = user_data_dir / \"models.yaml\"\n        if models_path.exists():\n            paths[\"models\"] = str(models_path)\n        else:\n            # Check for environment override\n            env_path = os.getenv(\"OMR_MODEL_REGISTRY_PATH\")\n            if env_path and Path(env_path).exists():\n                paths[\"models\"] = env_path\n            else:\n                paths[\"models\"] = None  # Bundled\n\n        # Get overrides.yaml path\n        overrides_path = user_data_dir / \"overrides.yaml\"\n        if overrides_path.exists():\n            paths[\"overrides\"] = str(overrides_path)\n        else:\n            paths[\"overrides\"] = None  # Bundled\n    else:\n        paths[\"models\"] = None\n        paths[\"overrides\"] = None\n\n    return paths\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_raw_model_data","title":"<code>get_raw_model_data(model_name)</code>","text":"<p>Get raw model data without provider overrides.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to get raw data for</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Raw model data dictionary, or None if not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_raw_model_data(self, model_name: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get raw model data without provider overrides.\n\n    Args:\n        model_name: Name of the model to get raw data for\n\n    Returns:\n        Raw model data dictionary, or None if not found\n    \"\"\"\n    try:\n        # Get raw models.yaml content\n        raw_paths = self.get_raw_data_paths()\n        models_path = raw_paths.get(\"models\")\n\n        from pathlib import Path\n\n        if models_path and Path(models_path).exists():\n            # Load from user data file\n            with open(models_path, \"r\") as f:\n                import yaml\n\n                raw_data = yaml.safe_load(f)\n        else:\n            # Load from bundled data\n            content = self.get_bundled_data_content(\"models.yaml\")\n            if content:\n                import yaml\n\n                raw_data = yaml.safe_load(content)\n            else:\n                return None\n\n        # Extract the specific model from raw data\n        if isinstance(raw_data, dict) and \"models\" in raw_data:\n            models = typing.cast(Dict[str, Any], raw_data[\"models\"])  # ensure typed\n            if model_name in models:\n                base_obj = models[model_name]\n                model_data: Dict[str, Any] = dict(base_obj) if isinstance(base_obj, dict) else {}\n                model_data[\"name\"] = model_name\n                model_data[\"metadata\"] = {\"source\": \"raw\", \"provider_applied\": None}\n                return model_data\n\n        return None\n\n    except Exception:\n        return None\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_sunset_headers","title":"<code>get_sunset_headers(model)</code>","text":"<p>Get RFC-compliant HTTP headers for model deprecation status.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model name</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary of HTTP headers</p> <p>Raises:</p> Type Description <code>ModelNotSupportedError</code> <p>If the model is not found</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_sunset_headers(self, model: str) -&gt; dict[str, str]:\n    \"\"\"Get RFC-compliant HTTP headers for model deprecation status.\n\n    Args:\n        model: Model name\n\n    Returns:\n        Dictionary of HTTP headers\n\n    Raises:\n        ModelNotSupportedError: If the model is not found\n    \"\"\"\n    capabilities = self.get_capabilities(model)\n    return sunset_headers(capabilities.deprecation)\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.get_update_info","title":"<code>get_update_info()</code>","text":"<p>Get detailed information about available updates.</p> <p>Returns:</p> Type Description <code>UpdateInfo</code> <p>UpdateInfo object containing update details</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_update_info(self) -&gt; UpdateInfo:\n    \"\"\"Get detailed information about available updates.\n\n    Returns:\n        UpdateInfo object containing update details\n    \"\"\"\n    try:\n        if not self._data_manager.should_update_data():\n            return UpdateInfo(\n                update_available=False,\n                current_version=self._data_manager._get_current_version(),\n                current_version_date=None,\n                latest_version=None,\n                latest_version_date=None,\n                download_url=None,\n                update_size_estimate=None,\n                latest_version_description=None,\n                accumulated_changes=[],\n                error_message=\"Updates are disabled via environment variable\",\n            )\n\n        latest_release = self._data_manager._fetch_latest_data_release()\n        if not latest_release:\n            return UpdateInfo(\n                update_available=False,\n                current_version=self._data_manager._get_current_version(),\n                current_version_date=None,\n                latest_version=None,\n                latest_version_date=None,\n                download_url=None,\n                update_size_estimate=None,\n                latest_version_description=None,\n                accumulated_changes=[],\n                error_message=\"No releases found on GitHub\",\n            )\n\n        latest_version_raw = latest_release.get(\"tag_name\", \"\")\n        current_version = self._data_manager._get_current_version()\n\n        # Clean the latest version to match current version format (remove data-v prefix)\n        latest_version = latest_version_raw\n        if latest_version_raw.startswith(\"data-v\"):\n            latest_version = latest_version_raw[len(\"data-v\") :]\n\n        # Get current version info with date\n        current_version_info = self._data_manager._get_current_version_info()\n        current_version_date = current_version_info.get(\"published_at\") if current_version_info else None\n\n        update_available = not (\n            current_version and self._data_manager._compare_versions(latest_version_raw, current_version) &lt;= 0\n        )\n\n        # Get accumulated changes between current and latest version\n        accumulated_changes = []\n        if update_available:\n            accumulated_changes = self._data_manager.get_accumulated_changes(current_version, latest_version_raw)\n\n        # Estimate update size based on assets\n        update_size_estimate = None\n        total_size = 0\n        assets = latest_release.get(\"assets\", [])\n        for asset in assets:\n            if asset.get(\"name\") in [\"models.yaml\", \"overrides.yaml\"]:\n                total_size += asset.get(\"size\", 0)\n\n        if total_size &gt; 0:\n            if total_size &lt; 1024:\n                update_size_estimate = f\"{total_size} bytes\"\n            elif total_size &lt; 1024 * 1024:\n                update_size_estimate = f\"{total_size / 1024:.1f} KB\"\n            else:\n                update_size_estimate = f\"{total_size / (1024 * 1024):.1f} MB\"\n\n        # Extract one-sentence description from latest release body\n        latest_version_description = None\n        if latest_release.get(\"body\"):\n            latest_version_description = self._data_manager._extract_change_summary(latest_release.get(\"body\", \"\"))\n\n        return UpdateInfo(\n            update_available=update_available,\n            current_version=current_version,\n            current_version_date=current_version_date,\n            latest_version=latest_version,\n            latest_version_date=latest_release.get(\"published_at\"),\n            download_url=latest_release.get(\"html_url\"),\n            update_size_estimate=update_size_estimate,\n            latest_version_description=latest_version_description,\n            accumulated_changes=accumulated_changes,\n            error_message=None,\n        )\n\n    except Exception as e:\n        return UpdateInfo(\n            update_available=False,\n            current_version=self._data_manager._get_current_version(),\n            current_version_date=None,\n            latest_version=None,\n            latest_version_date=None,\n            download_url=None,\n            update_size_estimate=None,\n            latest_version_description=None,\n            accumulated_changes=[],\n            error_message=str(e),\n        )\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.list_providers","title":"<code>list_providers()</code>","text":"<p>List all providers available in the overrides configuration.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of provider names found in overrides data</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def list_providers(self) -&gt; List[str]:\n    \"\"\"List all providers available in the overrides configuration.\n\n    Returns:\n        List of provider names found in overrides data\n    \"\"\"\n    import os\n\n    providers = set()\n\n    # Add the current provider\n    current_provider = os.getenv(\"OMR_PROVIDER\", \"openai\").lower()\n    providers.add(current_provider)\n\n    # Add providers from overrides\n    if hasattr(self, \"_overrides\") and self._overrides:\n        overrides_data = self._overrides.get(\"overrides\", {})\n        for provider_name in overrides_data.keys():\n            providers.add(provider_name.lower())\n\n    return sorted(list(providers))\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.manual_update_workflow","title":"<code>manual_update_workflow(prompt_user_func=None)</code>","text":"<p>Manual update workflow with user approval.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_user_func</code> <code>Optional[Callable[[UpdateInfo], bool]]</code> <p>Optional function to prompt user for approval.             Should take UpdateInfo as parameter and return bool.             If None, uses a default console prompt.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was performed, False otherwise</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def manual_update_workflow(self, prompt_user_func: Optional[Callable[[UpdateInfo], bool]] = None) -&gt; bool:\n    \"\"\"Manual update workflow with user approval.\n\n    Args:\n        prompt_user_func: Optional function to prompt user for approval.\n                        Should take UpdateInfo as parameter and return bool.\n                        If None, uses a default console prompt.\n\n    Returns:\n        True if update was performed, False otherwise\n    \"\"\"\n    try:\n        # Get update information\n        update_info = self.get_update_info()\n\n        if update_info.error_message:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                f\"Failed to check for updates: {update_info.error_message}\",\n            )\n            return False\n\n        if not update_info.update_available:\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                f\"Registry is up to date (version {update_info.current_version or 'bundled'})\",\n            )\n            return False\n\n        # Use custom prompt function or default\n        if prompt_user_func is None:\n            prompt_user_func = self._default_update_prompt\n\n        # Ask user for approval\n        if prompt_user_func(update_info):\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                f\"User approved update from {update_info.current_version or 'bundled'} to {update_info.latest_version}\",\n            )\n\n            # Perform the update\n            success = self.update_data()\n\n            if success:\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    f\"Successfully updated to {update_info.latest_version}\",\n                )\n            else:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Update failed\",\n                )\n\n            return success\n        else:\n            log_info(\n                LogEvent.MODEL_REGISTRY,\n                \"User declined update\",\n            )\n            return False\n\n    except Exception as e:\n        log_error(\n            LogEvent.MODEL_REGISTRY,\n            f\"Manual update workflow failed: {e}\",\n        )\n        return False\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.refresh_from_remote","title":"<code>refresh_from_remote(url=None, force=False, validate_only=False)</code>","text":"<p>Refresh the registry configuration from remote source.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Optional[str]</code> <p>Optional custom URL to fetch registry from</p> <code>None</code> <code>force</code> <code>bool</code> <p>Force refresh even if version is current</p> <code>False</code> <code>validate_only</code> <code>bool</code> <p>Only validate remote config without updating</p> <code>False</code> <p>Returns:</p> Type Description <code>RefreshResult</code> <p>Result of the refresh operation</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def refresh_from_remote(\n    self,\n    url: Optional[str] = None,\n    force: bool = False,\n    validate_only: bool = False,\n) -&gt; RefreshResult:\n    \"\"\"Refresh the registry configuration from remote source.\n\n    Args:\n        url: Optional custom URL to fetch registry from\n        force: Force refresh even if version is current\n        validate_only: Only validate remote config without updating\n\n    Returns:\n        Result of the refresh operation\n    \"\"\"\n    try:\n        # Get remote config\n        config_url = url or (\n            \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/models.yaml\"\n        )\n        remote_config = self._fetch_remote_config(config_url)\n        if not remote_config:\n            raise ValueError(\"Failed to fetch remote configuration\")\n\n        # Validate the remote config\n        self._validate_remote_config(remote_config)\n\n        if validate_only:\n            # Only validation was requested\n            return RefreshResult(\n                success=True,\n                status=RefreshStatus.VALIDATED,\n                message=\"Remote registry configuration validated successfully\",\n            )\n\n        # Check for updates only if not forcing and not validating\n        if not force:\n            result = self.check_for_updates(url=url)\n            if result.status == RefreshStatus.ALREADY_CURRENT:\n                return RefreshResult(\n                    success=True,\n                    status=RefreshStatus.ALREADY_CURRENT,\n                    message=\"Registry is already up to date\",\n                )\n\n        # Use DataManager to handle the update\n        try:\n            # Force update through DataManager\n            if self._data_manager.force_update():\n                log_info(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Successfully updated registry data via DataManager\",\n                )\n            else:\n                # Fallback to manual update if DataManager fails\n                # Note: This fallback downloads models.yaml and attempts overrides.yaml\n                log_warning(\n                    LogEvent.MODEL_REGISTRY,\n                    \"DataManager update failed, using limited fallback (models.yaml only)\",\n                )\n                target_path = get_user_data_dir() / \"models.yaml\"\n                with open(target_path, \"w\") as f:\n                    yaml.safe_dump(remote_config, f)\n\n                # Try to download overrides.yaml if possible\n                try:\n                    overrides_url = \"https://raw.githubusercontent.com/yaniv-golan/openai-model-registry/main/data/overrides.yaml\"\n\n                    # Simple fallback downloads\n                    try:\n                        import requests\n                    except ImportError:\n                        requests = None  # type: ignore\n\n                    if requests is not None:\n                        try:\n                            # Download overrides.yaml\n                            overrides_resp = requests.get(overrides_url, timeout=30)\n                            if overrides_resp.status_code == 200:\n                                overrides_content = overrides_resp.text\n                                overrides_path = get_user_data_dir() / \"overrides.yaml\"\n                                with open(overrides_path, \"w\") as f:\n                                    f.write(overrides_content)\n                                log_info(\n                                    LogEvent.MODEL_REGISTRY,\n                                    \"Downloaded overrides.yaml in fallback\",\n                                )\n\n                        except requests.RequestException as e:\n                            log_warning(\n                                LogEvent.MODEL_REGISTRY,\n                                f\"Failed to download additional files in fallback: {e}\",\n                            )\n                except Exception as e:\n                    log_warning(\n                        LogEvent.MODEL_REGISTRY,\n                        f\"Error in fallback additional file download: {e}\",\n                    )\n        except PermissionError as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Permission denied when writing registry configuration\",\n                path=str(target_path),\n                error=str(e),\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Permission denied when writing to {target_path}\",\n            )\n        except OSError as e:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"File system error when writing registry configuration\",\n                path=str(target_path),\n                error=str(e),\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=f\"Error writing to {target_path}: {str(e)}\",\n            )\n\n        # Reload the registry with new configuration\n        self._load_constraints()\n        self._load_capabilities()\n\n        # Verify that the reload was successful\n        if not self._capabilities:\n            log_error(\n                LogEvent.MODEL_REGISTRY,\n                \"Failed to reload registry after update\",\n                path=str(target_path),\n            )\n            return RefreshResult(\n                success=False,\n                status=RefreshStatus.ERROR,\n                message=\"Registry update failed: could not load capabilities after update\",\n            )\n\n        # Log success\n        log_info(\n            LogEvent.MODEL_REGISTRY,\n            \"Registry updated from remote\",\n            version=remote_config.get(\"version\", \"unknown\"),\n        )\n\n        return RefreshResult(\n            success=True,\n            status=RefreshStatus.UPDATED,\n            message=\"Registry updated successfully\",\n        )\n\n    except Exception as e:\n        error_msg = f\"Error refreshing registry: {str(e)}\"\n        log_error(\n            LogEvent.MODEL_REGISTRY,\n            error_msg,\n        )\n        return RefreshResult(\n            success=False,\n            status=RefreshStatus.ERROR,\n            message=error_msg,\n        )\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.ModelRegistry.update_data","title":"<code>update_data(force=False)</code>","text":"<p>Update model registry data using DataManager.</p> <p>Parameters:</p> Name Type Description Default <code>force</code> <code>bool</code> <p>If True, force update regardless of current version</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if update was successful, False otherwise</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def update_data(self, force: bool = False) -&gt; bool:\n    \"\"\"Update model registry data using DataManager.\n\n    Args:\n        force: If True, force update regardless of current version\n\n    Returns:\n        True if update was successful, False otherwise\n    \"\"\"\n    try:\n        if force:\n            success = self._data_manager.force_update()\n        else:\n            success = self._data_manager.check_for_updates()\n\n        if success:\n            # Reload capabilities after successful update\n            self._load_capabilities()\n\n        return success\n    except Exception:\n        return False\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.RefreshResult","title":"<code>RefreshResult</code>  <code>dataclass</code>","text":"<p>Result of a registry refresh operation.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@dataclass\nclass RefreshResult:\n    \"\"\"Result of a registry refresh operation.\"\"\"\n\n    success: bool\n    status: RefreshStatus\n    message: str\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.RefreshStatus","title":"<code>RefreshStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Status of a registry refresh operation.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class RefreshStatus(Enum):\n    \"\"\"Status of a registry refresh operation.\"\"\"\n\n    UPDATED = \"updated\"\n    ALREADY_CURRENT = \"already_current\"\n    ERROR = \"error\"\n    VALIDATED = \"validated\"\n    UPDATE_AVAILABLE = \"update_available\"\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.RegistryConfig","title":"<code>RegistryConfig</code>","text":"<p>Configuration for the model registry.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class RegistryConfig:\n    \"\"\"Configuration for the model registry.\"\"\"\n\n    def __init__(\n        self,\n        registry_path: Optional[str] = None,\n        constraints_path: Optional[str] = None,\n        auto_update: bool = False,\n        cache_size: int = 100,\n    ):\n        \"\"\"Initialize registry configuration.\n\n        Args:\n            registry_path: Custom path to registry YAML file. If None,\n                           default location is used.\n            constraints_path: Custom path to constraints YAML file. If None,\n                              default location is used.\n            auto_update: Whether to automatically update the registry.\n            cache_size: Size of model capabilities cache.\n        \"\"\"\n        self.registry_path = registry_path  # Will be handled by DataManager\n        self.constraints_path = constraints_path or get_parameter_constraints_path()\n        self.auto_update = auto_update\n\n        # Validate cache_size bounds to prevent excessive memory usage\n        if cache_size &lt; 1:\n            raise ValueError(\"cache_size must be at least 1\")\n        if cache_size &gt; 10000:\n            raise ValueError(\"cache_size must not exceed 10000 to prevent excessive memory usage\")\n        self.cache_size = cache_size\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.RegistryConfig-functions","title":"Functions","text":""},{"location":"api_reference/registry/#openai_model_registry.registry.RegistryConfig.__init__","title":"<code>__init__(registry_path=None, constraints_path=None, auto_update=False, cache_size=100)</code>","text":"<p>Initialize registry configuration.</p> <p>Parameters:</p> Name Type Description Default <code>registry_path</code> <code>Optional[str]</code> <p>Custom path to registry YAML file. If None,            default location is used.</p> <code>None</code> <code>constraints_path</code> <code>Optional[str]</code> <p>Custom path to constraints YAML file. If None,               default location is used.</p> <code>None</code> <code>auto_update</code> <code>bool</code> <p>Whether to automatically update the registry.</p> <code>False</code> <code>cache_size</code> <code>int</code> <p>Size of model capabilities cache.</p> <code>100</code> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def __init__(\n    self,\n    registry_path: Optional[str] = None,\n    constraints_path: Optional[str] = None,\n    auto_update: bool = False,\n    cache_size: int = 100,\n):\n    \"\"\"Initialize registry configuration.\n\n    Args:\n        registry_path: Custom path to registry YAML file. If None,\n                       default location is used.\n        constraints_path: Custom path to constraints YAML file. If None,\n                          default location is used.\n        auto_update: Whether to automatically update the registry.\n        cache_size: Size of model capabilities cache.\n    \"\"\"\n    self.registry_path = registry_path  # Will be handled by DataManager\n    self.constraints_path = constraints_path or get_parameter_constraints_path()\n    self.auto_update = auto_update\n\n    # Validate cache_size bounds to prevent excessive memory usage\n    if cache_size &lt; 1:\n        raise ValueError(\"cache_size must be at least 1\")\n    if cache_size &gt; 10000:\n        raise ValueError(\"cache_size must not exceed 10000 to prevent excessive memory usage\")\n    self.cache_size = cache_size\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.RegistryUpdateResult","title":"<code>RegistryUpdateResult</code>  <code>dataclass</code>","text":"<p>Result of a registry update operation.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@dataclass\nclass RegistryUpdateResult:\n    \"\"\"Result of a registry update operation.\"\"\"\n\n    success: bool\n    status: RegistryUpdateStatus\n    message: str\n    url: Optional[str] = None\n    error: Optional[Exception] = None\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.RegistryUpdateStatus","title":"<code>RegistryUpdateStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Status of a registry update operation.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class RegistryUpdateStatus(Enum):\n    \"\"\"Status of a registry update operation.\"\"\"\n\n    UPDATED = \"updated\"\n    ALREADY_CURRENT = \"already_current\"\n    NOT_FOUND = \"not_found\"\n    NETWORK_ERROR = \"network_error\"\n    PERMISSION_ERROR = \"permission_error\"\n    IMPORT_ERROR = \"import_error\"\n    UNKNOWN_ERROR = \"unknown_error\"\n    UPDATE_AVAILABLE = \"update_available\"\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.UpdateInfo","title":"<code>UpdateInfo</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Information about available updates.</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>class UpdateInfo(NamedTuple):\n    \"\"\"Information about available updates.\"\"\"\n\n    update_available: bool\n    current_version: Optional[str]\n    current_version_date: Optional[str]\n    latest_version: Optional[str]\n    latest_version_date: Optional[str]\n    download_url: Optional[str]\n    update_size_estimate: Optional[str]\n    latest_version_description: Optional[str]\n    accumulated_changes: List[Dict[str, Any]]\n    error_message: Optional[str] = None\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry.WebSearchBilling","title":"<code>WebSearchBilling</code>  <code>dataclass</code>","text":"<p>Web search billing policy and rates for a model.</p> <ul> <li>call_fee_per_1000: flat fee per 1000 calls</li> <li>content_token_policy: whether content tokens are included or billed at model rate</li> <li>currency: ISO currency code (default USD)</li> <li>notes: optional free-form notes</li> </ul> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>@dataclass(frozen=True)\nclass WebSearchBilling:\n    \"\"\"Web search billing policy and rates for a model.\n\n    - call_fee_per_1000: flat fee per 1000 calls\n    - content_token_policy: whether content tokens are included or billed at model rate\n    - currency: ISO currency code (default USD)\n    - notes: optional free-form notes\n    \"\"\"\n\n    call_fee_per_1000: float\n    content_token_policy: Literal[\"included_in_call_fee\", \"billed_at_model_rate\"]\n    currency: str = \"USD\"\n    notes: Optional[str] = None\n\n    def __post_init__(self) -&gt; None:\n        if self.call_fee_per_1000 &lt; 0:\n            raise ValueError(\"call_fee_per_1000 must be non-negative\")\n</code></pre>"},{"location":"api_reference/registry/#openai_model_registry.registry-functions","title":"Functions","text":""},{"location":"api_reference/registry/#openai_model_registry.registry.get_registry","title":"<code>get_registry()</code>","text":"<p>Get the model registry singleton instance.</p> <p>This is a convenience function for getting the registry instance.</p> <p>Returns:</p> Name Type Description <code>ModelRegistry</code> <code>ModelRegistry</code> <p>The singleton registry instance</p> Source code in <code>src/openai_model_registry/registry.py</code> <pre><code>def get_registry() -&gt; ModelRegistry:\n    \"\"\"Get the model registry singleton instance.\n\n    This is a convenience function for getting the registry instance.\n\n    Returns:\n        ModelRegistry: The singleton registry instance\n    \"\"\"\n    return ModelRegistry.get_instance()\n</code></pre>"},{"location":"api_reference/schema_version/","title":"Schema version","text":""},{"location":"api_reference/schema_version/#openai_model_registry.schema_version","title":"<code>openai_model_registry.schema_version</code>","text":"<p>Schema version validation and compatibility checking using semver.</p>"},{"location":"api_reference/schema_version/#openai_model_registry.schema_version-classes","title":"Classes","text":""},{"location":"api_reference/schema_version/#openai_model_registry.schema_version.SchemaVersionValidator","title":"<code>SchemaVersionValidator</code>","text":"<p>Handles schema version validation and compatibility checking using semver.</p> Source code in <code>src/openai_model_registry/schema_version.py</code> <pre><code>class SchemaVersionValidator:\n    \"\"\"Handles schema version validation and compatibility checking using semver.\"\"\"\n\n    # Define supported schema version ranges\n    SUPPORTED_SCHEMA_VERSIONS = {\n        \"1.x\": \"&gt;=1.0.0,&lt;2.0.0\",\n        # Future versions can be added here without legacy naming\n    }\n\n    DEFAULT_SCHEMA_VERSION = \"1.0.0\"\n\n    @classmethod\n    def _check_version_range(cls, version: str, range_spec: str) -&gt; bool:\n        \"\"\"Check if a version satisfies a range specification.\n\n        Args:\n            version: Version string to check\n            range_spec: Range specification like \"&gt;=1.0.0,&lt;2.0.0\"\n\n        Returns:\n            True if version satisfies the range\n        \"\"\"\n        if not SEMVER_AVAILABLE or not semver:\n            return False\n\n        try:\n            # Parse the version to get components\n            parsed_version = semver.VersionInfo.parse(version)\n\n            # Split range by comma and check each condition\n            conditions = [cond.strip() for cond in range_spec.split(\",\")]\n\n            for condition in conditions:\n                # Special handling for pre-release versions with &gt;= conditions\n                if condition.startswith(\"&gt;=\") and parsed_version.prerelease:\n                    # For pre-release versions, check if the base version (without prerelease)\n                    # would satisfy the condition\n                    base_version = f\"{parsed_version.major}.{parsed_version.minor}.{parsed_version.patch}\"\n                    base_version_info = semver.VersionInfo.parse(base_version)\n                    if not base_version_info.match(condition):\n                        return False\n                else:\n                    if not parsed_version.match(condition):\n                        return False\n            return True\n        except ValueError:\n            return False\n\n    @classmethod\n    def get_schema_version(cls, config_data: Dict[str, Any]) -&gt; str:\n        \"\"\"Extract and validate schema version from config data.\n\n        Args:\n            config_data: Configuration data dictionary\n\n        Returns:\n            Valid schema version string\n\n        Raises:\n            ValueError: If version is invalid\n        \"\"\"\n        version = config_data.get(\"version\")\n\n        if not version:\n            log_warning(\n                LogEvent.MODEL_REGISTRY,\n                \"Missing schema version, using default\",\n                default_version=cls.DEFAULT_SCHEMA_VERSION,\n            )\n            return cls.DEFAULT_SCHEMA_VERSION\n\n        # Ensure version is a string\n        version_str = str(version)\n\n        # Validate version format using semver\n        if not SEMVER_AVAILABLE or not semver:\n            log_warning(LogEvent.MODEL_REGISTRY, \"semver library not available, skipping version validation\")\n            return version_str\n\n        try:\n            # Try to parse as-is first\n            semver.VersionInfo.parse(version_str)\n        except ValueError:\n            # If that fails, try to normalize common formats\n            try:\n                # Handle formats like \"1.0\" -&gt; \"1.0.0\"\n                parts = version_str.split(\".\")\n                if len(parts) == 2:\n                    version_str = f\"{parts[0]}.{parts[1]}.0\"\n                    semver.VersionInfo.parse(version_str)\n                elif len(parts) == 1:\n                    version_str = f\"{parts[0]}.0.0\"\n                    semver.VersionInfo.parse(version_str)\n                else:\n                    raise ValueError(f\"Cannot normalize version: {version_str}\")\n            except ValueError as e:\n                log_error(LogEvent.MODEL_REGISTRY, \"Invalid schema version format\", version=version_str, error=str(e))\n                raise ValueError(f\"Invalid schema version format: {version_str}\") from e\n\n        return version_str\n\n    @classmethod\n    def is_compatible_schema(cls, version: str) -&gt; bool:\n        \"\"\"Check if schema version is compatible with this registry.\n\n        Args:\n            version: Schema version string\n\n        Returns:\n            True if version is supported, False otherwise\n        \"\"\"\n        if not SEMVER_AVAILABLE or not semver:\n            return True  # Assume compatible if semver not available\n\n        try:\n            # Check against all supported version ranges\n            for range_spec in cls.SUPPORTED_SCHEMA_VERSIONS.values():\n                if cls._check_version_range(version, range_spec):\n                    return True\n            return False\n        except ValueError:\n            return False\n\n    @classmethod\n    def get_compatible_range(cls, version: str) -&gt; Optional[str]:\n        \"\"\"Get the compatible version range for a given version.\n\n        Args:\n            version: Schema version string\n\n        Returns:\n            Version range string if compatible, None otherwise\n        \"\"\"\n        if not SEMVER_AVAILABLE or not semver:\n            return \"1.x\"  # Default range if semver not available\n\n        try:\n            for range_name, range_spec in cls.SUPPORTED_SCHEMA_VERSIONS.items():\n                if cls._check_version_range(version, range_spec):\n                    return range_name\n            return None\n        except ValueError:\n            return None\n\n    @classmethod\n    def validate_schema_structure(cls, config_data: Dict[str, Any], version: str) -&gt; bool:\n        \"\"\"Validate that data structure matches the declared schema version.\n\n        Args:\n            config_data: Configuration data dictionary\n            version: Schema version string\n\n        Returns:\n            True if structure is valid for the version\n        \"\"\"\n        if not SEMVER_AVAILABLE or not semver:\n            # Basic validation without semver\n            return \"models\" in config_data\n\n        try:\n            # For 1.x versions, require 'models' key\n            if cls._check_version_range(version, \"&gt;=1.0.0,&lt;2.0.0\"):\n                required_keys = [\"version\", \"models\"]\n                missing_keys = [key for key in required_keys if key not in config_data]\n                if missing_keys:\n                    log_error(\n                        LogEvent.MODEL_REGISTRY,\n                        \"Missing required keys for schema version\",\n                        version=version,\n                        missing_keys=missing_keys,\n                    )\n                    return False\n                return True\n\n            # Future versions can extend here with appropriate loader methods\n            return False\n\n        except ValueError:\n            return False\n\n    @classmethod\n    def get_loader_method_name(cls, version: str) -&gt; Optional[str]:\n        \"\"\"Get the appropriate loader method name for a schema version.\n\n        Args:\n            version: Schema version string\n\n        Returns:\n            Method name string if version is supported, None otherwise\n        \"\"\"\n        if not SEMVER_AVAILABLE or not semver:\n            return \"_load_capabilities_modern\"  # Default method\n\n        try:\n            if cls._check_version_range(version, \"&gt;=1.0.0,&lt;2.0.0\"):\n                return \"_load_capabilities_modern\"\n            # Future versions can extend here with appropriate loader methods\n            return None\n        except ValueError:\n            return None\n</code></pre>"},{"location":"api_reference/schema_version/#openai_model_registry.schema_version.SchemaVersionValidator-functions","title":"Functions","text":""},{"location":"api_reference/schema_version/#openai_model_registry.schema_version.SchemaVersionValidator.get_compatible_range","title":"<code>get_compatible_range(version)</code>  <code>classmethod</code>","text":"<p>Get the compatible version range for a given version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>Schema version string</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Version range string if compatible, None otherwise</p> Source code in <code>src/openai_model_registry/schema_version.py</code> <pre><code>@classmethod\ndef get_compatible_range(cls, version: str) -&gt; Optional[str]:\n    \"\"\"Get the compatible version range for a given version.\n\n    Args:\n        version: Schema version string\n\n    Returns:\n        Version range string if compatible, None otherwise\n    \"\"\"\n    if not SEMVER_AVAILABLE or not semver:\n        return \"1.x\"  # Default range if semver not available\n\n    try:\n        for range_name, range_spec in cls.SUPPORTED_SCHEMA_VERSIONS.items():\n            if cls._check_version_range(version, range_spec):\n                return range_name\n        return None\n    except ValueError:\n        return None\n</code></pre>"},{"location":"api_reference/schema_version/#openai_model_registry.schema_version.SchemaVersionValidator.get_loader_method_name","title":"<code>get_loader_method_name(version)</code>  <code>classmethod</code>","text":"<p>Get the appropriate loader method name for a schema version.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>Schema version string</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Method name string if version is supported, None otherwise</p> Source code in <code>src/openai_model_registry/schema_version.py</code> <pre><code>@classmethod\ndef get_loader_method_name(cls, version: str) -&gt; Optional[str]:\n    \"\"\"Get the appropriate loader method name for a schema version.\n\n    Args:\n        version: Schema version string\n\n    Returns:\n        Method name string if version is supported, None otherwise\n    \"\"\"\n    if not SEMVER_AVAILABLE or not semver:\n        return \"_load_capabilities_modern\"  # Default method\n\n    try:\n        if cls._check_version_range(version, \"&gt;=1.0.0,&lt;2.0.0\"):\n            return \"_load_capabilities_modern\"\n        # Future versions can extend here with appropriate loader methods\n        return None\n    except ValueError:\n        return None\n</code></pre>"},{"location":"api_reference/schema_version/#openai_model_registry.schema_version.SchemaVersionValidator.get_schema_version","title":"<code>get_schema_version(config_data)</code>  <code>classmethod</code>","text":"<p>Extract and validate schema version from config data.</p> <p>Parameters:</p> Name Type Description Default <code>config_data</code> <code>Dict[str, Any]</code> <p>Configuration data dictionary</p> required <p>Returns:</p> Type Description <code>str</code> <p>Valid schema version string</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If version is invalid</p> Source code in <code>src/openai_model_registry/schema_version.py</code> <pre><code>@classmethod\ndef get_schema_version(cls, config_data: Dict[str, Any]) -&gt; str:\n    \"\"\"Extract and validate schema version from config data.\n\n    Args:\n        config_data: Configuration data dictionary\n\n    Returns:\n        Valid schema version string\n\n    Raises:\n        ValueError: If version is invalid\n    \"\"\"\n    version = config_data.get(\"version\")\n\n    if not version:\n        log_warning(\n            LogEvent.MODEL_REGISTRY,\n            \"Missing schema version, using default\",\n            default_version=cls.DEFAULT_SCHEMA_VERSION,\n        )\n        return cls.DEFAULT_SCHEMA_VERSION\n\n    # Ensure version is a string\n    version_str = str(version)\n\n    # Validate version format using semver\n    if not SEMVER_AVAILABLE or not semver:\n        log_warning(LogEvent.MODEL_REGISTRY, \"semver library not available, skipping version validation\")\n        return version_str\n\n    try:\n        # Try to parse as-is first\n        semver.VersionInfo.parse(version_str)\n    except ValueError:\n        # If that fails, try to normalize common formats\n        try:\n            # Handle formats like \"1.0\" -&gt; \"1.0.0\"\n            parts = version_str.split(\".\")\n            if len(parts) == 2:\n                version_str = f\"{parts[0]}.{parts[1]}.0\"\n                semver.VersionInfo.parse(version_str)\n            elif len(parts) == 1:\n                version_str = f\"{parts[0]}.0.0\"\n                semver.VersionInfo.parse(version_str)\n            else:\n                raise ValueError(f\"Cannot normalize version: {version_str}\")\n        except ValueError as e:\n            log_error(LogEvent.MODEL_REGISTRY, \"Invalid schema version format\", version=version_str, error=str(e))\n            raise ValueError(f\"Invalid schema version format: {version_str}\") from e\n\n    return version_str\n</code></pre>"},{"location":"api_reference/schema_version/#openai_model_registry.schema_version.SchemaVersionValidator.is_compatible_schema","title":"<code>is_compatible_schema(version)</code>  <code>classmethod</code>","text":"<p>Check if schema version is compatible with this registry.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>Schema version string</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if version is supported, False otherwise</p> Source code in <code>src/openai_model_registry/schema_version.py</code> <pre><code>@classmethod\ndef is_compatible_schema(cls, version: str) -&gt; bool:\n    \"\"\"Check if schema version is compatible with this registry.\n\n    Args:\n        version: Schema version string\n\n    Returns:\n        True if version is supported, False otherwise\n    \"\"\"\n    if not SEMVER_AVAILABLE or not semver:\n        return True  # Assume compatible if semver not available\n\n    try:\n        # Check against all supported version ranges\n        for range_spec in cls.SUPPORTED_SCHEMA_VERSIONS.values():\n            if cls._check_version_range(version, range_spec):\n                return True\n        return False\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api_reference/schema_version/#openai_model_registry.schema_version.SchemaVersionValidator.validate_schema_structure","title":"<code>validate_schema_structure(config_data, version)</code>  <code>classmethod</code>","text":"<p>Validate that data structure matches the declared schema version.</p> <p>Parameters:</p> Name Type Description Default <code>config_data</code> <code>Dict[str, Any]</code> <p>Configuration data dictionary</p> required <code>version</code> <code>str</code> <p>Schema version string</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if structure is valid for the version</p> Source code in <code>src/openai_model_registry/schema_version.py</code> <pre><code>@classmethod\ndef validate_schema_structure(cls, config_data: Dict[str, Any], version: str) -&gt; bool:\n    \"\"\"Validate that data structure matches the declared schema version.\n\n    Args:\n        config_data: Configuration data dictionary\n        version: Schema version string\n\n    Returns:\n        True if structure is valid for the version\n    \"\"\"\n    if not SEMVER_AVAILABLE or not semver:\n        # Basic validation without semver\n        return \"models\" in config_data\n\n    try:\n        # For 1.x versions, require 'models' key\n        if cls._check_version_range(version, \"&gt;=1.0.0,&lt;2.0.0\"):\n            required_keys = [\"version\", \"models\"]\n            missing_keys = [key for key in required_keys if key not in config_data]\n            if missing_keys:\n                log_error(\n                    LogEvent.MODEL_REGISTRY,\n                    \"Missing required keys for schema version\",\n                    version=version,\n                    missing_keys=missing_keys,\n                )\n                return False\n            return True\n\n        # Future versions can extend here with appropriate loader methods\n        return False\n\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api_reference/schema_version/#openai_model_registry.schema_version-functions","title":"Functions","text":""},{"location":"api_reference/cli/app/","title":"App","text":""},{"location":"api_reference/cli/app/#openai_model_registry.cli.app","title":"<code>openai_model_registry.cli.app</code>","text":"<p>Main CLI application for OpenAI Model Registry.</p>"},{"location":"api_reference/cli/app/#openai_model_registry.cli.app-classes","title":"Classes","text":""},{"location":"api_reference/cli/app/#openai_model_registry.cli.app-functions","title":"Functions","text":""},{"location":"api_reference/cli/app/#openai_model_registry.cli.app.app","title":"<code>app(ctx, provider=None, format=None, verbose=0, quiet=0, debug=False, no_color=False, version=False)</code>","text":"<p>OpenAI Model Registry CLI - inspect and debug model registry data.</p> <p>The OMR CLI provides tools to inspect data sources, list models, manage cache, and debug provider configurations. It uses only public APIs and respects environment variable configuration.</p> <p>Examples:</p>"},{"location":"api_reference/cli/app/#openai_model_registry.cli.app.app--list-all-models-for-openai-provider","title":"List all models for OpenAI provider","text":"<p>omr models list --provider openai</p>"},{"location":"api_reference/cli/app/#openai_model_registry.cli.app.app--show-data-source-paths","title":"Show data source paths","text":"<p>omr data paths</p>"},{"location":"api_reference/cli/app/#openai_model_registry.cli.app.app--check-for-updates","title":"Check for updates","text":"<p>omr update check</p>"},{"location":"api_reference/cli/app/#openai_model_registry.cli.app.app--clear-cache","title":"Clear cache","text":"<p>omr cache clear --yes</p> Source code in <code>src/openai_model_registry/cli/app.py</code> <pre><code>@click.group()\n@click.option(\n    \"--provider\",\n    type=str,\n    help=\"Override active provider (openai, azure, etc.). Takes precedence over OMR_PROVIDER environment variable.\",\n)\n@click.option(\n    \"--format\",\n    type=click.Choice([\"table\", \"json\", \"csv\", \"yaml\"], case_sensitive=False),\n    help=\"Output format. Defaults to 'table' for TTY, 'json' for non-TTY.\",\n)\n@click.option(\"--verbose\", \"-v\", count=True, help=\"Increase verbosity (can be used multiple times).\")\n@click.option(\"--quiet\", \"-q\", count=True, help=\"Decrease verbosity (can be used multiple times).\")\n@click.option(\"--debug\", is_flag=True, help=\"Enable debug-level logging.\")\n@click.option(\"--no-color\", is_flag=True, help=\"Disable color output.\")\n@click.option(\"--version\", is_flag=True, is_eager=True, help=\"Print CLI and library version information.\")\n@click.option(\n    \"--help-json\",\n    is_flag=True,\n    is_eager=True,\n    expose_value=False,\n    callback=lambda ctx, param, value: _show_json_help(ctx) if value else None,\n    help=\"Show help in JSON format for programmatic use.\",\n)\n@click.pass_context\ndef app(\n    ctx: click.Context,\n    provider: Optional[str] = None,\n    format: Optional[str] = None,\n    verbose: int = 0,\n    quiet: int = 0,\n    debug: bool = False,\n    no_color: bool = False,\n    version: bool = False,\n) -&gt; None:\n    \"\"\"OpenAI Model Registry CLI - inspect and debug model registry data.\n\n    The OMR CLI provides tools to inspect data sources, list models, manage cache,\n    and debug provider configurations. It uses only public APIs and respects\n    environment variable configuration.\n\n    Examples:\n      # List all models for OpenAI provider\n      omr models list --provider openai\n\n      # Show data source paths\n      omr data paths\n\n      # Check for updates\n      omr update check\n\n      # Clear cache\n      omr cache clear --yes\n    \"\"\"\n    if version:\n        try:\n            from .. import __version__\n\n            library_version = __version__\n        except ImportError:\n            library_version = \"unknown\"\n\n        # Get dynamic CLI version (same as library version)\n        cli_version = library_version if library_version != \"unknown\" else \"1.0.0\"\n\n        click.echo(f\"OMR CLI version: {cli_version}\")\n        click.echo(f\"Library version: {library_version}\")\n        return\n\n    # Store global options in context for subcommands\n    ctx.ensure_object(dict)\n\n    # Resolve provider with precedence: CLI &gt; env &gt; default and track source\n    provider_source = \"default\"\n    provider_source_value = \"openai\"\n\n    if provider:\n        try:\n            provider = validate_provider(provider)\n            provider_source = \"CLI flag (--provider)\"\n            provider_source_value = provider\n        except click.BadParameter as e:\n            handle_error(e, ExitCode.INVALID_USAGE)\n    else:\n        # Check if environment variable is set\n        env_provider = os.getenv(\"OMR_PROVIDER\")\n        if env_provider:\n            provider_source = \"Environment variable (OMR_PROVIDER)\"\n            provider_source_value = env_provider\n\n    resolved_provider = resolve_provider(provider)\n    resolved_format = resolve_format(format)\n\n    # Configure logging level based on verbosity\n    log_level = \"WARNING\"\n    if debug:\n        log_level = \"DEBUG\"\n    elif verbose &gt; quiet:\n        if verbose &gt;= 2:\n            log_level = \"DEBUG\"\n        elif verbose &gt;= 1:\n            log_level = \"INFO\"\n    elif quiet &gt; verbose:\n        if quiet &gt;= 2:\n            log_level = \"ERROR\"\n        elif quiet &gt;= 1:\n            log_level = \"WARNING\"\n\n    # Set environment variable for provider if specified\n    if provider:\n        os.environ[\"OMR_PROVIDER\"] = resolved_provider\n    else:\n        # Validate provider from CLI if provided, otherwise ensure env/default is valid\n        try:\n            _ = validate_provider(resolved_provider)\n        except click.BadParameter as e:\n            handle_error(e, ExitCode.INVALID_USAGE)\n\n    ctx.obj.update(\n        {\n            \"provider\": resolved_provider,\n            \"provider_source\": provider_source,\n            \"provider_source_value\": provider_source_value,\n            \"format\": resolved_format,\n            \"format_explicit\": format is not None,\n            \"verbose\": verbose,\n            \"quiet\": quiet,\n            \"debug\": debug,\n            \"no_color\": no_color,\n            \"log_level\": log_level,\n        }\n    )\n</code></pre>"},{"location":"api_reference/cli/commands/cache/","title":"Cache","text":""},{"location":"api_reference/cli/commands/cache/#openai_model_registry.cli.commands.cache","title":"<code>openai_model_registry.cli.commands.cache</code>","text":"<p>Cache management commands for the OMR CLI.</p>"},{"location":"api_reference/cli/commands/cache/#openai_model_registry.cli.commands.cache-classes","title":"Classes","text":""},{"location":"api_reference/cli/commands/cache/#openai_model_registry.cli.commands.cache-functions","title":"Functions","text":""},{"location":"api_reference/cli/commands/cache/#openai_model_registry.cli.commands.cache.cache","title":"<code>cache()</code>","text":"<p>Manage registry cache.</p> Source code in <code>src/openai_model_registry/cli/commands/cache.py</code> <pre><code>@click.group()\ndef cache() -&gt; None:\n    \"\"\"Manage registry cache.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/cli/commands/cache/#openai_model_registry.cli.commands.cache.clear","title":"<code>clear(ctx, yes=False)</code>","text":"<p>Clear cached registry data files.</p> <p>This will remove cached models.yaml and overrides.yaml files from the user data directory. The registry will fall back to bundled data until the next update.</p> Source code in <code>src/openai_model_registry/cli/commands/cache.py</code> <pre><code>@cache.command()\n@click.option(\"--yes\", is_flag=True, help=\"Confirm deletion without prompting (required for non-interactive use).\")\n@click.pass_context\ndef clear(ctx: click.Context, yes: bool = False) -&gt; None:\n    \"\"\"Clear cached registry data files.\n\n    This will remove cached models.yaml and overrides.yaml files\n    from the user data directory. The registry will fall back to bundled data\n    until the next update.\n    \"\"\"\n    try:\n        if not yes:\n            # Interactive confirmation\n            cache_info = get_cache_info()\n            file_count = len(cache_info[\"files\"])\n\n            if file_count == 0:\n                click.echo(\"No cache files found to clear.\")\n                return\n\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n            console.print(f\"[yellow]Warning:[/yellow] This will delete {file_count} cache files:\")\n\n            for file_info in cache_info[\"files\"]:\n                console.print(f\"  - {file_info['name']} ({file_info['size_formatted']})\")\n\n            console.print(f\"\\nCache directory: {cache_info['directory']}\")\n\n            if not click.confirm(\"\\nAre you sure you want to clear the cache?\"):\n                console.print(\"Cache clear cancelled.\")\n                return\n\n        # Perform the cache clear\n        registry = ModelRegistry.get_default()\n\n        # Get files before clearing\n        cache_info_before = get_cache_info()\n        files_before = [f[\"name\"] for f in cache_info_before[\"files\"]]\n\n        # Clear the cache\n        registry.clear_cache()\n\n        # Get files after clearing to see what was actually removed\n        cache_info_after = get_cache_info()\n        files_after = [f[\"name\"] for f in cache_info_after[\"files\"]]\n\n        removed_files = [f for f in files_before if f not in files_after]\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            result_data = {\n                \"success\": True,\n                \"files_removed\": removed_files,\n                \"files_removed_count\": len(removed_files),\n                \"cache_directory\": cache_info_before[\"directory\"],\n            }\n            format_json(result_data)\n        else:\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n\n            if removed_files:\n                console.print(f\"\u2705 [green]Successfully cleared {len(removed_files)} cache files:[/green]\")\n                for filename in removed_files:\n                    console.print(f\"  - {filename}\")\n            else:\n                console.print(\"\u2139\ufe0f  No cache files were found to clear.\")\n\n            console.print(f\"\\nCache directory: {cache_info_before['directory']}\")\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/cache/#openai_model_registry.cli.commands.cache.get_cache_info","title":"<code>get_cache_info()</code>","text":"<p>Get information about cache files and directory.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing cache information</p> Source code in <code>src/openai_model_registry/cli/commands/cache.py</code> <pre><code>def get_cache_info() -&gt; Dict[str, Any]:\n    \"\"\"Get information about cache files and directory.\n\n    Returns:\n        Dictionary containing cache information\n    \"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n        data_info = registry.get_data_info()\n\n        if isinstance(data_info, dict) and \"user_data_dir\" in data_info:\n            cache_dir = Path(data_info[\"user_data_dir\"])\n        else:\n            # Fallback to getting user data dir directly\n            from ...config_paths import get_user_data_dir\n\n            cache_dir = get_user_data_dir()\n\n        cache_info: Dict[str, Any] = {\"directory\": str(cache_dir), \"exists\": cache_dir.exists(), \"files\": []}\n\n        if cache_dir.exists():\n            # Look for common cache files\n            cache_files = [\"models.yaml\", \"overrides.yaml\"]\n\n            for filename in cache_files:\n                file_path = cache_dir / filename\n                if file_path.exists():\n                    stat = file_path.stat()\n\n                    # Try to get ETag information if available\n                    etag = _get_file_etag_info(file_path)\n\n                    cache_info[\"files\"].append(\n                        {\n                            \"name\": filename,\n                            \"path\": str(file_path),\n                            \"size\": stat.st_size,\n                            \"size_formatted\": format_file_size(stat.st_size),\n                            \"modified\": datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\"),\n                            \"etag\": etag,\n                        }\n                    )\n\n        # Calculate total size\n        total_size = int(sum(int(f.get(\"size\", 0)) for f in cache_info[\"files\"]))\n        cache_info[\"total_size\"] = total_size\n        cache_info[\"total_size_formatted\"] = format_file_size(total_size)\n\n        return cache_info\n\n    except Exception as e:\n        return {\"directory\": \"Unknown\", \"exists\": False, \"files\": [], \"total_size\": 0, \"error\": str(e)}\n</code></pre>"},{"location":"api_reference/cli/commands/cache/#openai_model_registry.cli.commands.cache.info","title":"<code>info(ctx)</code>","text":"<p>Show cache directory and file information.</p> Source code in <code>src/openai_model_registry/cli/commands/cache.py</code> <pre><code>@cache.command()\n@click.pass_context\ndef info(ctx: click.Context) -&gt; None:\n    \"\"\"Show cache directory and file information.\"\"\"\n    try:\n        cache_info = get_cache_info()\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            formatted_data = format_cache_info_json(cache_info)\n            format_json(formatted_data)\n        else:\n            # Table format\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n            format_cache_info_table(cache_info, console)\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/data/","title":"Data","text":""},{"location":"api_reference/cli/commands/data/#openai_model_registry.cli.commands.data","title":"<code>openai_model_registry.cli.commands.data</code>","text":"<p>Data inspection commands for the OMR CLI.</p>"},{"location":"api_reference/cli/commands/data/#openai_model_registry.cli.commands.data-classes","title":"Classes","text":""},{"location":"api_reference/cli/commands/data/#openai_model_registry.cli.commands.data-functions","title":"Functions","text":""},{"location":"api_reference/cli/commands/data/#openai_model_registry.cli.commands.data.data","title":"<code>data()</code>","text":"<p>Inspect data sources and configuration.</p> Source code in <code>src/openai_model_registry/cli/commands/data.py</code> <pre><code>@click.group()\ndef data() -&gt; None:\n    \"\"\"Inspect data sources and configuration.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/cli/commands/data/#openai_model_registry.cli.commands.data.dump","title":"<code>dump(ctx, raw=False, effective=False, output=None)</code>","text":"<p>Dump registry data in various formats.</p> <p>If neither --raw nor --effective is specified, defaults to --effective.</p> Source code in <code>src/openai_model_registry/cli/commands/data.py</code> <pre><code>@data.command()\n@click.option(\"--raw\", is_flag=True, help=\"Dump original on-disk/bundled YAML (no provider merge).\")\n@click.option(\"--effective\", is_flag=True, help=\"Dump fully merged, provider-adjusted dataset.\")\n@click.option(\"--output\", \"-o\", type=click.Path(), help=\"Write output to file instead of stdout.\")\n@click.pass_context\ndef dump(\n    ctx: click.Context,\n    raw: bool = False,\n    effective: bool = False,\n    output: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Dump registry data in various formats.\n\n    If neither --raw nor --effective is specified, defaults to --effective.\n    \"\"\"\n    try:\n        # Default to effective if neither specified\n        if not raw and not effective:\n            effective = True\n\n        registry = ModelRegistry.get_default()\n        format_type = ctx.obj[\"format\"]\n\n        # Validate format support for data dump (no fallback: table/csv should error)\n        if format_type not in [\"json\", \"yaml\"]:\n            handle_error(\n                click.BadParameter(\n                    \"Format '\" + format_type + \"' is not supported for data dump. Use 'json' or 'yaml'.\"\n                ),\n                ExitCode.INVALID_USAGE,\n            )\n            return\n\n        # Prepare output\n        output_file: Optional[TextIO] = None\n        if output:\n            output_file = open(output, \"w\")\n\n        try:\n            if raw:\n                # Get raw data paths and read files directly\n                raw_paths = registry.get_raw_data_paths()\n                raw_data = {}\n\n                for file_type, path in raw_paths.items():\n                    if path and Path(path).exists():\n                        with open(path, \"r\") as f:\n                            raw_data[file_type] = yaml.safe_load(f)\n                    else:\n                        # Try to get bundled content using public API\n                        content = registry.get_bundled_data_content(f\"{file_type}.yaml\")\n                        if content:\n                            raw_data[file_type] = yaml.safe_load(content)\n\n                data_to_output = raw_data\n            else:\n                # Get effective merged data\n                data_to_output = registry.dump_effective()\n\n            # Output in requested format\n            if format_type == \"yaml\":\n                yaml_output = yaml.dump(data_to_output, default_flow_style=False, sort_keys=True)\n                if output_file:\n                    output_file.write(yaml_output)\n                else:\n                    click.echo(yaml_output)\n            else:  # json format (default for dump)\n                format_json(data_to_output, output_file or sys.stdout)\n\n        finally:\n            if output_file:\n                output_file.close()\n\n    except Exception as e:\n        handle_error(e, ExitCode.DATA_SOURCE_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/data/#openai_model_registry.cli.commands.data.env","title":"<code>env(ctx)</code>","text":"<p>Show effective OMR environment variables.</p> Source code in <code>src/openai_model_registry/cli/commands/data.py</code> <pre><code>@data.command()\n@click.pass_context\ndef env(ctx: click.Context) -&gt; None:\n    \"\"\"Show effective OMR environment variables.\"\"\"\n    try:\n        env_vars = get_omr_env_vars()\n\n        format_type = ctx.obj[\"format\"]\n        output_file = None\n\n        if format_type == \"json\":\n            formatted_data = format_env_vars_json(env_vars)\n            format_json(formatted_data, output_file)\n        else:  # table format\n            console = create_console(output_file, ctx.obj[\"no_color\"])\n            format_env_vars_table(env_vars, console)\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/data/#openai_model_registry.cli.commands.data.paths","title":"<code>paths(ctx)</code>","text":"<p>Show resolved data source paths and precedence.</p> Source code in <code>src/openai_model_registry/cli/commands/data.py</code> <pre><code>@data.command()\n@click.pass_context\ndef paths(ctx: click.Context) -&gt; None:\n    \"\"\"Show resolved data source paths and precedence.\"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n        raw_paths = registry.get_raw_data_paths()\n        data_info = registry.get_data_info()\n\n        # Enhance paths with additional info including etag/mtime\n        enhanced_paths: dict[str, dict[str, object]] = {}\n        for file_type, path in raw_paths.items():\n            # Determine the actual source of the path by comparing actual paths\n            source = \"Bundled Package\"\n            if path:\n                import os\n                from pathlib import Path\n\n                resolved_path = str(Path(path).resolve())\n\n                # Check if path matches OMR_MODEL_REGISTRY_PATH (only for models.yaml)\n                if file_type == \"models\" and \"OMR_MODEL_REGISTRY_PATH\" in os.environ:\n                    registry_path = os.getenv(\"OMR_MODEL_REGISTRY_PATH\")\n                    if registry_path and str(Path(registry_path).resolve()) == resolved_path:\n                        source = \"OMR_MODEL_REGISTRY_PATH\"\n                    else:\n                        # Check if path is from OMR_DATA_DIR\n                        omr_data_dir = os.getenv(\"OMR_DATA_DIR\")\n                        if omr_data_dir and resolved_path.startswith(str(Path(omr_data_dir).resolve())):\n                            source = \"OMR_DATA_DIR\"\n                        else:\n                            source = \"User Data\"\n                else:\n                    # Check if path is from OMR_DATA_DIR\n                    omr_data_dir = os.getenv(\"OMR_DATA_DIR\")\n                    if omr_data_dir and resolved_path.startswith(str(Path(omr_data_dir).resolve())):\n                        source = \"OMR_DATA_DIR\"\n                    else:\n                        source = \"User Data\"\n\n            file_info = {\n                \"path\": path or \"Bundled\",\n                \"source\": source,\n                \"exists\": Path(path).exists() if path else True,\n                \"etag\": None,\n                \"last_modified\": None,\n                \"file_size\": None,\n            }\n\n            # Add file-specific information if the file exists\n            if path and Path(path).exists():\n                try:\n                    file_path = Path(path)\n                    stat = file_path.stat()\n                    file_info[\"file_size\"] = stat.st_size\n                    file_info[\"last_modified\"] = datetime.fromtimestamp(stat.st_mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n                    # File integrity verified through other means\n\n                    # Try to get etag from accompanying metadata if available\n                    etag_info = _get_file_etag(file_path)\n                    if etag_info:\n                        file_info[\"etag\"] = etag_info\n\n                except (OSError, IOError):\n                    # If we can't read file info, leave the fields as None\n                    pass\n            elif not path:  # Bundled file\n                pass  # No additional info needed for bundled files\n\n            enhanced_paths[f\"{file_type}.yaml\"] = file_info\n\n        # Add data info if available\n        if isinstance(data_info, dict):\n            enhanced_paths.update(\n                {\n                    \"data_directory\": {\n                        \"path\": data_info.get(\"user_data_dir\", \"N/A\"),\n                        \"source\": \"System\",\n                        \"exists\": True,\n                    }\n                }\n            )\n\n        # Force strict validation: only json/yaml supported; table/csv should error\n        format_type = ctx.obj[\"format\"]\n        output_file = None\n\n        if format_type == \"json\":\n            formatted_data = format_data_paths_json(enhanced_paths)\n            format_json(formatted_data, output_file)\n        else:  # table format\n            console = create_console(output_file, ctx.obj[\"no_color\"])\n            format_data_paths_table(enhanced_paths, console)\n\n    except Exception as e:\n        handle_error(e, ExitCode.DATA_SOURCE_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/models/","title":"Models","text":""},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models","title":"<code>openai_model_registry.cli.commands.models</code>","text":"<p>Model inspection commands for the OMR CLI.</p>"},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models-classes","title":"Classes","text":""},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models-functions","title":"Functions","text":""},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models.extract_nested_value","title":"<code>extract_nested_value(obj, path)</code>","text":"<p>Extract nested value using dotted path notation.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Dict[str, Any]</code> <p>Object to extract from</p> required <code>path</code> <code>str</code> <p>Dotted path (e.g., 'pricing.input_cost_per_unit')</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Extracted value or None if path doesn't exist</p> Source code in <code>src/openai_model_registry/cli/commands/models.py</code> <pre><code>def extract_nested_value(obj: Dict[str, Any], path: str) -&gt; Any:\n    \"\"\"Extract nested value using dotted path notation.\n\n    Args:\n        obj: Object to extract from\n        path: Dotted path (e.g., 'pricing.input_cost_per_unit')\n\n    Returns:\n        Extracted value or None if path doesn't exist\n    \"\"\"\n    try:\n        current = obj\n        for part in path.split(\".\"):\n            if isinstance(current, dict) and part in current:\n                current = current[part]\n            else:\n                return None\n        return current\n    except (KeyError, TypeError):\n        return None\n</code></pre>"},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models.filter_models","title":"<code>filter_models(models, filter_expr)</code>","text":"<p>Apply filtering to models with support for structured queries.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Dict[str, Any]</code> <p>Models data</p> required <code>filter_expr</code> <code>str</code> <p>Filter expression - supports:         - Simple string matching: \"gpt-4\"         - Field comparison: \"supports_vision:true\", \"pricing.input_cost_per_unit:&gt;2.0\"         - Multiple conditions with AND: \"supports_vision:true AND context_window.total:&gt;100000\"</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Filtered models data</p> Source code in <code>src/openai_model_registry/cli/commands/models.py</code> <pre><code>def filter_models(models: Dict[str, Any], filter_expr: str) -&gt; Dict[str, Any]:\n    \"\"\"Apply filtering to models with support for structured queries.\n\n    Args:\n        models: Models data\n        filter_expr: Filter expression - supports:\n                    - Simple string matching: \"gpt-4\"\n                    - Field comparison: \"supports_vision:true\", \"pricing.input_cost_per_unit:&gt;2.0\"\n                    - Multiple conditions with AND: \"supports_vision:true AND context_window.total:&gt;100000\"\n\n    Returns:\n        Filtered models data\n    \"\"\"\n    filtered = {}\n\n    # Parse filter expression for structured queries\n    conditions = []\n    import re\n\n    # Split on AND (case insensitive) using regex\n    and_pattern = r\"\\s+AND\\s+\"\n    if re.search(and_pattern, filter_expr, re.IGNORECASE):\n        parts = [part.strip() for part in re.split(and_pattern, filter_expr, flags=re.IGNORECASE) if part.strip()]\n        conditions = parts\n    else:\n        conditions = [filter_expr.strip()]\n\n    for name, model_data in models.items():\n        # Check if all conditions match\n        matches_all = True\n\n        for condition in conditions:\n            if not _matches_condition(name, model_data, condition):\n                matches_all = False\n                break\n\n        if matches_all:\n            filtered[name] = model_data\n\n    return filtered\n</code></pre>"},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models.get","title":"<code>get(ctx, model_name, effective=False, raw=False, parameters_only=False, output=None)</code>","text":"<p>Get detailed information about a specific model.</p> Source code in <code>src/openai_model_registry/cli/commands/models.py</code> <pre><code>@models.command()\n@click.argument(\"model_name\", type=str)\n@click.option(\"--effective\", is_flag=True, help=\"Show effective model data (with provider overrides) - default.\")\n@click.option(\"--raw\", is_flag=True, help=\"Show raw model data (without provider overrides).\")\n@click.option(\"--parameters-only\", is_flag=True, help=\"Show only the model's parameters block.\")\n@click.option(\"--output\", \"-o\", type=click.Path(), help=\"Write output to file instead of stdout.\")\n@click.pass_context\ndef get(\n    ctx: click.Context,\n    model_name: str,\n    effective: bool = False,\n    raw: bool = False,\n    parameters_only: bool = False,\n    output: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Get detailed information about a specific model.\"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n\n        # Default to effective if neither specified\n        if not raw and not effective:\n            effective = True\n\n        # Prepare output\n        output_file: Optional[TextIO] = None\n        if output:\n            output_file = open(output, \"w\")\n\n        try:\n            if effective:\n                # Get effective model capabilities\n                try:\n                    # Pre-check existence using effective data if available (skip when parameters-only)\n                    if not parameters_only:\n                        try:\n                            effective_data = registry.dump_effective().get(\"models\", {})\n                            if model_name not in effective_data:\n                                handle_error(\n                                    Exception(f\"Model '{model_name}' not found\"),\n                                    ExitCode.MODEL_NOT_FOUND,\n                                )\n                                return\n                        except Exception:\n                            pass\n\n                    capabilities = registry.get_capabilities(model_name)\n                    wb = getattr(capabilities, \"web_search_billing\", None)\n                    if wb is not None and is_dataclass(wb):\n                        billing_block = {\"web_search\": asdict(wb)}\n                    elif isinstance(wb, dict):\n                        billing_block = {\"web_search\": wb}\n                    else:\n                        billing_block = None\n                    model_data = {\n                        \"name\": model_name,\n                        \"context_window\": {\n                            \"total\": capabilities.context_window,\n                            \"input\": getattr(capabilities, \"input_context_window\", None),\n                            \"output\": capabilities.max_output_tokens,\n                        },\n                        \"pricing\": {\n                            \"scheme\": getattr(capabilities.pricing, \"scheme\", \"per_token\"),\n                            \"unit\": getattr(capabilities.pricing, \"unit\", \"million_tokens\"),\n                            \"input_cost_per_unit\": getattr(capabilities.pricing, \"input_cost_per_unit\", 0.0),\n                            \"output_cost_per_unit\": getattr(capabilities.pricing, \"output_cost_per_unit\", 0.0),\n                            \"tiers\": getattr(capabilities.pricing, \"tiers\", None),\n                        },\n                        \"supports_vision\": capabilities.supports_vision,\n                        \"supports_function_calling\": getattr(\n                            capabilities,\n                            \"supports_function_calling\",\n                            getattr(capabilities, \"supports_functions\", False),\n                        ),\n                        \"supports_streaming\": capabilities.supports_streaming,\n                        \"billing\": billing_block,\n                        \"provider\": ctx.obj[\"provider\"],\n                        \"parameters\": getattr(capabilities, \"inline_parameters\", {}),\n                        \"metadata\": {\"source\": \"effective\", \"provider_applied\": ctx.obj[\"provider\"]},\n                    }\n                except Exception:\n                    handle_error(Exception(f\"Model '{model_name}' not found\"), ExitCode.MODEL_NOT_FOUND)\n                    return\n            else:\n                # Get raw model data without provider overrides\n                model_data_opt: Dict[str, Any] | None = registry.get_raw_model_data(model_name)\n\n                if model_data_opt is None:\n                    handle_error(Exception(f\"Model '{model_name}' not found\"), ExitCode.MODEL_NOT_FOUND)\n                    return\n                model_data = model_data_opt\n\n            # Output the data in requested format\n            format_type = ctx.obj[\"format\"]\n\n            # Validate format support for models get\n            try:\n                format_type = validate_format_support(format_type, [\"json\", \"yaml\"], \"models get\", ctx.obj)\n            except click.BadParameter as e:\n                handle_error(e, ExitCode.INVALID_USAGE)\n\n            # Reduce payload to only parameters if requested\n            payload: Dict[str, Any]\n            if parameters_only:\n                if effective:\n                    payload = getattr(capabilities, \"inline_parameters\", {}) or {}\n                else:\n                    payload = {}\n                    if isinstance(model_data, dict):\n                        payload = model_data.get(\"parameters\", {}) or {}\n            else:\n                payload = model_data\n\n            if format_type == \"yaml\":\n                import yaml\n\n                yaml_output = yaml.dump(payload, default_flow_style=False, sort_keys=True)\n                if output_file:\n                    output_file.write(yaml_output)\n                else:\n                    click.echo(yaml_output.rstrip())\n            else:  # json format (default)\n                format_json(payload, output_file or sys.stdout)\n\n        finally:\n            if output_file:\n                output_file.close()\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models.list","title":"<code>list(ctx, filter=None, columns=None)</code>","text":"<p>List all available models with their capabilities.</p> Source code in <code>src/openai_model_registry/cli/commands/models.py</code> <pre><code>@models.command()\n@click.option(\"--filter\", type=str, help=\"Filter models using simple expression.\")\n@click.option(\"--columns\", type=str, help=_COLUMNS_HELP_DYNAMIC)\n@click.pass_context\ndef list(\n    ctx: click.Context,\n    filter: Optional[str] = None,\n    columns: Optional[str] = None,\n) -&gt; None:\n    \"\"\"List all available models with their capabilities.\"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n\n        # Get effective models data\n        effective_data = registry.dump_effective()\n        models_data = effective_data.get(\"models\", {})\n\n        # Apply filtering if specified\n        if filter:\n            models_data = filter_models(models_data, filter)\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            formatted_data = format_models_list_json(models_data)\n            format_json(formatted_data)\n        elif format_type == \"csv\":\n            # CSV output\n            import csv\n            import io\n\n            output = io.StringIO()\n\n            # Determine columns\n            if columns:\n                column_list = [col.strip() for col in columns.split(\",\")]\n            else:\n                column_list = [\n                    \"name\",\n                    \"context_window.total\",\n                    \"context_window.output\",\n                    \"pricing.input_cost_per_unit\",\n                    \"pricing.output_cost_per_unit\",\n                    \"pricing.unit\",\n                    \"supports_vision\",\n                    \"supports_function_calling\",\n                ]\n\n            writer = csv.writer(output)\n            writer.writerow(column_list)\n\n            for name, model_data in models_data.items():\n                row = []\n                for col in column_list:\n                    if col == \"name\":\n                        row.append(name)\n                    else:\n                        value = extract_nested_value(model_data, col)\n                        row.append(str(value) if value is not None else \"N/A\")\n                writer.writerow(row)\n\n            click.echo(output.getvalue().strip())\n        else:\n            # Table format\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n\n            # Determine columns for table format\n            if columns:\n                column_list = [col.strip() for col in columns.split(\",\")]\n            else:\n                column_list = None  # Use default columns\n\n            format_models_table(models_data, console, columns=column_list)\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/models/#openai_model_registry.cli.commands.models.models","title":"<code>models()</code>","text":"<p>Inspect and list models.</p> Source code in <code>src/openai_model_registry/cli/commands/models.py</code> <pre><code>@click.group()\ndef models() -&gt; None:\n    \"\"\"Inspect and list models.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/cli/commands/providers/","title":"Providers","text":""},{"location":"api_reference/cli/commands/providers/#openai_model_registry.cli.commands.providers","title":"<code>openai_model_registry.cli.commands.providers</code>","text":"<p>Provider management commands for the OMR CLI.</p>"},{"location":"api_reference/cli/commands/providers/#openai_model_registry.cli.commands.providers-classes","title":"Classes","text":""},{"location":"api_reference/cli/commands/providers/#openai_model_registry.cli.commands.providers-functions","title":"Functions","text":""},{"location":"api_reference/cli/commands/providers/#openai_model_registry.cli.commands.providers.current","title":"<code>current(ctx)</code>","text":"<p>Show the currently active provider and its source.</p> Source code in <code>src/openai_model_registry/cli/commands/providers.py</code> <pre><code>@providers.command()\n@click.pass_context\ndef current(ctx: click.Context) -&gt; None:\n    \"\"\"Show the currently active provider and its source.\"\"\"\n    try:\n        current_provider = ctx.obj[\"provider\"]\n\n        # Get the tracked provider source information from context\n        source = ctx.obj.get(\"provider_source\", \"Unknown\")\n        source_value = ctx.obj.get(\"provider_source_value\", current_provider)\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            provider_info = {\n                \"current_provider\": current_provider,\n                \"source\": source,\n                \"source_value\": source_value,\n                \"precedence_order\": [\n                    \"CLI flag (--provider)\",\n                    \"Environment variable (OMR_PROVIDER)\",\n                    \"Default (openai)\",\n                ],\n            }\n            format_json(provider_info)\n        else:\n            # Table/human readable format\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n\n            console.print(f\"[bold]Current Provider:[/bold] {current_provider}\")\n            console.print(f\"[bold]Source:[/bold] {source}\")\n            console.print(f\"[bold]Value:[/bold] {source_value}\")\n\n            console.print(\"\\n[bold]Provider Resolution Precedence:[/bold]\")\n            console.print(\"  1. CLI flag (--provider)\")\n            console.print(\"  2. Environment variable (OMR_PROVIDER)\")\n            console.print(\"  3. Default (openai)\")\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/providers/#openai_model_registry.cli.commands.providers.list","title":"<code>list(ctx)</code>","text":"<p>List all available providers.</p> Source code in <code>src/openai_model_registry/cli/commands/providers.py</code> <pre><code>@providers.command()\n@click.pass_context\ndef list(ctx: click.Context) -&gt; None:\n    \"\"\"List all available providers.\"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n        available_providers = registry.list_providers()\n        current_provider = ctx.obj[\"provider\"]\n\n        # If the user explicitly provided --format, honor it.\n        # Otherwise: force table when sys.stdout.isatty() is True (tests patch this);\n        # fall back to JSON when not TTY.\n        if ctx.obj.get(\"format_explicit\"):\n            format_type = ctx.obj[\"format\"].lower()\n        else:\n            try:\n                is_tty = bool(sys.stdout.isatty())\n            except Exception:\n                is_tty = False\n            if is_tty:\n                format_type = \"table\"\n            else:\n                format_type = \"json\"\n\n        if format_type == \"json\":\n            formatted_data = format_providers_json(available_providers, current_provider)\n            format_json(formatted_data)\n        elif format_type == \"table\":\n            # Table format\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n            format_providers_table(available_providers, current_provider, console)\n        else:\n            # Only json and table are supported here; anything else is invalid usage\n            handle_error(\n                click.BadParameter(\n                    f\"Format '{format_type}' is not supported for providers list. Use 'table' or 'json'.\"\n                ),\n                ExitCode.INVALID_USAGE,\n            )\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/providers/#openai_model_registry.cli.commands.providers.providers","title":"<code>providers()</code>","text":"<p>Manage and inspect providers.</p> Source code in <code>src/openai_model_registry/cli/commands/providers.py</code> <pre><code>@click.group()\ndef providers() -&gt; None:\n    \"\"\"Manage and inspect providers.\"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/cli/commands/update/","title":"Update","text":""},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update","title":"<code>openai_model_registry.cli.commands.update</code>","text":"<p>Update management commands for the OMR CLI.</p>"},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update-classes","title":"Classes","text":""},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update-functions","title":"Functions","text":""},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update.apply","title":"<code>apply(ctx, force=False, url=None)</code>","text":"<p>Download and install available updates.</p> <p>Only downloads if an update is actually available (unless --force is used). Use 'omr update check' first to see what updates are available.</p> <p>This command only applies updates - it doesn't check or validate first. For a complete check-and-update workflow, use 'omr update refresh' instead.</p> Source code in <code>src/openai_model_registry/cli/commands/update.py</code> <pre><code>@update.command()\n@click.option(\"--force\", is_flag=True, help=\"Force update even if current version is newer.\")\n@click.option(\"--url\", type=str, help=\"Override update URL.\")\n@click.pass_context\ndef apply(ctx: click.Context, force: bool = False, url: Optional[str] = None) -&gt; None:\n    \"\"\"Download and install available updates.\n\n    Only downloads if an update is actually available (unless --force is used).\n    Use 'omr update check' first to see what updates are available.\n\n    This command only applies updates - it doesn't check or validate first.\n    For a complete check-and-update workflow, use 'omr update refresh' instead.\n    \"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n\n        # Get version info before update\n        update_info = registry.get_update_info()\n        current_version_before = update_info.current_version\n\n        if url:\n            # Use refresh_from_remote for URL override\n            # This method automatically reloads the registry after successful update\n            result = registry.refresh_from_remote(url=url, force=force)\n            success = result.success\n            message = result.message\n        else:\n            # Use update_data for standard updates\n            # This method automatically reloads the registry after successful update\n            success = registry.update_data(force=force)\n            message = \"Update completed successfully\" if success else \"Update failed\"\n\n        # Get version info after update\n        update_info_after = registry.get_update_info()\n        current_version_after = update_info_after.current_version\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            result_data = {\n                \"success\": success,\n                \"message\": message,\n                \"version_before\": current_version_before,\n                \"version_after\": current_version_after,\n            }\n            format_json(result_data)\n        else:\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n            if success:\n                console.print(\"\u2705 [green]Update applied successfully[/green]\")\n                if current_version_before != current_version_after:\n                    console.print(\n                        f\"Updated from: {current_version_before or 'bundled'} \u2192 {current_version_after or 'bundled'}\"\n                    )\n                else:\n                    console.print(f\"Version: {current_version_after or 'bundled'} (already up to date)\")\n                # Show message without \"Message:\" prefix if it's not generic\n                if message and message not in [\"Update completed successfully\", \"Update failed\"]:\n                    console.print(message)\n            else:\n                console.print(\"\u274c [red]Update failed[/red]\")\n                console.print(f\"Version: {current_version_before or 'bundled'} (unchanged)\")\n                # Show error message without \"Error:\" prefix if it's meaningful\n                if message and message != \"Update failed\":\n                    console.print(message)\n\n        exit(ExitCode.SUCCESS if success else ExitCode.GENERIC_ERROR)\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update.check","title":"<code>check(ctx, url=None)</code>","text":"<p>Check if newer model data is available without downloading.</p> <p>Shows current and latest versions. Useful for CI/CD pipelines to detect when updates are needed.</p> Exit codes <p>0: Registry is up to date</p> <p>10: Update available (CI-friendly)</p> Source code in <code>src/openai_model_registry/cli/commands/update.py</code> <pre><code>@update.command()\n@click.option(\"--url\", type=str, help=\"Override update URL.\")\n@click.pass_context\ndef check(ctx: click.Context, url: Optional[str] = None) -&gt; None:\n    \"\"\"Check if newer model data is available without downloading.\n\n    Shows current and latest versions. Useful for CI/CD pipelines\n    to detect when updates are needed.\n\n    Exit codes:\n      0: Registry is up to date\n     10: Update available (CI-friendly)\n    \"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n\n        # Check for updates\n        refresh_result = registry.check_for_updates(url)\n        update_info = registry.get_update_info()\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            # Determine if update is available based on the actual status\n            status_val = (\n                refresh_result.status.value if hasattr(refresh_result.status, \"value\") else str(refresh_result.status)\n            )\n            update_available = status_val == \"update_available\"\n\n            result_data = {\n                \"update_available\": update_available,\n                \"current_version\": update_info.current_version,\n                \"latest_version\": update_info.latest_version,\n                \"message\": refresh_result.message,\n                \"status\": status_val,\n            }\n            format_json(result_data)\n        else:\n            # Table/human readable format\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n\n            status_val = (\n                refresh_result.status.value if hasattr(refresh_result.status, \"value\") else str(refresh_result.status)\n            )\n\n            if status_val == \"already_current\":\n                console.print(\"\u2705 [green]Registry is up to date[/green]\")\n                console.print(f\"Current version: {update_info.current_version or 'bundled'}\")\n            elif status_val == \"update_available\":\n                console.print(\"\ud83d\udd04 [yellow]Update available[/yellow]\")\n                console.print(f\"Current version: {update_info.current_version or 'bundled'}\")\n                console.print(f\"Latest version: {update_info.latest_version}\")\n                # Only show message if it's meaningful and not contradictory\n                if refresh_result.message and \"up to date\" not in refresh_result.message.lower():\n                    console.print(refresh_result.message)\n            else:\n                # Handle error or unknown status\n                console.print(\"\u274c [red]Check failed[/red]\")\n                console.print(f\"Status: {status_val}\")\n                if refresh_result.message:\n                    console.print(refresh_result.message)\n\n        # Exit with appropriate code for CI\n        status_val = (\n            refresh_result.status.value if hasattr(refresh_result.status, \"value\") else str(refresh_result.status)\n        )\n        if status_val == \"already_current\":\n            exit(ExitCode.SUCCESS)\n        elif status_val == \"update_available\":\n            exit(ExitCode.UPDATE_AVAILABLE)\n        else:\n            exit(ExitCode.GENERIC_ERROR)\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update.refresh","title":"<code>refresh(ctx, url=None, validate_only=False, force=False)</code>","text":"<p>Complete update workflow: check, validate, and apply in one command.</p> <p>This is the recommended way to update your registry. It automatically: 1. Checks for available updates 2. Validates the remote data integrity 3. Downloads and applies updates if available</p> <p>Use --validate-only to test remote data without downloading. This is the most convenient command for regular updates.</p> Source code in <code>src/openai_model_registry/cli/commands/update.py</code> <pre><code>@update.command()\n@click.option(\"--url\", type=str, help=\"Override update URL.\")\n@click.option(\"--validate-only\", is_flag=True, help=\"Only validate remote data without applying updates.\")\n@click.option(\"--force\", is_flag=True, help=\"Force refresh even if current version is newer.\")\n@click.pass_context\ndef refresh(\n    ctx: click.Context,\n    url: Optional[str] = None,\n    validate_only: bool = False,\n    force: bool = False,\n) -&gt; None:\n    \"\"\"Complete update workflow: check, validate, and apply in one command.\n\n    This is the recommended way to update your registry. It automatically:\n    1. Checks for available updates\n    2. Validates the remote data integrity\n    3. Downloads and applies updates if available\n\n    Use --validate-only to test remote data without downloading.\n    This is the most convenient command for regular updates.\n    \"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n\n        # Get version info before update\n        update_info = registry.get_update_info()\n        current_version_before = update_info.current_version\n\n        result = registry.refresh_from_remote(url=url, force=force, validate_only=validate_only)\n\n        # Get version info after update (only if not validate_only)\n        if not validate_only:\n            update_info_after = registry.get_update_info()\n            current_version_after = update_info_after.current_version\n        else:\n            current_version_after = current_version_before\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            result_data = {\n                \"success\": result.success,\n                \"status\": result.status.value if hasattr(result.status, \"value\") else str(result.status),\n                \"message\": result.message,\n                \"validate_only\": validate_only,\n                \"version_before\": current_version_before,\n                \"version_after\": current_version_after,\n            }\n            format_json(result_data)\n        else:\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n\n            action = \"Validation\" if validate_only else \"Refresh\"\n            if result.success:\n                console.print(f\"\u2705 [green]{action} completed successfully[/green]\")\n            else:\n                console.print(f\"\u274c [red]{action} failed[/red]\")\n\n            # Show version information with friendly status\n            if not validate_only and current_version_before != current_version_after:\n                console.print(\n                    f\"Updated from: {current_version_before or 'bundled'} \u2192 {current_version_after or 'bundled'}\"\n                )\n            else:\n                # Show friendly status instead of enum\n                status_str = result.status.value if hasattr(result.status, \"value\") else str(result.status)\n                if status_str == \"already_current\":\n                    version_suffix = \" (already up to date)\"\n                elif status_str == \"updated\":\n                    version_suffix = \" (updated)\"\n                else:\n                    version_suffix = f\" ({status_str.replace('_', ' ')})\"\n\n                console.print(f\"Version: {current_version_before or 'bundled'}{version_suffix}\")\n\n            # Show message without \"Message:\" prefix\n            if result.message:\n                console.print(result.message)\n\n        exit(ExitCode.SUCCESS if result.success else ExitCode.GENERIC_ERROR)\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update.show_config","title":"<code>show_config(ctx)</code>","text":"<p>Show current update configuration and environment settings.</p> <p>Displays data sources, update policies, version pinning, and other settings that affect how updates work.</p> Source code in <code>src/openai_model_registry/cli/commands/update.py</code> <pre><code>@update.command(\"show-config\")\n@click.pass_context\ndef show_config(ctx: click.Context) -&gt; None:\n    \"\"\"Show current update configuration and environment settings.\n\n    Displays data sources, update policies, version pinning, and\n    other settings that affect how updates work.\n    \"\"\"\n    try:\n        registry = ModelRegistry.get_default()\n        data_info = registry.get_data_info()\n\n        # Get environment variables related to updates\n        import os\n\n        update_config: dict[str, object] = {\n            \"data_directory\": data_info.get(\"user_data_dir\") if isinstance(data_info, dict) else \"N/A\",\n            \"environment_variables\": {\n                \"OMR_DISABLE_DATA_UPDATES\": os.getenv(\"OMR_DISABLE_DATA_UPDATES\"),\n                \"OMR_DATA_VERSION_PIN\": os.getenv(\"OMR_DATA_VERSION_PIN\"),\n                \"OMR_DATA_DIR\": os.getenv(\"OMR_DATA_DIR\"),\n                \"OMR_MODEL_REGISTRY_PATH\": os.getenv(\"OMR_MODEL_REGISTRY_PATH\"),\n            },\n            \"update_settings\": {\n                \"updates_disabled\": os.getenv(\"OMR_DISABLE_DATA_UPDATES\") == \"true\",\n                \"version_pinned\": os.getenv(\"OMR_DATA_VERSION_PIN\") is not None,\n                \"custom_data_dir\": os.getenv(\"OMR_DATA_DIR\") is not None,\n                \"custom_registry_path\": os.getenv(\"OMR_MODEL_REGISTRY_PATH\") is not None,\n            },\n        }\n\n        format_type = ctx.obj[\"format\"]\n\n        if format_type == \"json\":\n            format_json(update_config)\n        else:\n            console = create_console(no_color=ctx.obj[\"no_color\"])\n\n            console.print(\"[bold]Update Configuration[/bold]\")\n            console.print(f\"Data Directory: {update_config['data_directory']}\")\n            console.print()\n\n            console.print(\"[bold]Environment Variables:[/bold]\")\n            env_vars = update_config.get(\"environment_variables\")\n            if isinstance(env_vars, dict):\n                for key, value in env_vars.items():\n                    status = value if value else \"[dim]&lt;not set&gt;[/dim]\"\n                    console.print(f\"  {key}: {status}\")\n\n            console.print()\n            console.print(\"[bold]Settings:[/bold]\")\n            settings = update_config.get(\"update_settings\")\n            if isinstance(settings, dict):\n                console.print(f\"  Updates Disabled: {'\u2713' if settings.get('updates_disabled') else '\u2717'}\")\n                console.print(f\"  Version Pinned: {'\u2713' if settings.get('version_pinned') else '\u2717'}\")\n                console.print(f\"  Custom Data Dir: {'\u2713' if settings.get('custom_data_dir') else '\u2717'}\")\n                console.print(f\"  Custom Registry Path: {'\u2713' if settings.get('custom_registry_path') else '\u2717'}\")\n\n    except Exception as e:\n        handle_error(e, ExitCode.GENERIC_ERROR)\n</code></pre>"},{"location":"api_reference/cli/commands/update/#openai_model_registry.cli.commands.update.update","title":"<code>update()</code>","text":"<p>Manage registry data updates.</p> <p>Keep your model registry data current with the latest OpenAI models, capabilities, and pricing information from GitHub releases.</p> Source code in <code>src/openai_model_registry/cli/commands/update.py</code> <pre><code>@click.group()\ndef update() -&gt; None:\n    \"\"\"Manage registry data updates.\n\n    Keep your model registry data current with the latest OpenAI models,\n    capabilities, and pricing information from GitHub releases.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/cli/formatters/json/","title":"Json","text":""},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json","title":"<code>openai_model_registry.cli.formatters.json</code>","text":"<p>JSON output formatter for CLI.</p>"},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json-functions","title":"Functions","text":""},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json.format_cache_info_json","title":"<code>format_cache_info_json(cache_info)</code>","text":"<p>Format cache information for JSON output.</p> <p>Parameters:</p> Name Type Description Default <code>cache_info</code> <code>Dict[str, Any]</code> <p>Cache information</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Formatted data structure</p> Source code in <code>src/openai_model_registry/cli/formatters/json.py</code> <pre><code>def format_cache_info_json(cache_info: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Format cache information for JSON output.\n\n    Args:\n        cache_info: Cache information\n\n    Returns:\n        Formatted data structure\n    \"\"\"\n    return {\n        \"cache_directory\": cache_info.get(\"directory\"),\n        \"files\": cache_info.get(\"files\", []),\n        \"total_size_bytes\": cache_info.get(\"total_size\", 0),\n        \"file_count\": len(cache_info.get(\"files\", [])),\n    }\n</code></pre>"},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json.format_data_paths_json","title":"<code>format_data_paths_json(paths)</code>","text":"<p>Format data paths for JSON output.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Dict[str, Any]</code> <p>Path information</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Formatted data structure</p> Source code in <code>src/openai_model_registry/cli/formatters/json.py</code> <pre><code>def format_data_paths_json(paths: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Format data paths for JSON output.\n\n    Args:\n        paths: Path information\n\n    Returns:\n        Formatted data structure\n    \"\"\"\n    return {\n        \"data_sources\": paths,\n        \"resolution_order\": [\n            \"OMR_MODEL_REGISTRY_PATH environment variable\",\n            \"OMR_DATA_DIR environment variable\",\n            \"User data directory\",\n            \"Bundled package data\",\n        ],\n    }\n</code></pre>"},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json.format_env_vars_json","title":"<code>format_env_vars_json(env_vars)</code>","text":"<p>Format environment variables for JSON output.</p> <p>Parameters:</p> Name Type Description Default <code>env_vars</code> <code>Dict[str, Optional[str]]</code> <p>Environment variables</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Formatted data structure</p> Source code in <code>src/openai_model_registry/cli/formatters/json.py</code> <pre><code>def format_env_vars_json(env_vars: Dict[str, Optional[str]]) -&gt; Dict[str, Any]:\n    \"\"\"Format environment variables for JSON output.\n\n    Args:\n        env_vars: Environment variables\n\n    Returns:\n        Formatted data structure\n    \"\"\"\n    return {\n        \"environment_variables\": {key: {\"value\": value, \"set\": value is not None} for key, value in env_vars.items()},\n        \"set_count\": sum(1 for v in env_vars.values() if v is not None),\n        \"total_count\": len(env_vars),\n    }\n</code></pre>"},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json.format_json","title":"<code>format_json(data, output=None, indent=2)</code>","text":"<p>Format data as JSON and write to output.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Data to format</p> required <code>output</code> <code>Optional[TextIO]</code> <p>Output stream (defaults to stdout)</p> <code>None</code> <code>indent</code> <code>int</code> <p>JSON indentation level</p> <code>2</code> Source code in <code>src/openai_model_registry/cli/formatters/json.py</code> <pre><code>def format_json(data: Any, output: Optional[TextIO] = None, indent: int = 2) -&gt; None:\n    \"\"\"Format data as JSON and write to output.\n\n    Args:\n        data: Data to format\n        output: Output stream (defaults to stdout)\n        indent: JSON indentation level\n    \"\"\"\n    if output is None:\n        output = sys.stdout\n\n    json.dump(\n        data,\n        output,\n        indent=indent,\n        ensure_ascii=False,\n        sort_keys=True,\n        default=_default_serializer,\n    )\n    output.write(\"\\n\")\n</code></pre>"},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json.format_models_list_json","title":"<code>format_models_list_json(models)</code>","text":"<p>Format models list for JSON output.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Dict[str, Any]</code> <p>Models data</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Formatted data structure</p> Source code in <code>src/openai_model_registry/cli/formatters/json.py</code> <pre><code>def format_models_list_json(models: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Format models list for JSON output.\n\n    Args:\n        models: Models data\n\n    Returns:\n        Formatted data structure\n    \"\"\"\n    # Sort models by name for stable output\n    sorted_models = [{\"name\": name, **model_data} for name, model_data in sorted(models.items())]\n    return {\"models\": sorted_models, \"count\": len(models)}\n</code></pre>"},{"location":"api_reference/cli/formatters/json/#openai_model_registry.cli.formatters.json.format_providers_json","title":"<code>format_providers_json(providers, current)</code>","text":"<p>Format providers data for JSON output.</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>List[str]</code> <p>List of available providers</p> required <code>current</code> <code>str</code> <p>Current active provider</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Formatted data structure</p> Source code in <code>src/openai_model_registry/cli/formatters/json.py</code> <pre><code>def format_providers_json(providers: List[str], current: str) -&gt; Dict[str, Any]:\n    \"\"\"Format providers data for JSON output.\n\n    Args:\n        providers: List of available providers\n        current: Current active provider\n\n    Returns:\n        Formatted data structure\n    \"\"\"\n    # Sort providers for stable output\n    sorted_providers = sorted(providers)\n    return {\"providers\": sorted_providers, \"current\": current, \"count\": len(providers)}\n</code></pre>"},{"location":"api_reference/cli/formatters/table/","title":"Table","text":""},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table","title":"<code>openai_model_registry.cli.formatters.table</code>","text":"<p>Rich table formatter for CLI output.</p>"},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table-functions","title":"Functions","text":""},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table.create_console","title":"<code>create_console(output=None, no_color=False)</code>","text":"<p>Create a Rich console instance.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Optional[TextIO]</code> <p>Output stream (defaults to stdout)</p> <code>None</code> <code>no_color</code> <code>bool</code> <p>Disable color output</p> <code>False</code> <p>Returns:</p> Type Description <code>Console</code> <p>Console instance</p> Source code in <code>src/openai_model_registry/cli/formatters/table.py</code> <pre><code>def create_console(output: Optional[TextIO] = None, no_color: bool = False) -&gt; Console:\n    \"\"\"Create a Rich console instance.\n\n    Args:\n        output: Output stream (defaults to stdout)\n        no_color: Disable color output\n\n    Returns:\n        Console instance\n    \"\"\"\n    if output is None:\n        output = sys.stdout\n\n    # Let Rich use the actual terminal width to avoid truncating headers\n    return Console(file=output, no_color=no_color)\n</code></pre>"},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table.format_cache_info_table","title":"<code>format_cache_info_table(cache_info, console=None)</code>","text":"<p>Format cache information as a Rich table.</p> <p>Parameters:</p> Name Type Description Default <code>cache_info</code> <code>Dict[str, Any]</code> <p>Cache information</p> required <code>console</code> <code>Optional[Console]</code> <p>Rich console (will create if None)</p> <code>None</code> Source code in <code>src/openai_model_registry/cli/formatters/table.py</code> <pre><code>def format_cache_info_table(cache_info: Dict[str, Any], console: Optional[Console] = None) -&gt; None:\n    \"\"\"Format cache information as a Rich table.\n\n    Args:\n        cache_info: Cache information\n        console: Rich console (will create if None)\n    \"\"\"\n    if console is None:\n        console = create_console()\n\n    # Summary info\n    console.print(f\"[bold]Cache Directory:[/bold] {cache_info.get('directory', 'N/A')}\")\n    console.print(f\"[bold]Total Files:[/bold] {len(cache_info.get('files', []))}\")\n    console.print()\n\n    # Files table\n    files = cache_info.get(\"files\", [])\n    if files:\n        table = Table(title=\"Cache Files\", show_header=True, header_style=\"bold magenta\")\n\n        table.add_column(\"File\", style=\"cyan\")\n        table.add_column(\"Size\", justify=\"right\")\n        table.add_column(\"Modified\", style=\"dim\")\n        table.add_column(\"ETag\", style=\"dim\")\n\n        for file_info in files:\n            etag = file_info.get(\"etag\")\n            etag_display = etag[:12] + \"...\" if etag and len(etag) &gt; 15 else (etag or \"N/A\")\n\n            table.add_row(\n                file_info.get(\"name\", \"Unknown\"),\n                file_info.get(\"size_formatted\", \"N/A\"),\n                file_info.get(\"modified\", \"N/A\"),\n                etag_display,\n            )\n\n        console.print(table)\n    else:\n        console.print(\"[dim]No cache files found[/dim]\")\n</code></pre>"},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table.format_data_paths_table","title":"<code>format_data_paths_table(paths, console=None)</code>","text":"<p>Format data paths as a Rich table.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Dict[str, Any]</code> <p>Path information</p> required <code>console</code> <code>Optional[Console]</code> <p>Rich console (will create if None)</p> <code>None</code> Source code in <code>src/openai_model_registry/cli/formatters/table.py</code> <pre><code>def format_data_paths_table(paths: Dict[str, Any], console: Optional[Console] = None) -&gt; None:\n    \"\"\"Format data paths as a Rich table.\n\n    Args:\n        paths: Path information\n        console: Rich console (will create if None)\n    \"\"\"\n    if console is None:\n        console = create_console()\n\n    table = Table(title=\"Data Source Paths\", show_header=True, header_style=\"bold magenta\")\n\n    table.add_column(\"File\", style=\"cyan\")\n    table.add_column(\"Source\", style=\"yellow\")\n    table.add_column(\"Path\", style=\"dim\")\n    table.add_column(\"Status\", justify=\"center\")\n    table.add_column(\"Modified\", style=\"dim\")\n\n    for file_type, path_info in paths.items():\n        if isinstance(path_info, dict):\n            path = path_info.get(\"path\", \"N/A\")\n            source = path_info.get(\"source\", \"Unknown\")\n            exists = path_info.get(\"exists\", False)\n            last_modified = path_info.get(\"last_modified\", \"N/A\")\n        else:\n            path = str(path_info) if path_info else \"Bundled\"\n            source = \"User Data\" if path_info else \"Bundled\"\n            exists = path_info is not None\n            last_modified = \"N/A\"\n\n        # Status column\n        status = \"\u2713\" if exists else \"\u2717\"\n        status_style = \"green\" if exists else \"red\"\n\n        table.add_row(\n            file_type,\n            source,\n            path,\n            Text(status, style=status_style),\n            last_modified,\n        )\n\n    console.print(table)\n</code></pre>"},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table.format_env_vars_table","title":"<code>format_env_vars_table(env_vars, console=None)</code>","text":"<p>Format environment variables as a Rich table.</p> <p>Parameters:</p> Name Type Description Default <code>env_vars</code> <code>Dict[str, Optional[str]]</code> <p>Environment variables</p> required <code>console</code> <code>Optional[Console]</code> <p>Rich console (will create if None)</p> <code>None</code> Source code in <code>src/openai_model_registry/cli/formatters/table.py</code> <pre><code>def format_env_vars_table(env_vars: Dict[str, Optional[str]], console: Optional[Console] = None) -&gt; None:\n    \"\"\"Format environment variables as a Rich table.\n\n    Args:\n        env_vars: Environment variables\n        console: Rich console (will create if None)\n    \"\"\"\n    if console is None:\n        console = create_console()\n\n    table = Table(title=\"OMR Environment Variables\", show_header=True, header_style=\"bold magenta\")\n\n    table.add_column(\"Variable\", style=\"cyan\")\n    table.add_column(\"Value\", style=\"yellow\")\n    table.add_column(\"Set\", justify=\"center\")\n\n    for key, value in sorted(env_vars.items()):\n        is_set = value is not None\n        display_value = value if is_set else \"[dim]&lt;not set&gt;[/dim]\"\n        status = \"\u2713\" if is_set else \"\u2717\"\n        status_style = \"green\" if is_set else \"red\"\n\n        table.add_row(key, display_value, Text(status, style=status_style))\n\n    console.print(table)\n</code></pre>"},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table.format_models_table","title":"<code>format_models_table(models, console=None, columns=None)</code>","text":"<p>Format models as a Rich table.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Dict[str, Any]</code> <p>Models data</p> required <code>console</code> <code>Optional[Console]</code> <p>Rich console (will create if None)</p> <code>None</code> <code>columns</code> <code>Optional[List[str]]</code> <p>Custom columns to display (dotted paths)</p> <code>None</code> Source code in <code>src/openai_model_registry/cli/formatters/table.py</code> <pre><code>def format_models_table(\n    models: Dict[str, Any], console: Optional[Console] = None, columns: Optional[List[str]] = None\n) -&gt; None:\n    \"\"\"Format models as a Rich table.\n\n    Args:\n        models: Models data\n        console: Rich console (will create if None)\n        columns: Custom columns to display (dotted paths)\n    \"\"\"\n    if console is None:\n        console = create_console()\n\n    table = Table(title=\"OpenAI Models\", show_header=True, header_style=\"bold magenta\")\n\n    # Define default columns if none specified\n    if columns is None:\n        columns = [\n            \"name\",\n            \"context_window.total\",\n            \"context_window.output\",\n            \"context_window.input\",\n            \"pricing.input_cost_per_unit\",\n            \"pricing.output_cost_per_unit\",\n            \"pricing.unit\",\n            \"supports_vision\",\n            \"supports_function_calling\",\n            \"supports_streaming\",\n            \"supports_structured_output\",\n            \"supports_json_mode\",\n            \"supports_web_search\",\n            \"supports_audio\",\n        ]\n\n    # Helper to compute minimal width from a multi-line header\n    def _min_width_from_header(header: str) -&gt; int:\n        lines = header.split(\"\\n\")\n        return max(len(line) for line in lines) if lines else len(header)\n\n    # Add columns to table\n    for column_path in columns:\n        display_name = _get_column_display_name(column_path)\n\n        # Set column styling based on content type\n        if column_path == \"name\":\n            table.add_column(display_name, style=\"cyan\", no_wrap=True)\n        elif \"cost\" in column_path.lower() or \"window\" in column_path.lower() or \"output\" in column_path.lower():\n            table.add_column(\n                display_name,\n                justify=\"right\",\n                no_wrap=True,\n                min_width=_min_width_from_header(display_name),\n            )\n        elif \"supports_\" in column_path or column_path in [\"vision\", \"function_calling\", \"streaming\"]:\n            table.add_column(\n                display_name,\n                justify=\"center\",\n                no_wrap=True,\n                min_width=_min_width_from_header(display_name),\n            )\n        else:\n            table.add_column(\n                display_name,\n                no_wrap=True,\n                min_width=_min_width_from_header(display_name),\n            )\n\n    # Add data rows\n    for name, model_data in models.items():\n        row = []\n        for column_path in columns:\n            if column_path == \"name\":\n                value = name\n            else:\n                value = _extract_nested_value(model_data, column_path)\n\n            formatted_value = _format_column_value(value, column_path)\n            row.append(formatted_value)\n\n        table.add_row(*row)\n\n    console.print(table)\n</code></pre>"},{"location":"api_reference/cli/formatters/table/#openai_model_registry.cli.formatters.table.format_providers_table","title":"<code>format_providers_table(providers, current, console=None)</code>","text":"<p>Format providers as a Rich table.</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>List[str]</code> <p>List of available providers</p> required <code>current</code> <code>str</code> <p>Current active provider</p> required <code>console</code> <code>Optional[Console]</code> <p>Rich console (will create if None)</p> <code>None</code> Source code in <code>src/openai_model_registry/cli/formatters/table.py</code> <pre><code>def format_providers_table(providers: List[str], current: str, console: Optional[Console] = None) -&gt; None:\n    \"\"\"Format providers as a Rich table.\n\n    Args:\n        providers: List of available providers\n        current: Current active provider\n        console: Rich console (will create if None)\n    \"\"\"\n    if console is None:\n        console = create_console()\n\n    table = Table(title=\"Available Providers\", show_header=True, header_style=\"bold magenta\")\n\n    table.add_column(\"Provider\", style=\"cyan\")\n    table.add_column(\"Status\", justify=\"center\")\n\n    for provider in providers:\n        status = \"\u2713 Active\" if provider == current else \"\"\n        style = \"bold green\" if provider == current else \"\"\n        table.add_row(provider, status, style=style)\n\n    console.print(table)\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/","title":"Helpers","text":""},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers","title":"<code>openai_model_registry.cli.utils.helpers</code>","text":"<p>Helper functions for CLI operations.</p>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers-classes","title":"Classes","text":""},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.ExitCode","title":"<code>ExitCode</code>","text":"<p>Standard exit codes for the CLI.</p> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>class ExitCode:\n    \"\"\"Standard exit codes for the CLI.\"\"\"\n\n    SUCCESS = 0\n    GENERIC_ERROR = 1\n    INVALID_USAGE = 2\n    MODEL_NOT_FOUND = 3\n    DATA_SOURCE_ERROR = 4\n    UPDATE_AVAILABLE = 10  # CI-friendly code for update check\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers-functions","title":"Functions","text":""},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.format_file_size","title":"<code>format_file_size(size_bytes)</code>","text":"<p>Format file size in human-readable format.</p> <p>Parameters:</p> Name Type Description Default <code>size_bytes</code> <code>int</code> <p>Size in bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable size string</p> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>def format_file_size(size_bytes: int) -&gt; str:\n    \"\"\"Format file size in human-readable format.\n\n    Args:\n        size_bytes: Size in bytes\n\n    Returns:\n        Human-readable size string\n    \"\"\"\n    if size_bytes == 0:\n        return \"0 B\"\n\n    size_names = [\"B\", \"KB\", \"MB\", \"GB\"]\n    i: int = 0\n    while size_bytes &gt;= 1024 and i &lt; len(size_names) - 1:\n        size_bytes = int(size_bytes / 1024.0) if i &lt; len(size_names) - 1 else size_bytes\n        i += 1\n\n    return f\"{float(size_bytes):.1f} {size_names[i]}\"\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.get_omr_env_vars","title":"<code>get_omr_env_vars()</code>","text":"<p>Get all OMR_* environment variables.</p> <p>Returns:</p> Type Description <code>Dict[str, Optional[str]]</code> <p>Dictionary of OMR environment variables and their values</p> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>def get_omr_env_vars() -&gt; Dict[str, Optional[str]]:\n    \"\"\"Get all OMR_* environment variables.\n\n    Returns:\n        Dictionary of OMR environment variables and their values\n    \"\"\"\n    omr_vars: Dict[str, Optional[str]] = {}\n    for key, value in os.environ.items():\n        if key.startswith(\"OMR_\"):\n            omr_vars[key] = value\n\n    # Include commonly used variables even if not set\n    common_vars: List[str] = [\n        \"OMR_PROVIDER\",\n        \"OMR_DATA_DIR\",\n        \"OMR_DISABLE_DATA_UPDATES\",\n        \"OMR_DATA_VERSION_PIN\",\n        \"OMR_MODEL_REGISTRY_PATH\",\n        \"OMR_PARAMETER_CONSTRAINTS_PATH\",\n    ]\n\n    for var in common_vars:\n        if var not in omr_vars:\n            omr_vars[var] = None\n\n    return omr_vars\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.handle_error","title":"<code>handle_error(error, exit_code=ExitCode.GENERIC_ERROR)</code>","text":"<p>Handle CLI errors with consistent formatting.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>Exception</code> <p>Exception to handle</p> required <code>exit_code</code> <code>int</code> <p>Exit code to use</p> <code>GENERIC_ERROR</code> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>def handle_error(error: Exception, exit_code: int = ExitCode.GENERIC_ERROR) -&gt; None:\n    \"\"\"Handle CLI errors with consistent formatting.\n\n    Args:\n        error: Exception to handle\n        exit_code: Exit code to use\n    \"\"\"\n    click.echo(f\"Error: {str(error)}\", err=True)\n    sys.exit(exit_code)\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.resolve_format","title":"<code>resolve_format(cli_format=None, default_tty='table', default_non_tty='json')</code>","text":"<p>Resolve output format with TTY detection.</p> <p>Parameters:</p> Name Type Description Default <code>cli_format</code> <code>Optional[str]</code> <p>Format specified via CLI flag</p> <code>None</code> <code>default_tty</code> <code>str</code> <p>Default format for TTY output</p> <code>'table'</code> <code>default_non_tty</code> <code>str</code> <p>Default format for non-TTY output</p> <code>'json'</code> <p>Returns:</p> Type Description <code>str</code> <p>Resolved format name</p> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>def resolve_format(cli_format: Optional[str] = None, default_tty: str = \"table\", default_non_tty: str = \"json\") -&gt; str:\n    \"\"\"Resolve output format with TTY detection.\n\n    Args:\n        cli_format: Format specified via CLI flag\n        default_tty: Default format for TTY output\n        default_non_tty: Default format for non-TTY output\n\n    Returns:\n        Resolved format name\n    \"\"\"\n    if cli_format:\n        return cli_format.lower()\n\n    # Auto-detect based on TTY\n    if sys.stdout.isatty():\n        return default_tty\n    else:\n        return default_non_tty\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.resolve_provider","title":"<code>resolve_provider(cli_provider=None)</code>","text":"<p>Resolve provider using precedence: CLI flag &gt; OMR_PROVIDER env &gt; default 'openai'.</p> <p>Parameters:</p> Name Type Description Default <code>cli_provider</code> <code>Optional[str]</code> <p>Provider specified via CLI flag</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Resolved provider name</p> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>def resolve_provider(cli_provider: Optional[str] = None) -&gt; str:\n    \"\"\"Resolve provider using precedence: CLI flag &gt; OMR_PROVIDER env &gt; default 'openai'.\n\n    Args:\n        cli_provider: Provider specified via CLI flag\n\n    Returns:\n        Resolved provider name\n    \"\"\"\n    if cli_provider:\n        return cli_provider.lower()\n\n    env_provider = os.getenv(\"OMR_PROVIDER\")\n    if env_provider:\n        return env_provider.lower()\n\n    return \"openai\"\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.validate_format_support","title":"<code>validate_format_support(format_type, supported_formats, command_name, ctx_obj)</code>","text":"<p>Validate format support for a command with consistent fallback behavior.</p> <p>Parameters:</p> Name Type Description Default <code>format_type</code> <code>str</code> <p>The requested format</p> required <code>supported_formats</code> <code>List[str]</code> <p>List of supported formats for this command</p> required <code>command_name</code> <code>str</code> <p>Name of the command for error messages</p> required <code>ctx_obj</code> <code>Dict[str, Any]</code> <p>Click context object containing verbosity settings</p> required <p>Returns:</p> Type Description <code>str</code> <p>The validated format (may be changed from input for fallback)</p> <p>Raises:</p> Type Description <code>BadParameter</code> <p>For unsupported formats that can't fall back</p> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>def validate_format_support(\n    format_type: str,\n    supported_formats: List[str],\n    command_name: str,\n    ctx_obj: Dict[str, Any],\n) -&gt; str:\n    \"\"\"Validate format support for a command with consistent fallback behavior.\n\n    Args:\n        format_type: The requested format\n        supported_formats: List of supported formats for this command\n        command_name: Name of the command for error messages\n        ctx_obj: Click context object containing verbosity settings\n\n    Returns:\n        The validated format (may be changed from input for fallback)\n\n    Raises:\n        click.BadParameter: For unsupported formats that can't fall back\n    \"\"\"\n    if format_type in supported_formats:\n        return format_type\n\n    # Common fallback behavior for table/csv\n    if format_type in [\"table\", \"csv\"]:\n        fallback_format = \"json\" if \"json\" in supported_formats else supported_formats[0]\n        # Only show message in verbose mode to avoid cluttering output\n        if ctx_obj.get(\"verbose\", 0) &gt; 0:\n            click.echo(\n                f\"Note: {command_name} doesn't support '{format_type}' format, using {fallback_format} instead.\",\n                err=True,\n            )\n        return fallback_format\n    else:\n        # For other unsupported formats, show clear error\n        supported_list = \"', '\".join(supported_formats)\n        raise click.BadParameter(f\"Format '{format_type}' is not supported for {command_name}. Use '{supported_list}'.\")\n</code></pre>"},{"location":"api_reference/cli/utils/helpers/#openai_model_registry.cli.utils.helpers.validate_provider","title":"<code>validate_provider(provider)</code>","text":"<p>Validate and normalize provider name.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Provider name to validate</p> required <p>Returns:</p> Type Description <code>str</code> <p>Normalized provider name</p> <p>Raises:</p> Type Description <code>BadParameter</code> <p>If provider is invalid</p> Source code in <code>src/openai_model_registry/cli/utils/helpers.py</code> <pre><code>def validate_provider(provider: str) -&gt; str:\n    \"\"\"Validate and normalize provider name.\n\n    Args:\n        provider: Provider name to validate\n\n    Returns:\n        Normalized provider name\n\n    Raises:\n        click.BadParameter: If provider is invalid\n    \"\"\"\n    provider_lower = provider.lower()\n\n    # Try to get valid providers dynamically from registry\n    try:\n        from ...registry import ModelRegistry\n\n        registry = ModelRegistry.get_default()\n        valid_providers = registry.list_providers()\n\n        if valid_providers and provider_lower in [p.lower() for p in valid_providers]:\n            return provider_lower\n    except Exception:\n        # Fallback to static validation if registry is not available\n        pass\n\n    # Fallback to basic known providers + extensible validation\n    basic_providers = [\"openai\", \"azure\"]\n    if provider_lower in basic_providers:\n        return provider_lower\n\n    # If none of the above matched, it's invalid\n    raise click.BadParameter(f\"Invalid provider '{provider}'. Must be one of: {', '.join(basic_providers)}\")\n</code></pre>"},{"location":"api_reference/cli/utils/options/","title":"Options","text":""},{"location":"api_reference/cli/utils/options/#openai_model_registry.cli.utils.options","title":"<code>openai_model_registry.cli.utils.options</code>","text":"<p>Common CLI options and decorators.</p>"},{"location":"api_reference/cli/utils/options/#openai_model_registry.cli.utils.options-functions","title":"Functions","text":""},{"location":"api_reference/cli/utils/options/#openai_model_registry.cli.utils.options.common_options","title":"<code>common_options(func)</code>","text":"<p>Add common options (provider, format, verbosity) to a command.</p> Source code in <code>src/openai_model_registry/cli/utils/options.py</code> <pre><code>def common_options(func: F) -&gt; F:\n    \"\"\"Add common options (provider, format, verbosity) to a command.\"\"\"\n    func = provider_option(func)\n    func = format_option(func)\n    func = verbosity_options(func)\n    return func\n</code></pre>"},{"location":"api_reference/cli/utils/options/#openai_model_registry.cli.utils.options.format_option","title":"<code>format_option(func)</code>","text":"<p>Add --format option to a command.</p> Source code in <code>src/openai_model_registry/cli/utils/options.py</code> <pre><code>def format_option(func: F) -&gt; F:\n    \"\"\"Add --format option to a command.\"\"\"\n\n    @click.option(\n        \"--format\",\n        type=click.Choice([\"table\", \"json\", \"csv\", \"yaml\"], case_sensitive=False),\n        help=\"Output format. Defaults to 'table' for TTY, 'json' for non-TTY.\",\n    )\n    @wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n        return func(*args, **kwargs)\n\n    return cast(F, wrapper)\n</code></pre>"},{"location":"api_reference/cli/utils/options/#openai_model_registry.cli.utils.options.output_option","title":"<code>output_option(func)</code>","text":"<p>Add --output option to a command.</p> Source code in <code>src/openai_model_registry/cli/utils/options.py</code> <pre><code>def output_option(func: F) -&gt; F:\n    \"\"\"Add --output option to a command.\"\"\"\n\n    @click.option(\"--output\", \"-o\", type=click.Path(), help=\"Write output to file instead of stdout.\")\n    @wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n        return func(*args, **kwargs)\n\n    return cast(F, wrapper)\n</code></pre>"},{"location":"api_reference/cli/utils/options/#openai_model_registry.cli.utils.options.provider_option","title":"<code>provider_option(func)</code>","text":"<p>Add --provider option to a command.</p> Source code in <code>src/openai_model_registry/cli/utils/options.py</code> <pre><code>def provider_option(func: F) -&gt; F:\n    \"\"\"Add --provider option to a command.\"\"\"\n\n    @click.option(\n        \"--provider\",\n        type=str,\n        help=\"Override active provider (openai, azure, etc.). Takes precedence over OMR_PROVIDER environment variable.\",\n    )\n    @wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n        if kwargs.get(\"provider\"):\n            kwargs[\"provider\"] = validate_provider(kwargs[\"provider\"])\n        return func(*args, **kwargs)\n\n    return cast(F, wrapper)\n</code></pre>"},{"location":"api_reference/cli/utils/options/#openai_model_registry.cli.utils.options.verbosity_options","title":"<code>verbosity_options(func)</code>","text":"<p>Add verbosity options to a command.</p> Source code in <code>src/openai_model_registry/cli/utils/options.py</code> <pre><code>def verbosity_options(func: F) -&gt; F:\n    \"\"\"Add verbosity options to a command.\"\"\"\n\n    @click.option(\"--verbose\", \"-v\", count=True, help=\"Increase verbosity (can be used multiple times).\")\n    @click.option(\"--quiet\", \"-q\", count=True, help=\"Decrease verbosity (can be used multiple times).\")\n    @click.option(\"--debug\", is_flag=True, help=\"Enable debug-level logging.\")\n    @wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n        return func(*args, **kwargs)\n\n    return cast(F, wrapper)\n</code></pre>"},{"location":"api_reference/scripts/data_update/","title":"Data update","text":""},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update","title":"<code>openai_model_registry.scripts.data_update</code>","text":"<p>CLI script for managing OpenAI Model Registry data files.</p> <p>This script provides commands for updating, checking, and managing model registry data files with various options for customization.</p>"},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update-classes","title":"Classes","text":""},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update-functions","title":"Functions","text":""},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update.cmd_check","title":"<code>cmd_check(args)</code>","text":"<p>Check current data status and available updates.</p> Source code in <code>src/openai_model_registry/scripts/data_update.py</code> <pre><code>def cmd_check(args: argparse.Namespace) -&gt; int:\n    \"\"\"Check current data status and available updates.\"\"\"\n    setup_data_dir_env(args.data_dir)\n\n    try:\n        data_manager = DataManager()\n\n        # Check current version\n        current_version = data_manager._get_current_version()\n        if current_version:\n            print(f\"Current data version: {current_version}\")\n        else:\n            print(\"No local data version found (using bundled data)\")\n\n        # Check for updates\n        print(\"Checking for updates...\")\n        latest_release = data_manager._fetch_latest_data_release()\n\n        if not latest_release:\n            print(\"No data releases found on GitHub\")\n            return 1\n\n        latest_version = latest_release.get(\"tag_name\", \"\")\n        print(f\"Latest available version: {latest_version}\")\n\n        if current_version and data_manager._compare_versions(latest_version, current_version) &lt;= 0:\n            print(\"\u2713 Data is up to date\")\n            return 0\n        else:\n            print(\"\u26a0 Update available\")\n            return 0\n\n    except Exception as e:\n        logger.error(f\"Failed to check data status: {e}\")\n        return 1\n</code></pre>"},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update.cmd_clean","title":"<code>cmd_clean(args)</code>","text":"<p>Clean local data files (forces re-download on next use).</p> Source code in <code>src/openai_model_registry/scripts/data_update.py</code> <pre><code>def cmd_clean(args: argparse.Namespace) -&gt; int:\n    \"\"\"Clean local data files (forces re-download on next use).\"\"\"\n    setup_data_dir_env(args.data_dir)\n\n    try:\n        data_manager = DataManager()\n\n        if not args.yes:\n            response = input(f\"Remove all data files from {data_manager._data_dir}? [y/N]: \")\n            if response.lower() not in (\"y\", \"yes\"):\n                print(\"Cancelled\")\n                return 0\n\n        # Remove data files\n        files_removed = 0\n        for filename in [\"models.yaml\", \"overrides.yaml\", \"version_info.json\"]:\n            file_path = data_manager._data_dir / filename\n            if file_path.exists():\n                file_path.unlink()\n                files_removed += 1\n                print(f\"Removed: {filename}\")\n\n        if files_removed &gt; 0:\n            print(f\"\u2713 Removed {files_removed} files\")\n        else:\n            print(\"No files to remove\")\n\n        return 0\n\n    except Exception as e:\n        logger.error(f\"Failed to clean data: {e}\")\n        return 1\n</code></pre>"},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update.cmd_info","title":"<code>cmd_info(args)</code>","text":"<p>Show information about data configuration.</p> Source code in <code>src/openai_model_registry/scripts/data_update.py</code> <pre><code>def cmd_info(args: argparse.Namespace) -&gt; int:\n    \"\"\"Show information about data configuration.\"\"\"\n    setup_data_dir_env(args.data_dir)\n\n    try:\n        data_manager = DataManager()\n\n        print(\"Data Configuration:\")\n        print(f\"  Data directory: {data_manager._data_dir}\")\n        print(f\"  Update disabled: {not data_manager.should_update_data()}\")\n\n        # Environment variables\n        print(\"\\nEnvironment Variables:\")\n        print(f\"  OMR_DISABLE_DATA_UPDATES: {os.getenv('OMR_DISABLE_DATA_UPDATES', 'not set')}\")\n        print(f\"  OMR_DATA_VERSION_PIN: {os.getenv('OMR_DATA_VERSION_PIN', 'not set')}\")\n        print(f\"  OMR_DATA_DIR: {os.getenv('OMR_DATA_DIR', 'not set')}\")\n\n        # File status\n        print(\"\\nData Files:\")\n        for filename in [\"models.yaml\", \"overrides.yaml\"]:\n            file_path = data_manager.get_data_file_path(filename)\n            if file_path:\n                print(f\"  {filename}: {file_path} (exists)\")\n            else:\n                print(f\"  {filename}: not found (will use bundled)\")\n\n        # Version info\n        current_version = data_manager._get_current_version()\n        if current_version:\n            print(f\"\\nCurrent version: {current_version}\")\n        else:\n            print(\"\\nNo version info (using bundled data)\")\n\n        return 0\n\n    except Exception as e:\n        logger.error(f\"Failed to get data info: {e}\")\n        return 1\n</code></pre>"},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update.cmd_update","title":"<code>cmd_update(args)</code>","text":"<p>Update data files.</p> Source code in <code>src/openai_model_registry/scripts/data_update.py</code> <pre><code>def cmd_update(args: argparse.Namespace) -&gt; int:\n    \"\"\"Update data files.\"\"\"\n    setup_data_dir_env(args.data_dir)\n\n    try:\n        data_manager = DataManager()\n\n        if args.force:\n            print(\"Force updating data files...\")\n            success = data_manager.force_update()\n        else:\n            print(\"Checking for data updates...\")\n            success = data_manager.check_for_updates()\n\n        if success:\n            print(\"\u2713 Data files updated successfully\")\n            return 0\n        else:\n            print(\"\u26a0 No updates available or update failed\")\n            return 1\n\n    except Exception as e:\n        logger.error(f\"Failed to update data: {e}\")\n        return 1\n</code></pre>"},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update.create_parser","title":"<code>create_parser()</code>","text":"<p>Create the argument parser.</p> Source code in <code>src/openai_model_registry/scripts/data_update.py</code> <pre><code>def create_parser() -&gt; argparse.ArgumentParser:\n    \"\"\"Create the argument parser.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Manage OpenAI Model Registry data files\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  %(prog)s check                    # Check current status\n  %(prog)s update                   # Update if newer version available\n  %(prog)s update --force           # Force update to latest version\n  %(prog)s info                     # Show configuration and file status\n  %(prog)s clean                    # Remove local data files\n\nEnvironment Variables:\n  OMR_DISABLE_DATA_UPDATES=1        # Disable automatic updates\n  OMR_DATA_VERSION_PIN=v1.2.3       # Pin to specific version\n  OMR_DATA_DIR=/custom/path         # Use custom data directory\n        \"\"\",\n    )\n\n    parser.add_argument(\"--data-dir\", type=str, help=\"Custom data directory (overrides OMR_DATA_DIR)\")\n\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Enable verbose logging\")\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n\n    # Check command\n    check_parser = subparsers.add_parser(\"check\", help=\"Check data status and available updates\")\n    check_parser.set_defaults(func=cmd_check)\n\n    # Update command\n    update_parser = subparsers.add_parser(\"update\", help=\"Update data files\")\n    update_parser.add_argument(\"--force\", action=\"store_true\", help=\"Force update to latest version\")\n    update_parser.set_defaults(func=cmd_update)\n\n    # Info command\n    info_parser = subparsers.add_parser(\"info\", help=\"Show data configuration and status\")\n    info_parser.set_defaults(func=cmd_info)\n\n    # Clean command\n    clean_parser = subparsers.add_parser(\"clean\", help=\"Remove local data files\")\n    clean_parser.add_argument(\"--yes\", \"-y\", action=\"store_true\", help=\"Skip confirmation prompt\")\n    clean_parser.set_defaults(func=cmd_clean)\n\n    return parser\n</code></pre>"},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update.main","title":"<code>main()</code>","text":"<p>Main entry point.</p> Source code in <code>src/openai_model_registry/scripts/data_update.py</code> <pre><code>def main() -&gt; int:\n    \"\"\"Main entry point.\"\"\"\n    parser = create_parser()\n    args = parser.parse_args()\n\n    if not args.command:\n        parser.print_help()\n        return 1\n\n    # Set up logging\n    if args.verbose:\n        import logging\n\n        logging.basicConfig(level=logging.DEBUG)\n\n    # Run the command\n    try:\n        return cast(int, args.func(args))\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted\")\n        return 130\n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        return 1\n</code></pre>"},{"location":"api_reference/scripts/data_update/#openai_model_registry.scripts.data_update.setup_data_dir_env","title":"<code>setup_data_dir_env(data_dir)</code>","text":"<p>Set up the data directory environment variable if provided.</p> Source code in <code>src/openai_model_registry/scripts/data_update.py</code> <pre><code>def setup_data_dir_env(data_dir: Optional[str]) -&gt; None:\n    \"\"\"Set up the data directory environment variable if provided.\"\"\"\n    if data_dir:\n        os.environ[ENV_DATA_DIR] = str(Path(data_dir).expanduser().resolve())\n</code></pre>"},{"location":"api_reference/scripts/fetch_pricing_ostruct/","title":"Fetch pricing ostruct","text":""},{"location":"api_reference/scripts/fetch_pricing_ostruct/#openai_model_registry.scripts.fetch_pricing_ostruct","title":"<code>openai_model_registry.scripts.fetch_pricing_ostruct</code>","text":"<p>Fetch model pricing using the ostruct CLI.</p> <p>The script calls::</p> <pre><code>ostruct run &lt;template&gt; &lt;schema&gt; -V model=&lt;MODEL&gt;\n</code></pre> <p>and writes deterministic YAML to <code>pricing_data/{model}.yaml</code> so that the GitHub Action can commit only on change.</p> <p>Uses gpt-4.1 as the underlying LLM to ensure deterministic high-quality extraction.</p>"},{"location":"api_reference/scripts/fetch_pricing_ostruct/#openai_model_registry.scripts.fetch_pricing_ostruct-functions","title":"Functions","text":""},{"location":"api_reference/scripts/fetch_pricing_ostruct/#openai_model_registry.scripts.fetch_pricing_ostruct.main","title":"<code>main()</code>","text":"<p>CLI entry point that fetches pricing and updates cached YAML files.</p> Source code in <code>src/openai_model_registry/scripts/fetch_pricing_ostruct.py</code> <pre><code>def main() -&gt; None:  # noqa: D401\n    \"\"\"CLI entry point that fetches pricing and updates cached YAML files.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Fetch pricing via ostruct\")\n    parser.add_argument(\"--model\", required=True, help=\"Model name, e.g. gpt-4o\")\n    args = parser.parse_args()\n\n    pricing = run_ostruct(args.model)\n\n    output_path = OUTPUT_DIR / f\"{args.model}.yaml\"\n    tmp_path = output_path.with_suffix(\".tmp\")\n    write_yaml(pricing, tmp_path)\n\n    new_checksum = sha256_of_file(tmp_path)\n    old_checksum = sha256_of_file(output_path) if output_path.exists() else \"\"\n\n    if new_checksum != old_checksum:\n        tmp_path.rename(output_path)\n        print(f\"Updated pricing file: {output_path} ({new_checksum})\")\n    else:\n        tmp_path.unlink()\n        print(\"No pricing change detected \u2013 nothing to commit.\")\n</code></pre>"},{"location":"api_reference/scripts/fetch_pricing_ostruct/#openai_model_registry.scripts.fetch_pricing_ostruct.run_ostruct","title":"<code>run_ostruct(model)</code>","text":"<p>Invoke ostruct CLI and return parsed JSON.</p> <p>Resolution order: 1) Use system-resolved \"ostruct\" if available (recommended, provisioned by CI) 2) If not found and OMR_ALLOW_PIPX in {1, true, yes}, try pipx run with pinned version 3) If OMR_TEST_ALLOW_FAKE=1, return a minimal deterministic stub for tests</p> Source code in <code>src/openai_model_registry/scripts/fetch_pricing_ostruct.py</code> <pre><code>def run_ostruct(model: str) -&gt; Dict[str, Any]:\n    \"\"\"Invoke ostruct CLI and return parsed JSON.\n\n    Resolution order:\n    1) Use system-resolved \"ostruct\" if available (recommended, provisioned by CI)\n    2) If not found and OMR_ALLOW_PIPX in {1, true, yes}, try pipx run with pinned version\n    3) If OMR_TEST_ALLOW_FAKE=1, return a minimal deterministic stub for tests\n    \"\"\"\n    base_args = [\n        \"run\",\n        str(TEMPLATE),\n        str(SCHEMA),\n        \"--enable-tool\",\n        \"web-search\",\n        \"-V\",\n        f\"model={model}\",\n        \"-V\",\n        \"root_url=https://platform.openai.com/docs/models/\",\n        \"--model\",\n        \"gpt-4.1\",\n        \"--temperature\",\n        \"0\",\n    ]\n\n    def _exec(cmd: list[str]) -&gt; Dict[str, Any]:\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n        parsed: Dict[str, Any] = json.loads(result.stdout.strip())\n        return parsed\n\n    # First: try system ostruct if on PATH\n    if shutil.which(\"ostruct\"):\n        return _exec([\"ostruct\", *base_args])\n\n    # Fallback: optional pipx run if allowed\n    allow_pipx = os.getenv(\"OMR_ALLOW_PIPX\", \"\").lower() in {\"1\", \"true\", \"yes\"}\n    if allow_pipx and shutil.which(\"pipx\"):\n        return _exec([\"pipx\", \"run\", \"--spec\", \"ostruct-cli==1.6.1\", \"ostruct\", *base_args])\n\n    # Test-only fallback to avoid failing unit tests on missing tooling\n    if os.getenv(\"OMR_TEST_ALLOW_FAKE\", \"\").lower() in {\"1\", \"true\", \"yes\"}:\n        # Return minimal deterministic payload compatible with tests\n        return {\n            \"model\": model,\n            \"currency\": \"USD\",\n            \"input_per_million\": 0.0,\n            \"output_per_million\": 0.0,\n        }\n\n    raise RuntimeError(\"ostruct CLI not found. Install it or run with OMR_ALLOW_PIPX=1 and pipx available.\")\n</code></pre>"},{"location":"api_reference/scripts/fetch_pricing_ostruct/#openai_model_registry.scripts.fetch_pricing_ostruct.sha256_of_file","title":"<code>sha256_of_file(path)</code>","text":"<p>Return the SHA-256 checksum (hex digest) of the file at path.</p> Source code in <code>src/openai_model_registry/scripts/fetch_pricing_ostruct.py</code> <pre><code>def sha256_of_file(path: Path) -&gt; str:\n    \"\"\"Return the SHA-256 checksum (hex digest) of the file at *path*.\"\"\"\n    h = hashlib.sha256()\n    with path.open(\"rb\") as f:\n        h.update(f.read())\n    return h.hexdigest()\n</code></pre>"},{"location":"api_reference/scripts/fetch_pricing_ostruct/#openai_model_registry.scripts.fetch_pricing_ostruct.write_yaml","title":"<code>write_yaml(data, path)</code>","text":"<p>Write pricing data to path in deterministic YAML format.</p> Source code in <code>src/openai_model_registry/scripts/fetch_pricing_ostruct.py</code> <pre><code>def write_yaml(data: Dict[str, Any], path: Path) -&gt; None:\n    \"\"\"Write pricing *data* to *path* in deterministic YAML format.\"\"\"\n    import yaml  # local import to avoid hard dep if script unused\n\n    with path.open(\"w\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(data, f, sort_keys=True)\n</code></pre>"},{"location":"api_reference/scripts/update_registry/","title":"Update registry","text":""},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry","title":"<code>openai_model_registry.scripts.update_registry</code>","text":"<p>Command line utility for refreshing the model registry from remote source.</p>"},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry-classes","title":"Classes","text":""},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry-functions","title":"Functions","text":""},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry.main","title":"<code>main(verbose=False, force=False, url=None, validate=False, check=False)</code>","text":"<p>Update model registry from remote source.</p> <p>This command updates the local model registry configuration from a remote source. By default, it fetches the configuration from the official repository.</p> <p>The command will: 1. Download the latest model configuration 2. Validate the configuration format and values 3. Update the local configuration file</p> <p>Examples:</p>"},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry.main--basic-update-with-confirmation","title":"Basic update with confirmation","text":"<p>$ openai-model-registry-update</p>"},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry.main--update-with-verbose-output","title":"Update with verbose output","text":"<p>$ openai-model-registry-update -v</p>"},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry.main--update-from-custom-url-without-confirmation","title":"Update from custom URL without confirmation","text":"<p>$ openai-model-registry-update -f --url https://example.com/models.yaml</p>"},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry.main--validate-current-configuration-without-updating","title":"Validate current configuration without updating","text":"<p>$ openai-model-registry-update --validate</p>"},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry.main--check-for-updates-without-downloading","title":"Check for updates without downloading","text":"<p>$ openai-model-registry-update --check</p> Source code in <code>src/openai_model_registry/scripts/update_registry.py</code> <pre><code>@click.command()\n@click.option(\"-v\", \"--verbose\", is_flag=True, help=\"Show verbose output\")\n@click.option(\"-f\", \"--force\", is_flag=True, help=\"Skip confirmation prompt\")\n@click.option(\"--url\", help=\"Custom config URL\")\n@click.option(\"--validate\", is_flag=True, help=\"Validate without updating\")\n@click.option(\"--check\", is_flag=True, help=\"Check for updates without downloading\")\ndef main(\n    verbose: bool = False,\n    force: bool = False,\n    url: Optional[str] = None,\n    validate: bool = False,\n    check: bool = False,\n) -&gt; int:\n    \"\"\"Update model registry from remote source.\n\n    This command updates the local model registry configuration from a remote\n    source. By default, it fetches the configuration from the official repository.\n\n    The command will:\n    1. Download the latest model configuration\n    2. Validate the configuration format and values\n    3. Update the local configuration file\n\n    Examples:\n        # Basic update with confirmation\n        $ openai-model-registry-update\n\n        # Update with verbose output\n        $ openai-model-registry-update -v\n\n        # Update from custom URL without confirmation\n        $ openai-model-registry-update -f --url https://example.com/models.yaml\n\n        # Validate current configuration without updating\n        $ openai-model-registry-update --validate\n\n        # Check for updates without downloading\n        $ openai-model-registry-update --check\n    \"\"\"\n    return refresh_registry(\n        verbose=verbose,\n        force=force,\n        url=url,\n        validate=validate,\n        check_only=check,\n    )\n</code></pre>"},{"location":"api_reference/scripts/update_registry/#openai_model_registry.scripts.update_registry.refresh_registry","title":"<code>refresh_registry(verbose=False, force=False, url=None, validate=False, check_only=False)</code>","text":"<p>Refresh the model registry from remote source.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>Whether to print verbose output</p> <code>False</code> <code>force</code> <code>bool</code> <p>Skip confirmation prompt</p> <code>False</code> <code>url</code> <code>Union[str, None]</code> <p>Custom config URL</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Validate without updating</p> <code>False</code> <code>check_only</code> <code>bool</code> <p>Only check for updates without downloading</p> <code>False</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Exit code (0 for success, 1 for failure)</p> Source code in <code>src/openai_model_registry/scripts/update_registry.py</code> <pre><code>def refresh_registry(\n    verbose: bool = False,\n    force: bool = False,\n    url: Union[str, None] = None,\n    validate: bool = False,\n    check_only: bool = False,\n) -&gt; int:\n    \"\"\"Refresh the model registry from remote source.\n\n    Args:\n        verbose: Whether to print verbose output\n        force: Skip confirmation prompt\n        url: Custom config URL\n        validate: Validate without updating\n        check_only: Only check for updates without downloading\n\n    Returns:\n        int: Exit code (0 for success, 1 for failure)\n    \"\"\"\n    try:\n        registry = ModelRegistry.get_instance()\n\n        if validate:\n            registry._load_capabilities()  # Force revalidation\n            print(\"\u2705 Config validation successful\")\n            if verbose:\n                print(f\"\\nLocal registry file: {registry.config.registry_path}\")\n            return 0\n\n        if check_only:\n            result = registry.check_for_updates(url)\n            if result.success:\n                if result.status == RefreshStatus.UPDATE_AVAILABLE:\n                    print(\"\u2705 Registry update is available\")\n                    if verbose:\n                        print(f\"\\nStatus: {result.status.value}\")\n                        print(f\"Message: {result.message}\")\n                elif result.status == RefreshStatus.ALREADY_CURRENT:\n                    print(\"\u2713 Registry is already up to date\")\n                    if verbose:\n                        print(f\"\\nStatus: {result.status.value}\")\n                        print(f\"Message: {result.message}\")\n                return 0\n            else:\n                print(f\"\u274c Error checking for updates: {result.message}\")\n                return 1\n\n        # For actual update (when not in validate or check_only mode)\n        result = registry.check_for_updates(url)\n        if not force and result.status == RefreshStatus.ALREADY_CURRENT:\n            print(\"\u2713 Registry is already up to date\")\n            if verbose:\n                print(f\"\\nStatus: {result.status.value}\")\n                print(f\"Message: {result.message}\")\n            return 0\n\n        # Perform the update\n        result = registry.refresh_from_remote(url=url, force=force, validate_only=validate)\n        if result.success:\n            print(\"\u2705 Registry updated successfully\")\n            if verbose:\n                print(f\"\\nStatus: {result.status.value}\")\n                print(f\"Message: {result.message}\")\n            return 0\n        else:\n            print(f\"\u274c Error updating registry: {result.message}\")\n            return 1\n\n    except ModelNotSupportedError as e:\n        print(f\"\u274c Invalid model: {e}\")\n        return 1\n    except ModelVersionError as e:\n        print(f\"\u274c Config error: {e}\")\n        return 1\n    except Exception as e:\n        print(f\"\u274c Error refreshing model registry: {e}\")\n        return 1\n</code></pre>"},{"location":"contributing/","title":"Contributing Documentation","text":"<p>This section contains documentation for contributors and maintainers of the OpenAI Model Registry.</p>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>For maintainers involved in releasing new versions:</p> <ul> <li>Release Checklist - Step-by-step checklist for creating releases</li> <li>Release Workflow - Detailed workflow documentation and troubleshooting</li> <li>Includes a Contributor Data Release Playbook (how to publish data, tag RC/final, and verify client awareness)</li> </ul>"},{"location":"contributing/#general-contributing","title":"General Contributing","text":"<p>For general contribution guidelines, see the main Contributing Guide in the repository root.</p>"},{"location":"contributing/RELEASE_CHECKLIST/","title":"Release Readiness Checklist for OpenAI Model Registry","text":"<p>This checklist covers validation steps after you decide a version is ready for release.</p> <p>For detailed release workflow and branching rules, see the full guide in this section (Release Workflow).</p>"},{"location":"contributing/RELEASE_CHECKLIST/#dynamic-versioning-release-process","title":"\ud83c\udff7\ufe0f Dynamic Versioning &amp; Release Process","text":"<p>IMPORTANT: This project uses semantic versioning with Git tags for releases.</p>"},{"location":"contributing/RELEASE_CHECKLIST/#version-management","title":"Version Management","text":"<ul> <li>\u2705 DO: Create Git tags to set versions (<code>git tag v1.0.0</code> or <code>git tag v1.0.0-rc1</code>)</li> <li>\u2705 DO: Use semantic versioning (MAJOR.MINOR.PATCH)</li> <li>\u2705 Verification: Check version consistency with <code>poetry version --short</code></li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#release-types","title":"Release Types","text":""},{"location":"contributing/RELEASE_CHECKLIST/#1-library-releases","title":"1. Library Releases","text":"<ul> <li>Current Version: v1.0.0 (First stable release)</li> <li>Release Candidates: <code>v1.0.0-rc1</code>, <code>v1.0.0-rc2</code>, etc.</li> <li>Final Releases: <code>v1.0.0</code>, <code>v1.0.1</code>, <code>v1.1.0</code>, etc.</li> <li>Publishing: RC \u2192 TestPyPI, Final \u2192 PyPI</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#2-data-releases","title":"2. Data Releases","text":"<ul> <li>Current Version: data-v1.0.0 (First data format release)</li> <li>Release Candidates: <code>data-v1.0.0-rc1</code>, <code>data-v1.0.0-rc2</code>, etc.</li> <li>Final Releases: <code>data-v1.0.0</code>, <code>data-v1.0.1</code>, etc.</li> <li>Publishing: GitHub Releases with data packages</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#release-candidate-rc-process","title":"Release Candidate (RC) Process","text":"<ol> <li>Create RC tag:</li> </ol> <pre><code># Library RC\n./scripts/release/create-rc.sh 1.0.0 1\n\n# Data RC\n./scripts/release/create-rc.sh 1.0.0 1 --data\n</code></pre> <ol> <li> <p>CI automatically handles:</p> </li> <li> <p>\u2705 Building the package from the tagged commit</p> </li> <li>\u2705 Publishing library RCs to TestPyPI</li> <li>\u2705 Creating GitHub release with artifacts</li> <li> <p>\u2705 Marking as prerelease</p> </li> <li> <p>Test RC:</p> </li> </ol> <pre><code># Library RC from TestPyPI\npip install --index-url https://test.pypi.org/simple/ \\\n  --extra-index-url https://pypi.org/simple/ \\\n  \"openai-model-registry==1.0.0rc1\"\n</code></pre>"},{"location":"contributing/RELEASE_CHECKLIST/#data-rc-from-github-releases-see-releases-tab-in-repository","title":"Data RC from GitHub Releases (see Releases tab in repository)","text":"<pre><code>4. **Verify RC works:**\n\n```bash\npython -c \"import openai_model_registry; print('\u2705 Import successful')\"\npython -c \"from openai_model_registry import ModelRegistry, RegistryConfig; registry = ModelRegistry(RegistryConfig(auto_update=False)); print('\u2705 Registry creation successful')\"\n</code></pre>"},{"location":"contributing/RELEASE_CHECKLIST/#final-release-process","title":"Final Release Process","text":"<ol> <li>Create final tag:</li> </ol> <pre><code># Library release\n./scripts/release/create-final.sh 1.0.0\n\n# Data release\n./scripts/release/create-final.sh 1.0.0 --data\n</code></pre> <ol> <li> <p>CI automatically publishes:</p> </li> <li> <p>\u2705 Library releases to PyPI</p> </li> <li>\u2705 Data releases to GitHub Releases</li> <li>\u2705 Creates comprehensive release notes</li> </ol>"},{"location":"contributing/RELEASE_CHECKLIST/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>\ud83d\udeab Don't skip RC testing - Always test release candidates thoroughly</li> <li>\ud83d\udeab Don't manually publish to PyPI - CI handles this automatically</li> <li>\ud83d\udeab Don't forget to test both library and data releases - They're separate but related</li> <li>\ud83d\udeab Don't assume TestPyPI install worked - Always verify functionality</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#pre-release-testing-strategy","title":"Pre-Release Testing Strategy","text":""},{"location":"contributing/RELEASE_CHECKLIST/#1-automated-validation-required","title":"1. Automated Validation (REQUIRED)","text":"<p>Run the automated validation script:</p> <pre><code># Full validation with clean installation tests\n./scripts/release/create-rc.sh\n\n# Quick validation (skip slow clean install tests)\n./scripts/release/create-final.sh\n</code></pre> <p>This script performs:</p> <ul> <li>\u2705 Version consistency checks</li> <li>\u2705 pyproject.toml validation</li> <li>\u2705 Package building (wheel + sdist)</li> <li>\u2705 Dependency resolution testing</li> <li>\u2705 Test suite execution</li> <li>\u2705 Documentation building</li> <li>\u2705 Clean virtual environment installation testing</li> </ul> <p>You can also run individual commands manually:</p> <pre><code># Run tests\npoetry run pytest -v\n\n# Check package\npoetry check\npoetry build\npython -m twine check dist/*\n\n# Test installation\npip install dist/*.whl\n</code></pre>"},{"location":"contributing/RELEASE_CHECKLIST/#2-manual-testing-recommended","title":"2. Manual Testing (RECOMMENDED)","text":""},{"location":"contributing/RELEASE_CHECKLIST/#test-core-functionality","title":"Test Core Functionality","text":"<pre><code># Test basic registry operations\npython -c \"\nfrom openai_model_registry import ModelRegistry, RegistryConfig\nconfig = RegistryConfig(auto_update=False)\nregistry = ModelRegistry(config)\nprint(f'Registry has {len(registry.list_models())} models')\n\"\n\n# Test model lookup\npython -c \"\nfrom openai_model_registry import ModelRegistry, RegistryConfig\nconfig = RegistryConfig(auto_update=False)\nregistry = ModelRegistry(config)\ntry:\n    model = registry.get_model('gpt-4')\n    print(f'Model: {model.name}')\nexcept Exception as e:\n    print(f'Model lookup: {e}')\n\"\n</code></pre>"},{"location":"contributing/RELEASE_CHECKLIST/#test-update-functionality","title":"Test Update Functionality","text":"<pre><code># Test update information\npython -c \"\nfrom openai_model_registry import ModelRegistry, RegistryConfig\nconfig = RegistryConfig(auto_update=False)\nregistry = ModelRegistry(config)\nupdate_info = registry.get_update_info()\nprint(f'Update available: {update_info.update_available}')\nprint(f'Current version: {update_info.current_version}')\n\"\n</code></pre>"},{"location":"contributing/RELEASE_CHECKLIST/#test-in-fresh-virtual-environments","title":"Test in Fresh Virtual Environments","text":"<pre><code># Test Python 3.10\npython3.10 -m venv test_env_310\nsource test_env_310/bin/activate\npip install dist/*.whl\npython -c \"import openai_model_registry; print('\u2705 Python 3.10 works')\"\ndeactivate\n\n# Test Python 3.11\npython3.11 -m venv test_env_311\nsource test_env_311/bin/activate\npip install dist/*.whl\npython -c \"import openai_model_registry; print('\u2705 Python 3.11 works')\"\ndeactivate\n\n# Test Python 3.12\npython3.12 -m venv test_env_312\nsource test_env_312/bin/activate\npip install dist/*.whl\npython -c \"import openai_model_registry; print('\u2705 Python 3.12 works')\"\ndeactivate\n</code></pre>"},{"location":"contributing/RELEASE_CHECKLIST/#3-cross-platform-testing","title":"3. Cross-Platform Testing","text":"<p>The cross-platform workflow automatically tests:</p> <ul> <li>\u2705 Installation on Ubuntu, Windows, macOS</li> <li>\u2705 Python 3.10, 3.11, 3.12 compatibility</li> <li>\u2705 Wheel and sdist installation</li> <li>\u2705 Import performance</li> <li>\u2705 Memory usage</li> <li>\u2705 Dependency resolution</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#release-validation-checklist","title":"Release Validation Checklist","text":""},{"location":"contributing/RELEASE_CHECKLIST/#pre-release-validation","title":"Pre-Release Validation","text":"<ul> <li> Automated validation passes: CI workflows (build/tests/docs) are green</li> <li> All tests pass: <code>poetry run pytest -v</code></li> <li> Package builds cleanly: <code>poetry build</code></li> <li> Documentation builds: Check docs build in CI</li> <li> Version consistency: Poetry and package versions match</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#package-quality","title":"Package Quality","text":"<ul> <li> Package size is reasonable: Check <code>dist/</code> directory (\\&lt; 10MB for wheel)</li> <li> Both wheel and sdist created: Check <code>dist/</code> directory</li> <li> Package metadata is correct: <code>python -m twine check dist/*</code></li> <li> Dependencies are correct: <code>poetry show --tree</code></li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#functionality-testing","title":"Functionality Testing","text":"<ul> <li> Basic import works: <code>python -c \"import openai_model_registry\"</code></li> <li> Registry creation works: Test ModelRegistry instantiation</li> <li> Model lookup works: Test getting models from registry</li> <li> Update system works: Test update information retrieval</li> <li> Configuration system works: Test RegistryConfig options</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#cross-platform-testing","title":"Cross-Platform Testing","text":"<ul> <li> CI tests pass: All GitHub Actions workflows green</li> <li> Multiple Python versions: 3.10, 3.11, 3.12 tested</li> <li> Multiple OS: Ubuntu, Windows, macOS tested</li> <li> Installation methods: wheel, sdist, and PyPI installation tested</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#github-actions-ci-validation","title":"GitHub Actions CI Validation","text":"<p>Ensure your GitHub Actions CI is passing:</p> <ul> <li> Main CI workflow: Tests pass on all supported Python versions and OS</li> <li> Cross-platform tests: Installation tests pass</li> <li> Documentation builds: Docs build successfully</li> <li> Security scans: No security issues found</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#final-steps-before-release","title":"Final Steps Before Release","text":"<ol> <li>Run comprehensive validation:</li> </ol> <pre><code>./scripts/release/create-rc.sh\n</code></pre> <ol> <li> <p>Verify CI is green on the main branch</p> </li> <li> <p>Update CHANGELOG.md with version changes</p> </li> <li> <p>Create RC first (recommended):</p> </li> </ol> <pre><code>./scripts/release/create-rc.sh 1.0.0 1\n</code></pre> <ol> <li> <p>Test RC thoroughly from TestPyPI</p> </li> <li> <p>Create final release:</p> </li> </ol> <pre><code>./scripts/release/create-final.sh 1.0.0\n</code></pre> <ol> <li>CI automatically publishes to PyPI (no manual action needed)</li> </ol>"},{"location":"contributing/RELEASE_CHECKLIST/#post-release-verification","title":"Post-Release Verification","text":"<p>After publishing to PyPI:</p> <ol> <li>Test installation from PyPI:</li> </ol> <pre><code>pip install openai-model-registry==&lt;VERSION&gt;\npython -c \"import openai_model_registry; print('\u2705 PyPI installation works')\"\n</code></pre> <ol> <li>Test in fresh environment:</li> </ol> <pre><code>python -m venv test_env\nsource test_env/bin/activate  # or test_env\\Scripts\\activate on Windows\npip install openai-model-registry==&lt;VERSION&gt;\npython -c \"from openai_model_registry import ModelRegistry; print('\u2705 Fresh install works')\"\ndeactivate\n</code></pre> <ol> <li> <p>Verify GitHub release has correct artifacts and release notes</p> </li> <li> <p>Test data releases if applicable</p> </li> </ol>"},{"location":"contributing/RELEASE_CHECKLIST/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/RELEASE_CHECKLIST/#common-issues","title":"Common Issues","text":"<ol> <li>Version mismatch: Ensure poetry and package versions match</li> <li>Test failures: Run <code>poetry run pytest -v</code> to see detailed errors</li> <li>Build failures: Check <code>poetry build</code> output for errors</li> <li>Import errors: Test in clean virtual environment</li> </ol>"},{"location":"contributing/RELEASE_CHECKLIST/#getting-help","title":"Getting Help","text":"<ul> <li>CI logs: Check GitHub Actions for detailed error messages</li> <li>Local testing: Use <code>poetry run pytest -q</code> and <code>poetry run mkdocs build</code> for smoke testing</li> <li>Documentation: See Release Workflow for detailed procedures</li> </ul>"},{"location":"contributing/RELEASE_CHECKLIST/#related-files","title":"Related Files","text":"<ul> <li>scripts/release/create-rc.sh - Create release candidates</li> <li>scripts/release/create-final.sh - Create final releases</li> <li>Release automation uses shell scripts and GitHub Actions; <code>validate-release.py</code> is no longer used</li> <li>.github/workflows/release.yml - Library release workflow</li> <li>.github/workflows/data-release-enhanced.yml - Data release workflow</li> <li>.github/workflows/cross-platform-test.yml - Cross-platform testing</li> </ul>"},{"location":"contributing/RELEASE_WORKFLOW/","title":"Release &amp; Hot-Fix Workflow","text":"<p>This document explains how to cut a new feature release, data release, or hot-fix in the OpenAI Model Registry project.</p>"},{"location":"contributing/RELEASE_WORKFLOW/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Branching Model</li> <li>Release Types</li> <li>Library Release Workflow</li> <li>Data Release Workflow</li> <li>Hot-Fix Workflow</li> <li>Do's and Don'ts</li> <li>Troubleshooting</li> </ul>"},{"location":"contributing/RELEASE_WORKFLOW/#branching-model","title":"Branching Model","text":"<ul> <li><code>main</code> \u2013 the only long-lived branch. All daily work happens here.</li> <li>Tags \u2013 every published version (<code>v1.0.0</code>, <code>v1.0.1</code>, <code>data-v1.0.0</code>) is an annotated git tag on <code>main</code>.</li> <li>Hot-fix branch \u2013 created only if you must fix the current release without pulling in unreleased work that is already on <code>main</code>. Delete it right after the tag is pushed.</li> </ul>"},{"location":"contributing/RELEASE_WORKFLOW/#release-types","title":"Release Types","text":""},{"location":"contributing/RELEASE_WORKFLOW/#1-library-releases","title":"1. Library Releases","text":"<p>Library releases contain the Python package code and are published to PyPI.</p> <ul> <li>Tag Format: <code>v1.0.0</code>, <code>v1.0.1</code>, <code>v1.1.0</code></li> <li>RC Format: <code>v1.0.0-rc1</code>, <code>v1.0.0-rc2</code></li> <li>Publishing:</li> <li>RC \u2192 TestPyPI</li> <li>Final \u2192 PyPI</li> </ul>"},{"location":"contributing/RELEASE_WORKFLOW/#2-data-releases","title":"2. Data Releases","text":"<p>Data releases contain model configuration files and are published as GitHub releases.</p> <ul> <li>Tag Format: <code>data-v1.0.0</code>, <code>data-v1.0.1</code></li> <li>RC Format: <code>data-v1.0.0-rc1</code>, <code>data-v1.0.0-rc2</code></li> <li>Publishing: GitHub Releases with data packages</li> </ul>"},{"location":"contributing/RELEASE_WORKFLOW/#library-release-workflow","title":"Library Release Workflow","text":""},{"location":"contributing/RELEASE_WORKFLOW/#before-writing-code","title":"Before Writing Code","text":"<ol> <li>Make sure you are on <code>main</code> and up-to-date:</li> </ol> <pre><code>git switch main\ngit pull --ff-only origin main\n</code></pre> <ol> <li>Create a short feature branch (optional but recommended):</li> </ol> <pre><code>git switch -c feature/&lt;short-name&gt;\n</code></pre> <ol> <li> <p>Implement the feature, update tests, update <code>CHANGELOG.md</code> incrementally.</p> </li> <li> <p>Open a PR (even if you merge it yourself) \u2013 CI must be green.</p> </li> </ol>"},{"location":"contributing/RELEASE_WORKFLOW/#creating-a-release-candidate","title":"Creating a Release Candidate","text":"<ol> <li> <p>Merge your PR into <code>main</code>.</p> </li> <li> <p>Verify <code>CHANGELOG.md</code> and docs are ready.</p> </li> <li> <p>Run pre-release validation:</p> </li> </ol> <pre><code>./scripts/release/create-rc.sh\n</code></pre> <ol> <li>Create RC tag:</li> </ol> <pre><code>./scripts/release/create-rc.sh 1.0.0 1\n</code></pre> <ol> <li> <p>CI automatically:</p> </li> <li> <p>Builds the package</p> </li> <li>Runs tests</li> <li>Publishes to TestPyPI</li> <li> <p>Creates GitHub release (marked as prerelease)</p> </li> <li> <p>Test the RC:</p> </li> </ol> <pre><code>pip install --index-url https://test.pypi.org/simple/ \\\n  --extra-index-url https://pypi.org/simple/ \\\n  \"openai-model-registry==1.0.0rc1\"\n</code></pre> <ol> <li>Verify functionality:</li> </ol> <pre><code>python -c \"import openai_model_registry; print('\u2705 Import works')\"\npython -c \"from openai_model_registry import ModelRegistry, RegistryConfig; registry = ModelRegistry(RegistryConfig(auto_update=False)); print('\u2705 Registry works')\"\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#creating-the-final-release","title":"Creating the Final Release","text":"<ol> <li>After RC testing is complete and successful:</li> </ol> <pre><code>./scripts/release/create-final.sh 1.0.0\n</code></pre> <ol> <li> <p>CI automatically:</p> </li> <li> <p>Builds the package</p> </li> <li>Runs tests</li> <li>Publishes to PyPI</li> <li> <p>Creates GitHub release (production)</p> </li> <li> <p>Verify PyPI publication:</p> </li> </ol> <pre><code>pip install openai-model-registry==1.0.0\npython -c \"import openai_model_registry; print('\u2705 PyPI install works')\"\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#data-release-workflow","title":"Data Release Workflow","text":""},{"location":"contributing/RELEASE_WORKFLOW/#automatic-data-releases","title":"Automatic Data Releases","text":"<p>Data releases are automatically triggered when configuration files change:</p> <ol> <li> <p>Edit data files in <code>data/</code>:</p> </li> <li> <p><code>models.yaml</code> - Model definitions</p> </li> <li> <p><code>overrides.yaml</code> - Provider-specific overrides</p> </li> <li> <p>Commit and push to <code>main</code>:</p> </li> </ol> <pre><code>git add data/\ngit commit -m \"feat: add new model definitions\"\ngit push origin main\n</code></pre> <ol> <li> <p>CI automatically:</p> </li> <li> <p>Detects config changes</p> </li> <li>Validates YAML files</li> <li>Creates data package</li> <li>Increments version</li> <li>Creates GitHub release</li> </ol>"},{"location":"contributing/RELEASE_WORKFLOW/#contributor-data-release-playbook","title":"Contributor Data Release Playbook","text":"<p>Use this checklist when publishing a new data release (or RC):</p> <ol> <li>Update files:</li> <li><code>data/models.yaml</code></li> <li> <p><code>data/overrides.yaml</code></p> </li> <li> <p>Update <code>data/data-changelog.md</code> (summarize changes, breaking notes)</p> </li> <li>Tag and publish:</li> <li>RC: <code>data-vX.Y.Z-rcN</code></li> <li>Final: <code>data-vX.Y.Z</code></li> <li>Attach assets: <code>models.yaml</code>, <code>overrides.yaml</code></li> <li>Provide clear Release notes and link to changelog</li> <li>Verify client awareness:</li> <li>Run <code>omr --format json update check</code> \u2192 exit code <code>10</code> indicates an available update</li> <li>In code: <code>ModelRegistry.check_for_updates()</code> / <code>get_update_info()</code> returns latest</li> <li>Optionally notify via GitHub Releases (watch \u2192 releases)</li> <li>Respect environments:</li> <li>Clients may pin via <code>OMR_DATA_VERSION_PIN</code></li> <li>Clients may disable updates via <code>OMR_DISABLE_DATA_UPDATES</code></li> <li>Confirm integrity:</li> <li>Assets properly attached and downloadable</li> <li>Fallback raw URLs (only if needed) should be tag-pinned for immutability</li> </ol>"},{"location":"contributing/RELEASE_WORKFLOW/#manual-data-releases","title":"Manual Data Releases","text":"<p>For manual data releases or RCs:</p> <ol> <li>Create RC:</li> </ol> <pre><code>./scripts/release/create-rc.sh 1.0.0 1 --data\n</code></pre> <ol> <li>Create final release:</li> </ol> <pre><code>./scripts/release/create-final.sh 1.0.0 --data\n</code></pre> <ol> <li>Test data release:</li> </ol> <pre><code># Download and verify\ncurl -L https://github.com/yaniv-golan/openai-model-registry/releases/download/data-v1.0.0/openai-model-registry-data-1.0.0.tar.gz -o data.tar.gz\ntar -xzf data.tar.gz\ncd data-package\n# Verify files are present\nls -la models.yaml overrides.yaml\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#hot-fix-workflow","title":"Hot-Fix Workflow","text":""},{"location":"contributing/RELEASE_WORKFLOW/#scenario-a-fix-is-already-on-main-and-safe-to-release","title":"Scenario A \u2013 Fix is already on <code>main</code> and safe to release","text":"<ol> <li> <p>Ensure <code>CHANGELOG.md</code> has a Hot-fix entry.</p> </li> <li> <p>Tag and push directly from <code>main</code>:</p> </li> </ol> <pre><code>git switch main &amp;&amp; git pull --ff-only\n./scripts/release/create-final.sh 1.1.1\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#scenario-b-fix-must-not-include-unreleased-work-on-main","title":"Scenario B \u2013 Fix must not include unreleased work on <code>main</code>","text":"<ol> <li>Create a throw-away branch from the last tag:</li> </ol> <pre><code>git switch -c hotfix/v1.1.1 v1.1.0\n</code></pre> <ol> <li> <p>Cherry-pick or implement just the necessary commits.</p> </li> <li> <p>Update <code>CHANGELOG.md</code> in that branch.</p> </li> <li> <p>Tag &amp; push:</p> </li> </ol> <pre><code>git tag -a v1.1.1 -m \"v1.1.1: security hot-fix\"\ngit push origin hotfix/v1.1.1 v1.1.1\n</code></pre> <ol> <li>After CI turns green, delete the branch:</li> </ol> <pre><code>git push origin --delete hotfix/v1.1.1\ngit branch -D hotfix/v1.1.1\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#dos-and-donts","title":"Do's and Don'ts","text":""},{"location":"contributing/RELEASE_WORKFLOW/#do","title":"Do \u2713","text":"<ul> <li>Keep <code>main</code> in a releasable state; let CI protect you.</li> <li>Use annotated tags (<code>-a</code>) for every version.</li> <li>Update the CHANGELOG steadily while you code.</li> <li>Delete merged or obsolete branches immediately.</li> <li>Test release candidates thoroughly before final release.</li> <li>Use the provided scripts for consistent release process.</li> <li>Run validation before creating releases.</li> </ul>"},{"location":"contributing/RELEASE_WORKFLOW/#dont","title":"Don't \u2717","text":"<ul> <li>\u2717 Don't create long-lived <code>release/x.y</code> branches \u2013 history gets messy.</li> <li>\u2717 Don't manually publish to PyPI \u2013 CI handles this automatically.</li> <li>\u2717 Don't force-push to shared branches or tags.</li> <li>\u2717 Don't skip RC testing for significant releases.</li> <li>\u2717 Don't forget to update CHANGELOG.md.</li> </ul>"},{"location":"contributing/RELEASE_WORKFLOW/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/RELEASE_WORKFLOW/#common-issues","title":"Common Issues","text":""},{"location":"contributing/RELEASE_WORKFLOW/#release-script-fails","title":"Release Script Fails","text":"<pre><code># Check if you're on main branch\ngit branch --show-current\n\n# Ensure you're up to date\ngit pull --ff-only origin main\n\n# Check for existing tags\ngit tag -l | grep v1.0.0\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#ci-build-fails","title":"CI Build Fails","text":"<ol> <li> <p>Check GitHub Actions logs for detailed error messages</p> </li> <li> <p>Run validation locally:</p> </li> </ol> <pre><code>./scripts/release/create-final.sh\n</code></pre> <ol> <li>Test locally:</li> </ol> <pre><code>poetry run pytest -v\npoetry build\npython -m twine check dist/*\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#testpypi-installation-fails","title":"TestPyPI Installation Fails","text":"<pre><code># Try with verbose output\npip install -v --index-url https://test.pypi.org/simple/ \\\n  --extra-index-url https://pypi.org/simple/ \\\n  \"openai-model-registry==1.0.0rc1\"\n\n# Check if dependencies are available\npip install --index-url https://test.pypi.org/simple/ \\\n  --extra-index-url https://pypi.org/simple/ \\\n  --dry-run \"openai-model-registry==1.0.0rc1\"\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#version-mismatch","title":"Version Mismatch","text":"<pre><code># Check current versions\npoetry version --short\npython -c \"import openai_model_registry; print(openai_model_registry.__version__)\"\n\n# Update if needed\npoetry version 1.0.0\n</code></pre>"},{"location":"contributing/RELEASE_WORKFLOW/#getting-help","title":"Getting Help","text":"<ol> <li>Check CI logs in GitHub Actions</li> <li>Review validation output from CI workflows (build/tests/docs) triggered by the release tag</li> <li>Test in clean environment to isolate issues</li> <li>Check existing tags to avoid conflicts</li> </ol>"},{"location":"contributing/RELEASE_WORKFLOW/#automation-overview","title":"Automation Overview","text":""},{"location":"contributing/RELEASE_WORKFLOW/#github-actions-workflows","title":"GitHub Actions Workflows","text":"<ol> <li> <p>release.yml - Library releases</p> </li> <li> <p>Triggered by <code>v*</code> and <code>v*-rc*</code> tags</p> </li> <li>Builds package, runs tests, publishes to PyPI/TestPyPI</li> <li> <p>Creates GitHub releases</p> </li> <li> <p>data-release-enhanced.yml - Data releases</p> </li> <li> <p>Triggered by config file changes or <code>data-v*</code> tags</p> </li> <li>Validates YAML, creates data packages</li> <li> <p>Creates GitHub releases</p> </li> <li> <p>cross-platform-test.yml - Cross-platform testing</p> </li> <li> <p>Tests installation across OS and Python versions</p> </li> <li>Validates package quality and performance</li> </ol>"},{"location":"contributing/RELEASE_WORKFLOW/#scripts","title":"Scripts","text":"<ol> <li>create-rc.sh - Create release candidates</li> <li>create-final.sh - Create final releases</li> <li>Use GitHub Actions logs for validation (build/tests/docs) instead of local <code>validate-release.py</code></li> </ol>"},{"location":"contributing/RELEASE_WORKFLOW/#related-documentation","title":"Related Documentation","text":"<ul> <li>RELEASE_CHECKLIST.md - Pre-release validation checklist</li> <li>CHANGELOG.md - Version history and changes</li> <li>CONTRIBUTING.md - General contribution guidelines</li> <li>ostruct pricing automation - Detector and per-model pricing extraction templates used by CI</li> </ul>"},{"location":"contributing/cli-maintainers-guide/","title":"CLI Maintainers Guide","text":""},{"location":"contributing/cli-maintainers-guide/#cli-maintainers-guide","title":"CLI Maintainers Guide","text":"<p>This document is for contributors maintaining the <code>omr</code> CLI. It captures design goals, architecture, and maintainer-centric practices. User-facing usage and examples live in <code>docs/user-guide/cli.md</code> and the CLI's <code>--help</code>/<code>--help-json</code>.</p>"},{"location":"contributing/cli-maintainers-guide/#goals","title":"Goals","text":"<ul> <li>Keep CLI thin and stable; delegate logic to public library APIs</li> <li>Human-friendly tables and robust machine outputs (JSON/CSV/YAML)</li> <li>Read-only by default; explicit flags for destructive actions</li> <li>Cross-platform, non-interactive CI-friendly behavior</li> </ul>"},{"location":"contributing/cli-maintainers-guide/#architecture","title":"Architecture","text":"<ul> <li>Entry point: <code>omr</code> \u2192 <code>openai_model_registry.cli:app</code></li> <li>Frameworks: <code>click</code>, <code>rich-click</code>, <code>rich</code></li> <li>Module layout:</li> </ul> <pre><code>src/openai_model_registry/cli/\n\u251c\u2500\u2500 app.py                  # Root command group and global options\n\u251c\u2500\u2500 commands/\n\u2502   \u251c\u2500\u2500 data.py             # data paths/env/dump\n\u2502   \u251c\u2500\u2500 update.py           # update check/apply/refresh/show-config\n\u2502   \u251c\u2500\u2500 models.py           # models list/get\n\u2502   \u251c\u2500\u2500 providers.py        # providers list/current\n\u2502   \u2514\u2500\u2500 cache.py            # cache info/clear\n\u251c\u2500\u2500 formatters/\n\u2502   \u251c\u2500\u2500 table.py            # Rich tables\n\u2502   \u2514\u2500\u2500 json.py             # JSON/YAML/CSV\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 options.py          # Shared options\n    \u2514\u2500\u2500 helpers.py          # Helpers and exit codes\n</code></pre>"},{"location":"contributing/cli-maintainers-guide/#public-api-contract","title":"Public API contract","text":"<p>CLI uses only public APIs:</p> <ul> <li><code>ModelRegistry.get_default()</code> / <code>ModelRegistry()</code></li> <li><code>ModelRegistry.models</code></li> <li><code>ModelRegistry.get_capabilities(model)</code></li> <li><code>ModelRegistry.get_data_info()</code></li> <li><code>ModelRegistry.check_for_updates()</code> / <code>get_update_info()</code> / <code>update_data()</code> / <code>refresh_from_remote()</code></li> <li>Added for CLI ergonomics:</li> <li><code>list_providers()</code></li> <li><code>dump_effective()</code></li> <li><code>get_raw_data_paths()</code></li> <li><code>clear_cache()</code></li> <li><code>get_bundled_data_content(filename)</code></li> <li><code>get_raw_model_data(model_name)</code></li> </ul> <p>If any of these contracts change, update <code>docs/user-guide/cli.md</code> and <code>tests/test_cli.py</code> accordingly.</p>"},{"location":"contributing/cli-maintainers-guide/#output-conventions","title":"Output conventions","text":"<ul> <li>TTY default: tables; non-TTY default: JSON</li> <li><code>models get</code>/<code>data dump</code> only support JSON/YAML (fallback to JSON if table/csv requested)</li> <li>JSON outputs sorted for stability; custom serializer for dates/enums</li> </ul>"},{"location":"contributing/cli-maintainers-guide/#columns-and-filtering","title":"Columns and filtering","text":"<ul> <li><code>models list --columns</code> supports dotted paths; dynamic column discovery includes third-level keys (e.g., <code>billing.web_search.call_fee_per_1000</code>)</li> <li><code>--filter</code> supports simple expressions and case-insensitive AND</li> </ul>"},{"location":"contributing/cli-maintainers-guide/#provider-handling","title":"Provider handling","text":"<ul> <li>Precedence: <code>--provider</code> &gt; <code>OMR_PROVIDER</code> env &gt; default <code>openai</code></li> <li><code>providers current</code> surfaces provider and source (flag/env/default)</li> </ul>"},{"location":"contributing/cli-maintainers-guide/#testing","title":"Testing","text":"<ul> <li>Keep <code>tests/test_cli.py</code> updated for:</li> <li><code>--help-json</code> structure</li> <li>Provider resolution precedence</li> <li>Format fallbacks and CSV/JSON/YAML correctness</li> <li>Exit codes for update/check, model not found, invalid usage</li> <li><code>--parameters-only</code> for <code>models get</code></li> </ul>"},{"location":"contributing/cli-maintainers-guide/#pricing-automation-ostruct","title":"Pricing automation (ostruct)","text":"<ul> <li>See <code>scripts/ostruct/README.md</code> for detector/per-model templates and CI workflow</li> <li>In short: detector web-search step + dynamic per-model fetch + merge/normalize/validate/commit</li> </ul>"},{"location":"contributing/cli-maintainers-guide/#maintenance-checklist","title":"Maintenance checklist","text":"<ul> <li>When adding new model fields, ensure <code>dump_effective()</code> surfaces them and <code>formatters/table.py</code> handles display</li> <li>Keep <code>docs/user-guide/cli.md</code> aligned with CLI behaviors</li> <li>Run pre-commit hooks locally; keep mypy/ruff clean</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the OpenAI Model Registry user guide. This guide will help you get started with the library and explore its features.</p>"},{"location":"user-guide/#overview","title":"Overview","text":"<p>The OpenAI Model Registry provides:</p> <ul> <li>A centralized registry of OpenAI model information</li> <li>Automatic updates from GitHub releases with version tracking</li> <li>Validation for model parameters</li> <li>Access to model capabilities (context window, token limits, etc.)</li> <li>Version tracking and compatibility checks</li> <li>Robust fallback mechanisms for offline usage</li> </ul>"},{"location":"user-guide/#getting-started","title":"Getting Started","text":"<p>See the Getting Started guide to install and begin using the library.</p>"},{"location":"user-guide/#for-ai-assistants","title":"For AI Assistants","text":"<p>If you're an AI assistant or LLM helping users with this library, check out our <code>llms.txt</code> file which provides comprehensive, token-efficient documentation designed specifically for programmatic assistance.</p>"},{"location":"user-guide/#features","title":"Features","text":"<ul> <li>CLI Reference - Command-line interface for debugging and inspection</li> <li>Model Capabilities - Working with model capabilities and constraints</li> <li>Parameter Validation - Validating parameters against model-specific constraints</li> <li>Azure OpenAI Usage - Important considerations for Azure OpenAI endpoints</li> <li>Advanced Usage - Advanced features, data management, and configurations</li> <li>Testing - Testing patterns and best practices for applications using the registry</li> </ul>"},{"location":"user-guide/advanced-usage/","title":"Advanced Usage","text":"<p>This guide covers advanced features and configuration options for the OpenAI Model Registry.</p>"},{"location":"user-guide/advanced-usage/#custom-registry-configuration","title":"Custom Registry Configuration","text":"<p>The registry uses a modern data management system with automatic updates and fallback mechanisms. You can customize this with the <code>RegistryConfig</code> class:</p> <pre><code>from openai_model_registry import ModelRegistry, RegistryConfig\n\n# Create a custom configuration\nconfig = RegistryConfig(\n    registry_path=\"/path/to/custom/registry.yml\",  # Optional: custom registry path\n    constraints_path=\"/path/to/custom/constraints.yml\",  # Custom constraints path\n    auto_update=True,  # Enable automatic updates\n    cache_size=200,  # Increase cache size\n)\n\n# Initialize registry with the custom configuration\nregistry = ModelRegistry(config)\n\n# Use the registry\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n# Expected output: Successfully loads capabilities with custom configuration\n</code></pre> <p>The <code>RegistryConfig</code> class supports the following options:</p> <ul> <li><code>registry_path</code>: Custom path to the registry YAML file (if None, DataManager handles loading)</li> <li><code>constraints_path</code>: Custom path to the constraints YAML file</li> <li><code>auto_update</code>: Whether to automatically update the registry</li> <li><code>cache_size</code>: Size of the model capabilities cache</li> </ul>"},{"location":"user-guide/advanced-usage/#data-management-system","title":"Data Management System","text":"<p>The registry uses a modern DataManager that provides:</p> <ul> <li>Automatic Updates: Fetches latest model data from GitHub releases</li> <li>Version Tracking: Maintains version information and update history</li> <li>Fallback Mechanisms: Environment variable \u2192 User directory \u2192 Bundled data</li> <li>Data Validation: Comprehensive validation for downloaded data</li> </ul>"},{"location":"user-guide/advanced-usage/#environment-variables","title":"Environment Variables","text":"<p>Control data management behavior with these environment variables:</p> <pre><code># Disable automatic data updates (useful for CI/tests)\nexport OMR_DISABLE_DATA_UPDATES=1\n\n# Pin to a specific data version\nexport OMR_DATA_VERSION_PIN=v1.2.3\n\n# Use custom data directory\nexport OMR_DATA_DIR=/path/to/custom/data\n\n# Override registry path (for testing)\nexport OMR_MODEL_REGISTRY_PATH=/path/to/test/models.yaml\n</code></pre>"},{"location":"user-guide/advanced-usage/#data-update-api","title":"Data Update API","text":"<p>The registry provides methods for managing data updates:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\n# Check if updates are available\nif registry.check_data_updates():\n    print(\"Updates are available!\")\n\n    # Update the data\n    if registry.update_data():\n        print(\"Data updated successfully\")\n    else:\n        print(\"Update failed\")\n\n# Force update regardless of current version\nregistry.update_data(force=True)\n\n# Get current data version\nversion = registry.get_data_version()\nprint(f\"Current version: {version}\")\n\n# Get detailed data information\ninfo = registry.get_data_info()\nprint(f\"Data directory: {info['data_directory']}\")\nprint(f\"Updates enabled: {info['updates_enabled']}\")\n</code></pre>"},{"location":"user-guide/advanced-usage/#multiple-registry-instances","title":"Multiple Registry Instances","text":"<p>With the new API, you can create multiple registry instances with different configurations:</p> <pre><code>from openai_model_registry import ModelRegistry, RegistryConfig\n\n# Create registries for different environments\nprod_config = RegistryConfig(registry_path=\"/path/to/prod/registry.yml\")\nstaging_config = RegistryConfig(registry_path=\"/path/to/staging/registry.yml\")\n\nprod_registry = ModelRegistry(prod_config)\nstaging_registry = ModelRegistry(staging_config)\n\n# Use different registries as needed\nprod_capabilities = prod_registry.get_capabilities(\"gpt-4o\")\nstaging_capabilities = staging_registry.get_capabilities(\"gpt-4o\")\n# Expected output: Successfully loads capabilities from different registry instances\n</code></pre> <p>This is particularly useful for testing or when you need to support different configurations in the same application.</p>"},{"location":"user-guide/advanced-usage/#accessing-the-singleton","title":"Accessing the Singleton","text":"<p>Use <code>get_default()</code> to access the singleton instance.</p> <pre><code>registry = ModelRegistry.get_default()\n</code></pre>"},{"location":"user-guide/advanced-usage/#registry-updates","title":"Registry Updates","text":"<p>The registry supports both automatic and manual updates through the DataManager system:</p> <pre><code>from openai_model_registry import ModelRegistry\nfrom openai_model_registry.registry import RefreshStatus\n\n# Get registry instance\nregistry = ModelRegistry.get_default()\n\n# Check for updates using DataManager\nif registry.check_data_updates():\n    print(\"DataManager updates are available\")\n\n    # Update using DataManager\n    if registry.update_data():\n        print(\"Registry updated successfully via DataManager\")\n    else:\n        print(\"DataManager update failed\")\n\n# Legacy update method (also available)\ncheck_result = registry.check_for_updates()\nif check_result.status == RefreshStatus.UPDATE_AVAILABLE:\n    print(f\"Legacy update available: {check_result.message}\")\n\n    # Refresh from remote source\n    refresh_result = registry.refresh_from_remote()\n\n    if refresh_result.success:\n        print(\"Registry updated successfully\")\n    else:\n        print(f\"Update failed: {refresh_result.message}\")\nelif check_result.status == RefreshStatus.ALREADY_CURRENT:\n    print(\"Registry is already up to date\")\nelse:\n    print(f\"Update check failed: {check_result.message}\")\n</code></pre>"},{"location":"user-guide/advanced-usage/#programmatic-updates-recommended-pattern","title":"Programmatic updates (recommended pattern)","text":"<p>For the minimal snippet, see Getting Started \u2192 Keeping Data Up-to-Date. Below is the same pattern using the typed enum and intended for adaptation in advanced scenarios (scheduling, retries/backoff, metrics):</p> <pre><code>from openai_model_registry import ModelRegistry\nfrom openai_model_registry.registry import RefreshStatus\n\nregistry = ModelRegistry.get_default()\n\ntry:\n    result = registry.check_for_updates()\n    if result.status is RefreshStatus.UPDATE_AVAILABLE:\n        # Apply the update (writes to user data dir or OMR_DATA_DIR)\n        registry.update_data()\nexcept Exception:\n    # Never crash the application due to update issues\n    pass\n</code></pre> <p>Notes:</p> <ul> <li>The library automatically honors <code>OMR_DISABLE_DATA_UPDATES</code> and <code>OMR_DATA_VERSION_PIN</code>.</li> <li>No network calls occur during normal loads; network is used only on explicit update checks/applies.</li> <li><code>OMR_MODEL_REGISTRY_PATH</code> is a read\u2011only override and is never modified by updates.</li> </ul> <p>For advanced patterns (scheduling, retries/backoff, version pinning strategies, multi\u2011registry setups), adapt the above to your app\u2019s lifecycle (e.g., run on startup or on a cron/scheduler, add retry/backoff, emit metrics/logs on update checks).</p>"},{"location":"user-guide/advanced-usage/#data-loading-priority","title":"Data Loading Priority","text":"<p>The registry loads data with the following priority:</p> <ol> <li>Environment Variable: <code>OMR_MODEL_REGISTRY_PATH</code> (if set and file exists)</li> <li>User Data Directory: <code>~/Library/Application Support/openai-model-registry/models.yaml</code> (macOS)</li> <li>Bundled Data: Included with the package as fallback</li> </ol> <p>This ensures reliable operation even without network access while allowing customization for testing and development.</p>"},{"location":"user-guide/advanced-usage/#schema-versioning","title":"Schema Versioning","text":"<p>The registry uses semantic versioning for schema compatibility:</p> <pre><code># The registry automatically detects and validates schema versions\n# Current supported range: 1.x (&gt;=1.0.0, &lt;2.0.0)\n# Schema version is read from the 'version' field in data files\n\nregistry = ModelRegistry.get_default()\n\n# Works with any compatible schema version\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# All models include comprehensive metadata\nprint(f\"Status: {capabilities.deprecation.status}\")\nprint(f\"Context window: {capabilities.context_window:,}\")\nprint(f\"Supports vision: {capabilities.supports_vision}\")\n# Expected output: Status: active\n</code></pre>"},{"location":"user-guide/advanced-usage/#schema-version-detection","title":"Schema Version Detection","text":"<p>The registry uses proper semantic versioning:</p> <ul> <li>Version field: Schema version is read from the <code>version</code> field in configuration files</li> <li>Compatibility checking: Uses semver ranges (e.g., \"&gt;=1.0.0,\\&lt;2.0.0\" for 1.x support)</li> <li>Validation: Ensures data structure matches the declared schema version</li> <li>Error handling: Clear error messages for unsupported or invalid versions</li> </ul>"},{"location":"user-guide/advanced-usage/#model-data-accuracy","title":"Model Data Accuracy","text":"<p>The registry maintains accurate model information based on official OpenAI documentation:</p> <pre><code># Model release dates are accurate to OpenAI's official announcements\ncapabilities = registry.get_capabilities(\"gpt-4o-2024-05-13\")  # Correct release date\nprint(f\"Model: {capabilities.model_name}\")\n\n# Streaming capabilities reflect current API support\no1_mini = registry.get_capabilities(\"o1-mini\")\nprint(f\"O1-mini supports streaming: {o1_mini.supports_streaming}\")  # True\n\no1_latest = registry.get_capabilities(\"o1-2024-12-17\")\nprint(\n    f\"O1-2024-12-17 supports streaming: {o1_latest.supports_streaming}\"\n)  # False (not yet in public API)\n\n# Deprecation dates use null values for unknown timelines instead of placeholder dates\nprint(\n    f\"Deprecates on: {capabilities.deprecation.deprecates_on}\"\n)  # None for active models\n</code></pre>"},{"location":"user-guide/advanced-usage/#command-line-interface","title":"Command Line Interface","text":"<p>The package provides command-line interfaces for managing registry data:</p>"},{"location":"user-guide/advanced-usage/#data-management-cli-legacy-script","title":"Data Management CLI (Legacy Script)","text":"<pre><code># Check current data status\npython -m openai_model_registry.scripts.data_update check\n\n# Update data files\npython -m openai_model_registry.scripts.data_update update\n\n# Force update to latest version\npython -m openai_model_registry.scripts.data_update update --force\n\n# Show data configuration\npython -m openai_model_registry.scripts.data_update info\n\n# Clean local data files\npython -m openai_model_registry.scripts.data_update clean\n</code></pre>"},{"location":"user-guide/advanced-usage/#legacy-registry-update-cli-legacy-script","title":"Legacy Registry Update CLI (Legacy Script)","text":"<pre><code># Update the registry from the default source\nopenai-model-registry-update\n\n# Update with verbose output\nopenai-model-registry-update --verbose\n\n# Use a custom source URL\nopenai-model-registry-update --source https://custom-source.example/registry.json\n</code></pre>"},{"location":"user-guide/advanced-usage/#working-with-parameter-references","title":"Working with Parameter References","text":"<p>The registry uses parameter references to define relationships between parameters:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Get all parameter references\nfor param_ref in capabilities.supported_parameters:\n    print(f\"Parameter: {param_ref.ref}\")\n    if param_ref.max_value is not None:\n        print(f\"  Max value: {param_ref.max_value}\")\n    if param_ref.min_value is not None:\n        print(f\"  Min value: {param_ref.min_value}\")\n</code></pre>"},{"location":"user-guide/advanced-usage/#error-handling","title":"Error Handling","text":"<p>The registry provides comprehensive error handling for various scenarios:</p> <pre><code>from openai_model_registry import ModelRegistry\nfrom openai_model_registry.errors import (\n    ModelNotSupportedError,\n    ParameterValidationError,\n    ConfigurationError,\n)\n\nregistry = ModelRegistry.get_default()\n\ntry:\n    capabilities = registry.get_capabilities(\"non-existent-model\")\nexcept ModelNotSupportedError as e:\n    print(f\"Model not supported: {e}\")\n\ntry:\n    capabilities = registry.get_capabilities(\"gpt-4o\")\n    capabilities.validate_parameter(\"temperature\", 5.0)  # Invalid value\nexcept ParameterValidationError as e:\n    print(f\"Parameter validation failed: {e}\")\n\ntry:\n    # This might fail if data files are corrupted\n    registry._load_capabilities()\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"user-guide/advanced-usage/#error-hierarchy","title":"Error Hierarchy","text":"<ul> <li><code>ModelRegistryError</code>: Base class for all registry errors</li> <li><code>ConfigurationError</code>: Base class for configuration-related errors<ul> <li><code>ConfigFileNotFoundError</code>: Configuration file not found</li> <li><code>InvalidConfigFormatError</code>: Invalid configuration format</li> </ul> </li> <li><code>ModelVersionError</code>: Base class for version-related errors<ul> <li><code>InvalidDateError</code>: Invalid date format in a model version</li> <li><code>ModelFormatError</code>: Invalid model name format</li> <li><code>VersionTooOldError</code>: Model version is too old</li> </ul> </li> <li><code>ParameterValidationError</code>: Base class for parameter validation errors<ul> <li><code>ParameterNotSupportedError</code>: Parameter not supported for model</li> <li><code>TokenParameterError</code>: Token-related parameter error</li> </ul> </li> <li><code>ConstraintNotFoundError</code>: Constraint reference not found</li> <li><code>NetworkError</code>: Error during network operations</li> <li><code>ModelNotSupportedError</code>: Model not supported by registry</li> </ul>"},{"location":"user-guide/advanced-usage/#logging-configuration","title":"Logging Configuration","text":"<p>The library uses standard Python logging. You can configure it like any other Python logger:</p> <pre><code>import logging\nfrom openai_model_registry import get_logger\n\n# Configure the root logger\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n)\n\n# Get the library logger\nlogger = get_logger()\n\n# Add custom handlers if needed\nfile_handler = logging.FileHandler(\"registry.log\")\nfile_handler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n)\nlogger.addHandler(file_handler)\n</code></pre> <p>The package logger name is \"openai_model_registry\".</p>"},{"location":"user-guide/advanced-usage/#performance-optimization","title":"Performance Optimization","text":"<p>For applications that make frequent validation calls, consider caching capabilities:</p> <pre><code>from openai_model_registry import ModelRegistry\nimport functools\n\n\n# Create a cache of model capabilities\n@functools.lru_cache(maxsize=16)\ndef get_cached_capabilities(model_name):\n    registry = ModelRegistry.get_default()\n    return registry.get_capabilities(model_name)\n\n\n# Use cached capabilities\ncapabilities = get_cached_capabilities(\"gpt-4o\")\n</code></pre> <p>The registry itself uses LRU caching for capabilities, so repeated calls to <code>get_capabilities()</code> for the same model are automatically optimized.</p>"},{"location":"user-guide/advanced-usage/#data-files-and-provider-overrides","title":"Data files and provider overrides","text":"<p>The registry composes its effective dataset from two YAML files in <code>data/</code>:</p> <ul> <li><code>models.yaml</code>: Canonical base dataset for all models (capabilities, parameters,   pricing, deprecation, billing).</li> <li><code>overrides.yaml</code>: Provider-specific diffs applied on top of <code>models.yaml</code>.</li> </ul>"},{"location":"user-guide/advanced-usage/#provider-selection","title":"Provider selection","text":"<ul> <li>Default provider: <code>openai</code>.</li> <li>Override via environment <code>OMR_PROVIDER</code> or CLI flag <code>--provider &lt;openai|azure&gt;</code>.</li> </ul>"},{"location":"user-guide/advanced-usage/#structure-of-overridesyaml","title":"Structure of overrides.yaml","text":"<pre><code>overrides:\n  azure:\n    models:\n      gpt-4o:\n        pricing:\n          input_cost_per_unit: 5.0\n          output_cost_per_unit: 20.0\n        parameters:\n          max_tokens:\n            max: 12000\n        capabilities:\n          tools:\n            - file_search\n</code></pre> <ul> <li>Top-level key is <code>overrides</code>.</li> <li>Under each provider (e.g., <code>azure</code>), a <code>models</code> map contains partial model   entries. Only the fields you want to change need to be present.</li> <li>Unknown models under a provider are ignored; the base dataset remains intact.</li> </ul>"},{"location":"user-guide/advanced-usage/#merge-semantics","title":"Merge semantics","text":"<p>When building the effective dataset, the registry loads <code>models.yaml</code> and then applies provider overrides. Merge behavior mirrors the implementation in <code>ModelRegistry._apply_overrides()</code> and <code>_merge_model_override()</code>:</p> <ul> <li>pricing (dict): merged with base pricing via shallow update</li> <li>capabilities (dict): merged with base capabilities via shallow update</li> <li>parameters (dict): merged with base parameters via shallow update</li> <li>other top-level fields: replaced entirely</li> </ul> <p>Notes:</p> <ul> <li>Shallow updates mean nested dictionaries are updated key-by-key, but lists are   replaced as whole values. This keeps overrides concise and predictable.</li> <li>If no overrides exist for the selected provider, the base <code>models.yaml</code> data is   used as-is.</li> </ul>"},{"location":"user-guide/advanced-usage/#inspecting-raw-vs-effective-data","title":"Inspecting raw vs effective data","text":"<p>Use the CLI to compare the on-disk raw files with the provider-merged effective dataset:</p> <pre><code># Dump effective (merged) dataset\nomr data dump --effective --format json | jq '.'\n\n# Dump raw base dataset (no provider merge)\nomr data dump --raw --format yaml\n\n# Per-model views\nomr models get gpt-4o                # effective (default)\nomr models get gpt-4o --raw --format yaml\n</code></pre>"},{"location":"user-guide/advanced-usage/#where-updates-are-written","title":"Where updates are written","text":"<p>Data updates write <code>models.yaml</code> and <code>overrides.yaml</code> to the user data directory by default (or <code>OMR_DATA_DIR</code> if set). The <code>OMR_MODEL_REGISTRY_PATH</code> override is read-only and is never modified by updates.</p>"},{"location":"user-guide/azure-openai/","title":"Azure OpenAI Usage","text":"<p>This guide explains important considerations when using the OpenAI Model Registry with Azure OpenAI endpoints.</p>"},{"location":"user-guide/azure-openai/#overview","title":"Overview","text":"<p>Azure OpenAI provides access to OpenAI's models through Microsoft's cloud infrastructure, but there are important differences in feature support compared to OpenAI's direct API. The most significant difference relates to web search capabilities.</p> <p>The OpenAI Model Registry includes built-in provider support to handle these differences automatically. You can use <code>--provider azure</code> or set <code>OMR_PROVIDER=azure</code> to get Azure-specific model configurations that reflect platform limitations.</p>"},{"location":"user-guide/azure-openai/#web-search-limitations-on-azure","title":"Web Search Limitations on Azure","text":""},{"location":"user-guide/azure-openai/#standard-azure-openai-apis-chat-responses","title":"Standard Azure OpenAI APIs (Chat &amp; Responses)","text":"<p>\u26a0\ufe0f Important: Standard Azure OpenAI Chat Completions and Responses APIs do not support the <code>web_search_preview</code> tool, even for models that our registry indicates have <code>supports_web_search: true</code>.</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# This will return True (the model itself supports web search)\nprint(f\"Model supports web search: {capabilities.supports_web_search}\")\n# Output: Model supports web search: True\n\n# However, if you're using Azure OpenAI standard endpoints:\n# - *.openai.azure.com/openai/deployments/.../chat/completions\n# - *.openai.azure.com/openai/deployments/.../responses\n# Using web_search_preview tool will FAIL with an error\n</code></pre>"},{"location":"user-guide/azure-openai/#why-this-limitation-exists","title":"Why This Limitation Exists","text":"<p>Microsoft's Azure OpenAI documentation explicitly states that the <code>web_search_preview</code> tool is \"not currently supported\" in standard Azure endpoints. This is a platform-level limitation, not a model limitation.</p> <p>The OpenAI Model Registry reflects the underlying model capabilities as defined by OpenAI. When these models are deployed through Azure's infrastructure, certain features may be restricted or unavailable.</p>"},{"location":"user-guide/azure-openai/#recommended-approach-for-azure-users","title":"Recommended Approach for Azure Users","text":""},{"location":"user-guide/azure-openai/#1-use-the-built-in-provider-system-recommended","title":"1. Use the Built-in Provider System (Recommended)","text":"<p>The simplest approach is to use the registry's built-in Azure provider:</p> <pre><code># Set Azure provider globally\nexport OMR_PROVIDER=azure\n\n# Or use CLI flag\nomr --provider azure models get gpt-4o\n</code></pre> <pre><code>import os\nfrom openai_model_registry import ModelRegistry\n\n# Set provider via environment\nos.environ[\"OMR_PROVIDER\"] = \"azure\"\nregistry = ModelRegistry.get_default()\n\n# Get Azure-adjusted capabilities\ncapabilities = registry.get_capabilities(\"gpt-4o\")\nprint(f\"Web search support (Azure-adjusted): {capabilities.supports_web_search}\")\n# This will reflect Azure limitations automatically\n</code></pre>"},{"location":"user-guide/azure-openai/#2-manual-endpoint-detection-alternative","title":"2. Manual Endpoint Detection (Alternative)","text":"<p>Before using web search capabilities, determine if you're using Azure OpenAI:</p> <pre><code>import openai\nfrom openai_model_registry import ModelRegistry\n\n# Initialize your OpenAI client (however you normally do it)\nclient = openai.OpenAI(\n    api_key=\"your-azure-api-key\",\n    base_url=\"https://your-resource.openai.azure.com/openai/deployments/your-deployment\",\n)\n\n\n# Check if you're using Azure\ndef is_azure_endpoint(client):\n    \"\"\"Check if the OpenAI client is configured for Azure.\"\"\"\n    base_url = str(client.base_url) if client.base_url else \"\"\n    return \"openai.azure.com\" in base_url\n\n\n# Get model capabilities with Azure consideration\ndef get_web_search_capability(model_name, client):\n    \"\"\"Get accurate web search capability considering Azure limitations.\"\"\"\n    registry = ModelRegistry.get_default()\n    capabilities = registry.get_capabilities(model_name)\n\n    # Model supports web search, but check if endpoint allows it\n    model_supports_search = capabilities.supports_web_search\n    endpoint_supports_search = not is_azure_endpoint(client)\n\n    return model_supports_search and endpoint_supports_search\n\n\n# Usage\nregistry = ModelRegistry.get_default()\ncan_use_web_search = get_web_search_capability(\"gpt-4o\", client)\n\nif can_use_web_search:\n    print(\"\u2705 You can use web search with this model and endpoint\")\n    # Proceed with web_search_preview tool\nelse:\n    print(\"\u274c Web search not available (check model support and endpoint type)\")\n    # Use alternative approach or skip web search\n</code></pre>"},{"location":"user-guide/azure-openai/#2-alternative-approaches-for-azure-users","title":"2. Alternative Approaches for Azure Users","text":"<p>If you need web search functionality with Azure OpenAI, consider these options:</p>"},{"location":"user-guide/azure-openai/#option-a-use-azure-assistants-api-preview","title":"Option A: Use Azure Assistants API (Preview)","text":"<p>Azure's newer Assistants API includes a \"Browse\" tool that provides web search via Bing:</p> <pre><code># Note: This requires Azure Assistants API, not standard Chat/Responses API\n# The Browse tool must be enabled in Azure AI Studio\n\n# Example Azure Assistant with Browse tool (pseudo-code)\n# This is different from standard chat completions\nassistant_client = AzureAssistantsClient(...)\nassistant = assistant_client.create_assistant(\n    model=\"gpt-4o\",\n    tools=[{\"type\": \"browse\"}],  # Azure's Browse tool, not web_search_preview\n)\n</code></pre>"},{"location":"user-guide/azure-openai/#option-b-external-search-integration","title":"Option B: External Search Integration","text":"<p>Implement your own search integration using Bing Search API or other services:</p> <pre><code>from openai_model_registry import ModelRegistry\n\n\ndef enhanced_azure_chat(client, model, messages, use_search=False):\n    \"\"\"Enhanced chat function with optional external search for Azure.\"\"\"\n    registry = ModelRegistry.get_default()\n    capabilities = registry.get_capabilities(model)\n\n    # Check if this is an Azure endpoint\n    if is_azure_endpoint(client) and use_search:\n        # Use external search API (Bing, Google, etc.)\n        search_results = perform_external_search(messages[-1][\"content\"])\n\n        # Enhance the prompt with search results\n        enhanced_message = {\n            \"role\": \"user\",\n            \"content\": f\"{messages[-1]['content']}\\n\\nSearch results: {search_results}\",\n        }\n        messages = messages[:-1] + [enhanced_message]\n\n    # Make standard chat completion call (no web_search_preview tool)\n    return client.chat.completions.create(\n        model=model,\n        messages=messages,\n        # Note: No tools parameter for Azure standard endpoints\n    )\n</code></pre>"},{"location":"user-guide/azure-openai/#platform-specific-capability-detection","title":"Platform-Specific Capability Detection","text":"<p>For applications that need to work across both OpenAI direct and Azure endpoints:</p> <pre><code>from openai_model_registry import ModelRegistry\nfrom typing import Dict, Any\n\n\nclass PlatformAwareCapabilities:\n    \"\"\"Wrapper that provides platform-aware capability checking.\"\"\"\n\n    def __init__(self, client):\n        self.registry = ModelRegistry.get_default()\n        self.client = client\n        self.is_azure = self._detect_azure()\n\n    def _detect_azure(self) -&gt; bool:\n        \"\"\"Detect if client is using Azure endpoint.\"\"\"\n        base_url = str(self.client.base_url) if self.client.base_url else \"\"\n        return \"openai.azure.com\" in base_url\n\n    def get_effective_capabilities(self, model_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Get capabilities adjusted for the current platform.\"\"\"\n        capabilities = self.registry.get_capabilities(model_name)\n\n        # Create a dictionary of effective capabilities\n        effective_caps = {\n            \"model_name\": capabilities.model_name,\n            \"context_window\": capabilities.context_window,\n            \"max_output_tokens\": capabilities.max_output_tokens,\n            \"supports_structured\": capabilities.supports_structured,\n            \"supports_streaming\": capabilities.supports_streaming,\n            # Adjust web search based on platform\n            \"supports_web_search\": capabilities.supports_web_search\n            and not self.is_azure,\n            \"platform\": \"azure\" if self.is_azure else \"openai_direct\",\n        }\n\n        return effective_caps\n\n\n# Usage\nplatform_caps = PlatformAwareCapabilities(client)\neffective_capabilities = platform_caps.get_effective_capabilities(\"gpt-4o\")\n\nprint(f\"Platform: {effective_capabilities['platform']}\")\nprint(f\"Effective web search support: {effective_capabilities['supports_web_search']}\")\n</code></pre>"},{"location":"user-guide/azure-openai/#best-practices-for-azure-users","title":"Best Practices for Azure Users","text":""},{"location":"user-guide/azure-openai/#1-always-check-platform-before-using-web-search","title":"1. Always Check Platform Before Using Web Search","text":"<pre><code># \u2705 Good: Check platform first\nif not is_azure_endpoint(client) and capabilities.supports_web_search:\n    # Use web_search_preview tool\n    tools = [{\"type\": \"web_search_preview\"}]\nelse:\n    # Skip web search or use alternatives\n    tools = []\n\nresponse = client.chat.completions.create(model=model, messages=messages, tools=tools)\n</code></pre> <pre><code># \u274c Bad: Using registry info without platform consideration\ncapabilities = registry.get_capabilities(\"gpt-4o\")\nif capabilities.supports_web_search:\n    # This will fail on Azure standard endpoints!\n    tools = [{\"type\": \"web_search_preview\"}]\n</code></pre>"},{"location":"user-guide/azure-openai/#2-graceful-fallback-implementation","title":"2. Graceful Fallback Implementation","text":"<pre><code>def robust_chat_completion(client, model, messages, prefer_web_search=False):\n    \"\"\"Chat completion with robust handling of web search availability.\"\"\"\n    registry = ModelRegistry.get_default()\n    capabilities = registry.get_capabilities(model)\n\n    # Determine if web search is actually available\n    web_search_available = capabilities.supports_web_search and not is_azure_endpoint(\n        client\n    )\n\n    tools = []\n    if prefer_web_search and web_search_available:\n        tools.append({\"type\": \"web_search_preview\"})\n\n    try:\n        return client.chat.completions.create(\n            model=model, messages=messages, tools=tools\n        )\n    except Exception as e:\n        if \"web_search\" in str(e).lower() and tools:\n            # Fallback: retry without web search\n            print(\"Web search failed, retrying without search...\")\n            return client.chat.completions.create(\n                model=model,\n                messages=messages,\n                # No tools parameter\n            )\n        raise  # Re-raise if it's a different error\n</code></pre>"},{"location":"user-guide/azure-openai/#3-environment-specific-configuration","title":"3. Environment-Specific Configuration","text":"<pre><code>import os\nfrom openai_model_registry import ModelRegistry\n\n\ndef get_search_strategy():\n    \"\"\"Get the appropriate search strategy based on environment.\"\"\"\n    if os.getenv(\"OPENAI_API_TYPE\") == \"azure\":\n        return \"external_search\"  # Use Bing API or other external search\n    elif os.getenv(\"AZURE_ASSISTANTS_ENABLED\") == \"true\":\n        return \"azure_browse_tool\"  # Use Azure Assistants Browse tool\n    else:\n        return \"openai_web_search\"  # Use OpenAI's web_search_preview\n\n\ndef create_search_enabled_request(model, messages):\n    \"\"\"Create request with appropriate search strategy.\"\"\"\n    registry = ModelRegistry.get_default()\n    capabilities = registry.get_capabilities(model)\n    strategy = get_search_strategy()\n\n    if not capabilities.supports_web_search:\n        strategy = \"no_search\"\n\n    if strategy == \"openai_web_search\":\n        return {\n            \"model\": model,\n            \"messages\": messages,\n            \"tools\": [{\"type\": \"web_search_preview\"}],\n        }\n    elif strategy == \"external_search\":\n        # Enhance messages with external search results\n        enhanced_messages = add_external_search_context(messages)\n        return {\"model\": model, \"messages\": enhanced_messages}\n    # ... handle other strategies\n</code></pre>"},{"location":"user-guide/azure-openai/#summary","title":"Summary","text":"<ul> <li>Use <code>--provider azure</code> or <code>OMR_PROVIDER=azure</code> for automatic Azure-specific configurations</li> <li>The registry's provider system handles platform differences automatically</li> <li>Azure's standard endpoints don't support <code>web_search_preview</code> regardless of model capabilities</li> <li>Provider overrides in <code>overrides.yaml</code> adjust capabilities for Azure limitations</li> <li>Consider Azure Assistants API for web search functionality on Azure</li> <li>Implement graceful fallbacks for robust cross-platform applications</li> </ul>"},{"location":"user-guide/azure-openai/#provider-system-details","title":"Provider System Details","text":"<p>The registry uses <code>data/overrides.yaml</code> to apply Azure-specific adjustments:</p> <pre><code>overrides:\n  azure:\n    models:\n      gpt-4o:\n        capabilities:\n          supports_web_search: false  # Disabled for Azure standard endpoints\n        pricing:\n          input_cost_per_unit: 5.0    # Azure-specific pricing\n</code></pre> <p>For more details on the provider system, see Advanced Usage \u2192 Data files and provider overrides.</p> <p>For the most up-to-date information on Azure OpenAI feature support, consult Microsoft's Azure OpenAI documentation.</p>"},{"location":"user-guide/cli/","title":"OMR CLI Reference","text":"<p>The OpenAI Model Registry CLI (<code>omr</code>) provides a powerful command-line interface for inspecting, debugging, and managing your model registry data.</p>"},{"location":"user-guide/cli/#installation","title":"Installation","text":"<p>The CLI requires the optional <code>[cli]</code> extra dependencies:</p> <pre><code>pip install openai-model-registry[cli]\n</code></pre> <p>After installation, the <code>omr</code> command should be available in your shell.</p> <p>Note: The core library (<code>pip install openai-model-registry</code>) does not include CLI dependencies. You must install the <code>[cli]</code> extra to use the <code>omr</code> command.</p>"},{"location":"user-guide/cli/#quick-start","title":"Quick Start","text":"<pre><code># Show help\nomr --help\n\n# List all models\nomr models list\n\n# Show data source paths\nomr data paths\n\n# Check for updates\nomr update check\n\n# Clear cache\nomr cache clear --yes\n\n# Programmatic model-card access examples\n# Parameters-only view\nomr --format json models get gpt-4o --parameters-only | jq '.'\n\n# Inspect web-search billing block\nomr --format json models get gpt-4o | jq '.billing.web_search'\n\n# Inspect input/output modalities\nomr --format json models get gpt-4o | jq '.input_modalities, .output_modalities'\n\n# View per-image pricing tiers (e.g., DALL\u00b7E)\nomr --format json models get dall-e-3 | jq '.pricing.tiers'\n</code></pre>"},{"location":"user-guide/cli/#global-options","title":"Global Options","text":"<p>These options can be used with any command:</p> <ul> <li><code>--provider &lt;openai|azure&gt;</code>: Override the active provider (takes precedence over <code>OMR_PROVIDER</code> environment variable)</li> <li><code>--format &lt;table|json|csv|yaml&gt;</code>: Output format (defaults to <code>table</code> for TTY, <code>json</code> for non-TTY)</li> <li>Note: Not all commands support all formats - see individual command documentation</li> <li><code>--verbose</code>, <code>-v</code>: Increase verbosity (stackable: <code>-vv</code> for more verbose)</li> <li><code>--quiet</code>, <code>-q</code>: Decrease verbosity (stackable)</li> <li><code>--debug</code>: Enable debug-level logging</li> <li><code>--no-color</code>: Disable color output</li> <li><code>--version</code>: Show version information</li> <li><code>--help</code>: Show help information</li> <li><code>--help-json</code>: Show help in JSON format for programmatic use</li> </ul>"},{"location":"user-guide/cli/#commands","title":"Commands","text":""},{"location":"user-guide/cli/#data-commands-omr-data","title":"Data Commands (<code>omr data</code>)","text":"<p>Inspect data sources and configuration.</p>"},{"location":"user-guide/cli/#omr-data-paths","title":"<code>omr data paths</code>","text":"<p>Show resolved data source paths and their precedence:</p> <pre><code>omr data paths\nomr data paths --format json\n</code></pre>"},{"location":"user-guide/cli/#omr-data-env","title":"<code>omr data env</code>","text":"<p>Show effective OMR environment variables:</p> <pre><code>omr data env\nomr data env --format json\n</code></pre>"},{"location":"user-guide/cli/#omr-data-dump","title":"<code>omr data dump</code>","text":"<p>Dump registry data in various formats:</p> <pre><code># Dump effective (merged) data as JSON\nomr data dump --effective\n\n# Dump raw data as YAML\nomr data dump --raw --format yaml\n\n# Save to file\nomr data dump --effective --output effective.json\n</code></pre> <p>Supported formats: <code>json</code>, <code>yaml</code> (on TTY, requesting <code>table</code>/<code>csv</code> falls back to JSON; use <code>--format yaml</code> to force YAML)</p> <p>Options:</p> <ul> <li><code>--raw</code>: Dump original on-disk/bundled YAML (no provider merge)</li> <li><code>--effective</code>: Dump fully merged, provider-adjusted dataset (default)</li> <li><code>--output FILE</code>: Write to file instead of stdout</li> </ul> <p>See also: Data files and merge behavior in <code>Advanced Usage \u2192 Data files and provider overrides</code>.</p>"},{"location":"user-guide/cli/#update-commands-omr-update","title":"Update Commands (<code>omr update</code>)","text":"<p>Manage registry data updates.</p>"},{"location":"user-guide/cli/#omr-update-check","title":"<code>omr update check</code>","text":"<p>Check for available updates:</p> <pre><code>omr update check\nomr update check --format json\n</code></pre> <p>Exit codes:</p> <ul> <li><code>0</code>: Up to date</li> <li><code>10</code>: Update available (CI-friendly)</li> </ul>"},{"location":"user-guide/cli/#omr-update-apply","title":"<code>omr update apply</code>","text":"<p>Apply available updates:</p> <pre><code>omr update apply\nomr update apply --force\n</code></pre> <p>Options:</p> <ul> <li><code>--force</code>: Force update even if current version is newer</li> <li><code>--url URL</code>: Override update URL</li> </ul> <p>Note: This command writes updated data files (e.g., <code>models.yaml</code>, <code>overrides.yaml</code>) to the user data directory by default. If <code>OMR_DATA_DIR</code> is set, that directory is used. The <code>OMR_MODEL_REGISTRY_PATH</code> override is read-only and is never modified by updates.</p>"},{"location":"user-guide/cli/#omr-update-refresh","title":"<code>omr update refresh</code>","text":"<p>One-shot validate/check/apply wrapper:</p> <pre><code>omr update refresh\nomr update refresh --validate-only\nomr update refresh --force\n</code></pre> <p>Options:</p> <ul> <li><code>--validate-only</code>: Only validate remote data without applying</li> <li><code>--force</code>: Force refresh even if current version is newer</li> <li><code>--url URL</code>: Override update URL</li> </ul> <p>Note: When applying updates (without <code>--validate-only</code>), files are written to the user data directory (or <code>OMR_DATA_DIR</code> if set). <code>OMR_MODEL_REGISTRY_PATH</code> is only used for reading <code>models.yaml</code> and is not modified.</p>"},{"location":"user-guide/cli/#omr-update-show-config","title":"<code>omr update show-config</code>","text":"<p>Show effective update-related configuration:</p> <pre><code>omr update show-config\n</code></pre>"},{"location":"user-guide/cli/#model-commands-omr-models","title":"Model Commands (<code>omr models</code>)","text":"<p>Inspect and list models.</p>"},{"location":"user-guide/cli/#omr-models-list","title":"<code>omr models list</code>","text":"<p>List all available models:</p> <pre><code># Basic list\nomr models list\n\n# JSON format\nomr --format json models list\n\n# CSV format with custom columns (including billing)\nomr models list --format csv --columns \"name,pricing.input_cost_per_unit,pricing.unit,billing.web_search.call_fee_per_1000\"\n\n# Filter models\nomr models list --filter \"gpt-4\"\n</code></pre> <p>Options:</p> <ul> <li><code>--filter EXPR</code>: Filter models using simple expression</li> <li><code>--columns COLS</code>: Comma-separated columns to display (supports dotted paths)</li> </ul>"},{"location":"user-guide/cli/#omr-models-get","title":"<code>omr models get</code>","text":"<p>Get detailed information about a specific model:</p> <pre><code># Get effective model data (default)\nomr models get gpt-4o\n\n# Get raw model data as YAML\nomr models get gpt-4o --raw --format yaml\n\n# Parameters only (effective)\nomr --format json models get gpt-4o --parameters-only\n\n# Save to file\nomr models get gpt-4o --output gpt-4o.json\n</code></pre> <p>Supported formats: <code>json</code>, <code>yaml</code> (falls back to JSON for <code>table</code>/<code>csv</code> with verbose message)</p> <p>Options:</p> <ul> <li><code>--effective</code>: Show effective model data with provider overrides (default)</li> <li><code>--raw</code>: Show raw model data without provider overrides</li> <li><code>--output FILE</code>: Write to file instead of stdout</li> </ul> <p>For how provider overrides are applied, see <code>Advanced Usage \u2192 Data files and provider overrides</code>.</p> <p>Tip: To view web-search billing for a model in JSON:</p> <pre><code>omr --format json models get gpt-4o | jq '.billing.web_search'\n</code></pre>"},{"location":"user-guide/cli/#provider-commands-omr-providers","title":"Provider Commands (<code>omr providers</code>)","text":"<p>Manage and inspect providers.</p>"},{"location":"user-guide/cli/#omr-providers-list","title":"<code>omr providers list</code>","text":"<p>List all available providers:</p> <pre><code>omr providers list\nomr --format json providers list\n</code></pre>"},{"location":"user-guide/cli/#omr-providers-current","title":"<code>omr providers current</code>","text":"<p>Show the currently active provider and its source:</p> <pre><code>omr providers current\n</code></pre>"},{"location":"user-guide/cli/#cache-commands-omr-cache","title":"Cache Commands (<code>omr cache</code>)","text":"<p>Manage registry cache.</p>"},{"location":"user-guide/cli/#omr-cache-info","title":"<code>omr cache info</code>","text":"<p>Show cache directory and file information:</p> <pre><code>omr cache info\nomr --format json cache info\n</code></pre>"},{"location":"user-guide/cli/#omr-cache-clear","title":"<code>omr cache clear</code>","text":"<p>Clear cached registry data files:</p> <pre><code># Interactive confirmation\nomr cache clear\n\n# Non-interactive (required for scripts)\nomr cache clear --yes\n</code></pre> <p>Warning: This removes cached <code>models.yaml</code> and <code>overrides.yaml</code> files. The registry will fall back to bundled data until the next update.</p>"},{"location":"user-guide/cli/#environment-variables","title":"Environment Variables","text":"<p>The CLI respects these environment variables (all prefixed with <code>OMR_</code>):</p> <ul> <li><code>OMR_PROVIDER</code>: Default provider (<code>openai</code> or <code>azure</code>)</li> <li><code>OMR_DATA_DIR</code>: Custom data directory</li> <li><code>OMR_DISABLE_DATA_UPDATES</code>: Disable automatic updates (<code>true</code>/<code>false</code>)</li> <li><code>OMR_DATA_VERSION_PIN</code>: Pin to specific data version</li> <li><code>OMR_MODEL_REGISTRY_PATH</code>: Override models.yaml path</li> <li><code>OMR_PARAMETER_CONSTRAINTS_PATH</code>: Override constraints path</li> </ul> <p>Note: Pricing updates are fetched and merged via CI using ostruct. When running locally, you can execute the same workflow via our helper scripts or by invoking <code>ostruct</code> with <code>pipx</code>.</p>"},{"location":"user-guide/cli/#output-formats","title":"Output Formats","text":"<p>Format Support by Command:</p> <ul> <li>Full support (table, json, csv, yaml): <code>models list</code>, <code>providers list</code>, <code>providers current</code>, <code>data paths</code>, <code>data env</code>, <code>cache info</code>, <code>update</code> commands</li> <li>JSON/YAML only: <code>data dump</code>, <code>models get</code> (automatically falls back to JSON for table/csv with verbose message)</li> </ul>"},{"location":"user-guide/cli/#table-format-default-for-tty","title":"Table Format (Default for TTY)","text":"<p>Human-readable tables with colors and formatting:</p> <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Model     \u2503 Context Window \u2503 Input Cost  \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 gpt-4o    \u2502 128000         \u2502 $2.5        \u2502\n\u2502 gpt-4o-mini\u2502 128000        \u2502 $0.15       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Note on narrow terminals:</p> <ul> <li>Tables render to your current terminal width. If the console isn't wide   enough to fit all columns, cell contents may be truncated on the right.</li> <li>Two-line headers (e.g., \"Context\\nWindow\") will still display fully, but   their corresponding column values may be shortened to fit.</li> <li>To see more:</li> <li>Reduce columns: <code>omr models list --columns name,context_window.total</code></li> <li>Use a wider terminal or a pager without wrapping: <code>omr models list | less -S</code></li> <li>Switch to machine-friendly formats: <code>--format json</code> or <code>--format yaml</code></li> </ul>"},{"location":"user-guide/cli/#json-format-default-for-non-tty","title":"JSON Format (Default for non-TTY)","text":"<p>Machine-readable JSON with stable keys:</p> <pre><code>{\n  \"models\": [\n    {\n      \"name\": \"gpt-4o\",\n      \"context_window\": {\n        \"total\": 128000,\n        \"output\": 16384\n      },\n      \"pricing\": {\n        \"scheme\": \"per_token\",\n        \"unit\": \"million_tokens\",\n        \"input_cost_per_unit\": 2.5,\n        \"output_cost_per_unit\": 10.0\n      }\n    }\n  ],\n  \"count\": 1\n}\n</code></pre>"},{"location":"user-guide/cli/#csv-format","title":"CSV Format","text":"<p>Comma-separated values for spreadsheet import:</p> <pre><code>name,context_window.total,pricing.input_cost_per_unit,pricing.unit\ngpt-4o,128000,2.5,million_tokens\ngpt-4o-mini,128000,0.15,million_tokens\n</code></pre>"},{"location":"user-guide/cli/#yaml-format","title":"YAML Format","text":"<p>Human-readable YAML:</p> <pre><code>provider: openai\nmodels:\n  gpt-4o:\n    context_window:\n      total: 128000\n      output: 16384\n</code></pre>"},{"location":"user-guide/cli/#common-workflows","title":"Common Workflows","text":""},{"location":"user-guide/cli/#development-and-debugging","title":"Development and Debugging","text":"<pre><code># Check current configuration\nomr data env\nomr data paths\n\n# List available models with pricing\nomr models list --columns \"name,pricing.input_cost_per_unit,pricing.output_cost_per_unit,pricing.unit\"\n\n# Get detailed model info\nomr models get gpt-4o --effective\n\n# Switch provider and compare\nomr --provider azure models get gpt-4o --effective\n</code></pre>"},{"location":"user-guide/cli/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># Check for updates (exit code 0 = up to date, 10 = update available)\nomr update check --format json\n\n# Apply updates in CI\nomr update apply --force\n\n# Validate data without applying\nomr update refresh --validate-only\n</code></pre>"},{"location":"user-guide/cli/#data-management","title":"Data Management","text":"<pre><code># Export current effective data\nomr data dump --effective --output backup.json\n\n# Clear cache and start fresh\nomr cache clear --yes\nomr update apply\n\n# Check cache status\nomr cache info\n</code></pre>"},{"location":"user-guide/cli/#provider-comparison","title":"Provider Comparison","text":"<pre><code># Compare pricing between providers\nomr --provider openai models list --format json &gt; openai.json\nomr --provider azure models list --format json &gt; azure.json\n\n# Check which providers are available\nomr providers list\n</code></pre>"},{"location":"user-guide/cli/#exit-codes","title":"Exit Codes","text":"<p>The CLI uses standard exit codes for CI/CD integration:</p> <ul> <li><code>0</code>: Success</li> <li><code>1</code>: Generic error</li> <li><code>2</code>: Invalid usage/arguments</li> <li><code>3</code>: Model not found</li> <li><code>4</code>: Data source missing/corrupt</li> <li><code>10</code>: Update available (used by <code>omr update check</code>)</li> </ul>"},{"location":"user-guide/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/cli/#command-not-found","title":"Command Not Found","text":"<p>If <code>omr</code> command is not found after installation:</p> <pre><code># Check if it's installed\npip show openai-model-registry\n\n# Try using the module directly\npython -m openai_model_registry.cli --help\n</code></pre>"},{"location":"user-guide/cli/#no-models-listed","title":"No Models Listed","text":"<p>If <code>omr models list</code> shows no models:</p> <pre><code># Check data sources\nomr data paths\n\n# Check for data loading errors\nomr --debug models list\n\n# Try updating data\nomr update apply\n</code></pre>"},{"location":"user-guide/cli/#permission-errors","title":"Permission Errors","text":"<p>If you get permission errors with cache operations:</p> <pre><code># Check cache directory permissions\nomr cache info\n\n# Clear cache with appropriate permissions\nsudo omr cache clear --yes\n</code></pre>"},{"location":"user-guide/cli/#provider-issues","title":"Provider Issues","text":"<p>If provider switching doesn't work:</p> <pre><code># Check current provider\nomr providers current\n\n# Check environment\nomr data env\n\n# Set provider explicitly\nexport OMR_PROVIDER=azure\nomr providers current\n</code></pre>"},{"location":"user-guide/getting-started/","title":"Getting Started","text":"<p>This guide will help you install the OpenAI Model Registry and start using its features.</p>"},{"location":"user-guide/getting-started/#installation","title":"Installation","text":""},{"location":"user-guide/getting-started/#core-library","title":"Core Library","text":"<p>Install the OpenAI Model Registry package using pip:</p> <pre><code>pip install openai-model-registry\n</code></pre>"},{"location":"user-guide/getting-started/#with-cli-tools","title":"With CLI Tools","text":"<p>If you want to use the <code>omr</code> command-line interface, install with the CLI extra:</p> <pre><code>pip install openai-model-registry[cli]\n</code></pre> <p>\ud83d\udca1 Which installation should I choose? - Core only: Perfect for programmatic use in applications, scripts, or libraries - With CLI: Adds command-line tools for interactive exploration and debugging</p>"},{"location":"user-guide/getting-started/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example to get started:</p> <pre><code>from openai_model_registry import ModelRegistry\n\n# Get the registry instance (singleton)\nregistry = ModelRegistry.get_default()\n\n# Get capabilities for a specific model\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Access model information\nprint(f\"Context window: {capabilities.context_window}\")\nprint(f\"Max output tokens: {capabilities.max_output_tokens}\")\nprint(f\"Supports streaming: {capabilities.supports_streaming}\")\nprint(f\"Supports structured output: {capabilities.supports_structured}\")\n</code></pre>"},{"location":"user-guide/getting-started/#model-validation","title":"Model Validation","text":"<p>You can validate parameters against a model's constraints:</p> <pre><code>try:\n    # Valid parameter\n    capabilities.validate_parameter(\"temperature\", 0.7)\n    print(\"Temperature 0.7 is valid\")\n\n    # Invalid parameter\n    capabilities.validate_parameter(\"temperature\", 3.0)\n    print(\"This won't be reached\")\nexcept Exception as e:\n    print(f\"Invalid parameter: {e}\")\n</code></pre>"},{"location":"user-guide/getting-started/#keeping-data-up-to-date","title":"Keeping Data Up-to-Date","text":"<p>You can check and apply data updates programmatically or via the CLI.</p> <p>Programmatically:</p> <pre><code>from openai_model_registry import ModelRegistry\nfrom openai_model_registry.registry import RefreshStatus\n\nregistry = ModelRegistry.get_default()\n\nresult = registry.check_for_updates()\nif result.status is RefreshStatus.UPDATE_AVAILABLE:\n    registry.update_data()\n</code></pre> <p>Via CLI:</p> <pre><code># Check for updates (exit code 10 if update is available)\nomr --format json update check\n\n# Apply updates\nomr update apply\n</code></pre> <p>The library honors <code>OMR_DISABLE_DATA_UPDATES</code> and <code>OMR_DATA_VERSION_PIN</code>. Updates write to the user data directory (or <code>OMR_DATA_DIR</code>), never to <code>OMR_MODEL_REGISTRY_PATH</code>.</p>"},{"location":"user-guide/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you have the basics, explore the following topics:</p> <ul> <li>Model Capabilities - Learn about model capabilities and how to use them</li> <li>Parameter Validation - Deep dive into parameter validation</li> <li>Advanced Usage - Explore advanced features like custom configurations and registry updates</li> </ul>"},{"location":"user-guide/model-capabilities/","title":"Model Capabilities","text":"<p>This guide explains how to work with model capabilities in the OpenAI Model Registry.</p>"},{"location":"user-guide/model-capabilities/#what-are-model-capabilities","title":"What are Model Capabilities?","text":"<p>Model capabilities represent the features, limitations, and parameters supported by a specific model. These include:</p> <ul> <li>Context window size</li> <li>Maximum output tokens</li> <li>Support for streaming</li> <li>Support for structured output</li> <li>Support for web search</li> <li>Input and output modalities (e.g., <code>input_modalities: [text, image]</code>, <code>output_modalities: [text]</code>)</li> <li>Supported parameters and their constraints</li> </ul> <p>Note on Naming Conventions:</p> <ul> <li><code>gpt-4</code> \u2192 resolves to the latest dated GPT-4 release (e.g., <code>gpt-4o-2024-08-06</code>).</li> <li><code>*-mini</code> \u2192 a lower-cost, smaller-context sibling model.</li> <li>Structured output means the model supports JSON schema / function calling.</li> <li>Web search means the model can search the web for up-to-date information.</li> </ul>"},{"location":"user-guide/model-capabilities/#accessing-model-capabilities","title":"Accessing Model Capabilities","text":"<p>You can access model capabilities through the <code>ModelRegistry</code> class:</p> <pre><code>from openai_model_registry import ModelRegistry\n\n# Get the registry instance\nregistry = ModelRegistry.get_default()\n\n# Get capabilities for a model\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Access capability information\nprint(f\"Context window: {capabilities.context_window}\")\nprint(f\"Max output tokens: {capabilities.max_output_tokens}\")\nprint(f\"Supports structured output: {capabilities.supports_structured}\")\nprint(f\"Supports streaming: {capabilities.supports_streaming}\")\nprint(f\"Supports web search: {capabilities.supports_web_search}\")\n# Modalities\nprint(f\"Input modalities: {getattr(capabilities, 'input_modalities', [])}\")\nprint(f\"Output modalities: {getattr(capabilities, 'output_modalities', [])}\")\n# Expected output: Context window: 128000\n#                  Max output tokens: 16384\n#                  Supports structured output: True\n#                  Supports streaming: True\n#                  Supports web search: True\n</code></pre>"},{"location":"user-guide/model-capabilities/#comparing-model-capabilities","title":"Comparing Model Capabilities","text":"<p>You can compare capabilities between different models:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\ngpt4o = registry.get_capabilities(\"gpt-4o\")\ngpt4o_mini = registry.get_capabilities(\"gpt-4o-mini\")\n\nprint(f\"GPT-4o context window: {gpt4o.context_window}\")\nprint(f\"GPT-4o-mini context window: {gpt4o_mini.context_window}\")\n\nprint(f\"GPT-4o max output tokens: {gpt4o.max_output_tokens}\")\nprint(f\"GPT-4o-mini max output tokens: {gpt4o_mini.max_output_tokens}\")\n# Expected output: GPT-4o context window: 128000\n#                  GPT-4o-mini context window: 128000\n#                  GPT-4o max output tokens: 16384\n#                  GPT-4o-mini max output tokens: 16384\n</code></pre>"},{"location":"user-guide/model-capabilities/#capabilities-for-dated-versions","title":"Capabilities for Dated Versions","text":"<p>The registry supports dated model versions, which have specific capabilities that may differ from the base model:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\n# Get capabilities for a dated version\ncapabilities = registry.get_capabilities(\"gpt-4o-2024-05-13\")\n\n# These capabilities might differ from the base \"gpt-4o\" model\nprint(f\"Context window: {capabilities.context_window}\")\n# Expected output: Context window: 128000\n</code></pre>"},{"location":"user-guide/model-capabilities/#model-deprecation-status","title":"Model Deprecation Status","text":"<p>Models can have deprecation information that helps you understand their lifecycle:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\n\n# Get capabilities for a model\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Check deprecation status\nprint(f\"Deprecation status: {capabilities.deprecation.status}\")\n\nif capabilities.is_deprecated:\n    print(\"\u26a0\ufe0f  This model is deprecated\")\n    if capabilities.deprecation.replacement:\n        print(f\"Recommended replacement: {capabilities.deprecation.replacement}\")\n    if capabilities.deprecation.migration_guide:\n        print(f\"Migration guide: {capabilities.deprecation.migration_guide}\")\n\nif capabilities.is_sunset:\n    print(\"\ud83d\udeab This model is sunset and no longer available\")\n\n# Get HTTP headers for deprecation status\nheaders = registry.get_sunset_headers(\"gpt-4o\")\nif headers:\n    print(f\"Sunset headers: {headers}\")\n# Expected output: Deprecation status: active\n</code></pre>"},{"location":"user-guide/model-capabilities/#checking-model-status","title":"Checking Model Status","text":"<p>You can also check if a model is active before using it:</p> <pre><code>from openai_model_registry import ModelRegistry\nfrom openai_model_registry.deprecation import ModelSunsetError\n\nregistry = ModelRegistry.get_default()\n\ntry:\n    # This will raise an exception if the model is sunset\n    registry.assert_model_active(\"gpt-4o\")\n    print(\"Model is active and safe to use\")\nexcept ModelSunsetError as e:\n    print(f\"Model is sunset: {e}\")\n    # Use the replacement model if available\n    if e.replacement:\n        capabilities = registry.get_capabilities(e.replacement)\n# Expected output: Model is active and safe to use\n</code></pre>"},{"location":"user-guide/model-capabilities/#web-search-support","title":"Web Search Support","text":""},{"location":"user-guide/model-capabilities/#modalities-input-vs-output","title":"Modalities: Input vs Output","text":"<p>The registry distinguishes between what a model can accept as input and what it can produce as output.</p> <ul> <li><code>input_modalities</code>: list of accepted input types (e.g., <code>text</code>, <code>image</code>, <code>audio</code>)</li> <li><code>output_modalities</code>: list of produced output types</li> </ul> <p>Examples:</p> <ul> <li><code>gpt-4o</code>: input <code>[text, image]</code>, output <code>[text]</code></li> <li><code>whisper-1</code>: input <code>[audio]</code>, output <code>[text]</code></li> <li><code>tts-1</code>: input <code>[text]</code>, output <code>[audio]</code></li> <li><code>dall-e-3</code>: input <code>[text]</code>, output <code>[image]</code></li> </ul> <p>Some models can search the web for up-to-date information. The OpenAI Model Registry uses a single boolean flag, <code>supports_web_search</code>, to indicate this capability. However, how web search is invoked and its behavior differs between OpenAI's Chat Completions API and Responses API.</p> <p>\u26a0\ufe0f Azure OpenAI Users: If you're using Azure OpenAI endpoints, please note that standard Azure Chat Completions and Responses APIs do not support the <code>web_search_preview</code> tool, regardless of what <code>supports_web_search</code> indicates. This is a platform limitation, not a model limitation. See our Azure OpenAI Usage Guide for detailed guidance and alternative approaches.</p>"},{"location":"user-guide/model-capabilities/#two-approaches-to-web-search","title":"Two Approaches to Web Search","text":"<ol> <li> <p>Chat Completions API (Always Searches):</p> </li> <li> <p>Models: Special \"search-preview\" models like <code>gpt-4o-search-preview</code> and <code>gpt-4o-mini-search-preview</code>.</p> </li> <li>Behavior: These models automatically perform a web search before every response.</li> <li>Use Case: Suitable when you require web-augmented answers for every user query.</li> <li> <p>Limitations: May have a restricted set of supported API parameters compared to standard chat models.</p> </li> <li> <p>Responses API (Conditional, Tool-Based Search):</p> </li> <li> <p>Models: Reasoning-enabled models such as <code>gpt-4o</code>, <code>gpt-4.1</code> (excluding <code>gpt-4.1-nano</code>), and the \"O-series\" (<code>o1</code>, <code>o3</code>, etc.).</p> </li> <li>Behavior: These models can use web search as a tool. The model intelligently decides whether a web search is necessary to answer the current query.</li> <li>Use Case: Offers more flexibility, as web search is only performed when needed, potentially saving costs and reducing latency.</li> <li>Invocation: Requires using the <code>/v1/responses</code> API endpoint and including <code>{\"type\": \"web_search_preview\"}</code> in the <code>tools</code> array of your request.</li> </ol>"},{"location":"user-guide/model-capabilities/#how-the-registry-captures-this","title":"How the Registry Captures This","text":"<p>The Model Registry simplifies this by:</p> <ul> <li>Unified Flag: Using <code>capabilities.supports_web_search</code> (boolean) to indicate if a model can perform web search, regardless of the API.</li> <li>Descriptive Information: The <code>description</code> field for search-preview models in the underlying <code>models.yaml</code> often clarifies their \"always searches\" behavior (e.g., \"GPT-4o with built-in web search for Chat Completions API (always searches)\").</li> </ul> <p>Note: The O1 family uses <code>reasoning_effort</code> (enum) instead of classic sampling knobs like <code>temperature</code>/<code>top_p</code>. Check supported parameters with:</p> <pre><code>omr --format json models get o1 --parameters-only\n</code></pre> <ul> <li>Model Naming: Following OpenAI's naming (e.g., <code>-search-preview</code> suffix for Chat API models).</li> </ul> <p>The registry's role is to inform your application if web search is available. It's the application's responsibility to use the correct API endpoint and parameters based on the chosen model and desired search behavior.</p>"},{"location":"user-guide/model-capabilities/#example-checking-and-using-web-search","title":"Example: Checking and Using Web Search","text":"<pre><code>from openai_model_registry import ModelRegistry\n\n# Assuming 'openai' client is initialized\n\nregistry = ModelRegistry.get_default()\n\n\ndef get_web_augmented_response(model_name: str, query: str):\n    try:\n        capabilities = registry.get_capabilities(model_name)\n    except Exception as e:\n        print(f\"Error getting capabilities for {model_name}: {e}\")\n        return None\n\n    if not capabilities.supports_web_search:\n        print(\n            f\"Model {model_name} does not support web search. Using standard completion.\"\n        )\n        # Fallback to standard chat completion if web search is not supported\n        # This is a simplified example; you might have different fallback logic\n        try:\n            response = openai.chat.completions.create(\n                model=model_name,  # Or a default non-web-search model\n                messages=[{\"role\": \"user\", \"content\": query}],\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"Error with standard completion for {model_name}: {e}\")\n            return None\n\n    # Determine API and invocation based on model name convention\n    if model_name.endswith(\"-search-preview\"):\n        print(f\"Using Chat Completions API for {model_name} (always searches).\")\n        try:\n            response = openai.chat.completions.create(\n                model=model_name, messages=[{\"role\": \"user\", \"content\": query}]\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"Error with Chat API for {model_name}: {e}\")\n            return None\n    else:\n        print(f\"Using Responses API for {model_name} (conditional search).\")\n        try:\n            response = openai.responses.create(\n                model=model_name,\n                tools=[{\"type\": \"web_search_preview\"}],\n                input=query,\n                # Note: The actual 'input' structure for Responses API\n                # might be more complex depending on your needs (e.g., messages list)\n            )\n            # Process Responses API output (may differ from Chat Completions)\n            # This is a simplified example\n            if hasattr(response, \"output\") and response.output:\n                return response.output[0].text.value  # Example access\n            return \"Response format from Responses API needs specific handling.\"\n        except Exception as e:\n            print(f\"Error with Responses API for {model_name}: {e}\")\n            return None\n\n\n# Test cases\n# print(get_web_augmented_response(\"gpt-4o-search-preview\", \"What's the weather like today?\"))\n# print(get_web_augmented_response(\"gpt-4o\", \"Latest news on AI advancements?\"))\n# print(get_web_augmented_response(\"gpt-4.1-nano\", \"Tell me a joke.\")) # Should use fallback\n</code></pre> <p>Key Considerations:</p> <ul> <li><code>gpt-4.1-nano</code>: Explicitly does not support web search, even though other <code>gpt-4.1</code> variants do. The registry correctly reflects this (<code>supports_web_search: false</code>).</li> <li>API Endpoint: Remember to use <code>/v1/chat/completions</code> for search-preview models and <code>/v1/responses</code> for tool-based search with other compatible models.</li> <li>Parameter Validation: Web search specific parameters (e.g., <code>web_search_options</code>, <code>user_location</code>) are handled and validated by OpenAI's API, not by this registry. The registry only indicates the capability's existence.</li> </ul>"},{"location":"user-guide/model-capabilities/#next-steps","title":"Next Steps","text":"<p>Now that you understand model capabilities, learn about Parameter Validation to ensure your application uses valid parameter values.</p>"},{"location":"user-guide/parameter-validation/","title":"Parameter Validation","text":"<p>This guide explains how to validate parameters against model constraints using the OpenAI Model Registry.</p>"},{"location":"user-guide/parameter-validation/#why-validate-parameters","title":"Why Validate Parameters?","text":"<p>Parameter validation ensures that your application uses valid values for model parameters, preventing runtime errors when calling the OpenAI API. Different models may have different constraints for the same parameter.</p>"},{"location":"user-guide/parameter-validation/#basic-parameter-validation","title":"Basic Parameter Validation","text":"<p>You can validate parameters through the model capabilities object:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Validate a temperature value\ntry:\n    capabilities.validate_parameter(\"temperature\", 0.7)\n    print(\"\u2705 Valid temperature\")\nexcept ValueError as e:\n    print(f\"\u274c Invalid temperature: {e}\")\n# Expected output: \u2705 Valid temperature\n</code></pre>"},{"location":"user-guide/parameter-validation/#handling-validation-errors","title":"Handling Validation Errors","text":"<p>Validation errors provide detailed information about why a parameter is invalid:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\ntry:\n    # Invalid temperature (outside of allowed range)\n    capabilities.validate_parameter(\"temperature\", 3.0)\nexcept Exception as e:\n    print(f\"Validation error: {e}\")\n    # Expected output: Validation error: Parameter 'temperature' must be between 0 and 2.\n    # Description: Controls randomness: Lowering results in less random completions.\n    # Current value: 3.0\n</code></pre>"},{"location":"user-guide/parameter-validation/#common-parameter-types","title":"Common Parameter Types","text":""},{"location":"user-guide/parameter-validation/#numeric-parameters","title":"Numeric Parameters","text":"<p>Numeric parameters typically have constraints for:</p> <ul> <li>Minimum value</li> <li>Maximum value</li> <li>Whether the parameter allows floats, integers, or both</li> </ul> <p>Examples of numeric parameters:</p> <ul> <li><code>temperature</code>: Controls randomness (typically 0-2)</li> <li><code>top_p</code>: Controls diversity via nucleus sampling (typically 0-1)</li> <li><code>max_tokens</code>: Controls maximum completion length (typically 1-model_max)</li> </ul>"},{"location":"user-guide/parameter-validation/#enum-parameters","title":"Enum Parameters","text":"<p>Enum parameters accept only specific string values from a predefined list.</p> <p>Examples of enum parameters:</p> <ul> <li><code>response_format</code>: Format of the model's output (e.g., \"text\", \"json_schema\")</li> <li><code>reasoning_effort</code> (O1 model): Level of reasoning effort (e.g., \"low\", \"medium\", \"high\")</li> </ul>"},{"location":"user-guide/parameter-validation/#model-specific-parameters","title":"Model-Specific Parameters","text":"<p>Different models may support different parameters. For example, the O1 model has parameters not available in other models:</p> <pre><code>from openai_model_registry import ModelRegistry\n\n# Initialize registry\nregistry = ModelRegistry.get_default()\n\n# Get O1 capabilities\no1_capabilities = registry.get_capabilities(\"o1\")\n\n# Validate O1-specific parameter\ntry:\n    o1_capabilities.validate_parameter(\"reasoning_effort\", \"medium\")\n    print(\"reasoning_effort 'medium' is valid for O1\")\nexcept Exception as e:\n    print(f\"Invalid parameter: {e}\")\n# Expected output: reasoning_effort 'medium' is valid for O1\n</code></pre>"},{"location":"user-guide/parameter-validation/#getting-supported-parameters","title":"Getting Supported Parameters","text":"<p>You can retrieve the list of parameters supported by a specific model:</p> <pre><code>from openai_model_registry import ModelRegistry\n\nregistry = ModelRegistry.get_default()\ncapabilities = registry.get_capabilities(\"gpt-4o\")\n\n# Get supported parameters (inline)\nparam_names = list((capabilities.inline_parameters or {}).keys())\nprint(f\"Supported parameters: {', '.join(sorted(param_names))}\")\n# Expected output: Supported parameters: max_tokens, temperature, top_p, ...\n</code></pre>"},{"location":"user-guide/parameter-validation/#next-steps","title":"Next Steps","text":"<p>Now that you understand parameter validation, learn about Advanced Usage for more complex scenarios and registry management.</p>"},{"location":"user-guide/parameter-validation/#recommended-defaults-chat-models","title":"Recommended defaults (chat models)","text":"<p>These are sensible starting points for common chat models. Adjust for your app:</p> <ul> <li>Temperature: 0.7</li> <li>Top_p: 1.0</li> <li>Max tokens: choose per model up to its max_output_tokens</li> <li>Response format: \"text\" (use JSON mode where supported for structured outputs)</li> </ul> <p>Note: Not all models expose all knobs (e.g., O1 family does not support classic sampling controls). Check <code>omr --format json models get &lt;model&gt; --parameters-only</code> to see what a given model supports.</p>"},{"location":"user-guide/testing/","title":"Testing with OpenAI Model Registry","text":"<p>This guide shows how to test your applications that use the OpenAI Model Registry, including working with pytest, pyfakefs, and mocking strategies.</p>"},{"location":"user-guide/testing/#overview","title":"Overview","text":"<p>When testing applications that use OpenAI Model Registry, you may encounter challenges related to:</p> <ul> <li>Filesystem interactions - The registry reads configuration from disk</li> <li>Singleton behavior - The default registry instance is cached</li> <li>Cross-platform paths - The registry uses platform-specific directories</li> <li>Network operations - Registry updates fetch data from remote sources</li> </ul> <p>This guide provides patterns and best practices for handling these scenarios in your tests.</p>"},{"location":"user-guide/testing/#basic-testing-patterns","title":"Basic Testing Patterns","text":""},{"location":"user-guide/testing/#simple-mocking","title":"Simple Mocking","text":"<p>For basic tests where you just need to mock registry responses:</p> <pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom openai_model_registry import ModelRegistry, ModelCapabilities\n\n\ndef test_my_function_with_mocked_registry():\n    \"\"\"Test your function with a completely mocked registry.\"\"\"\n\n    # Create a mock capabilities object\n    mock_capabilities = Mock(spec=ModelCapabilities)\n    mock_capabilities.context_window = 4096\n    mock_capabilities.max_output_tokens = 1024\n    mock_capabilities.supports_streaming = True\n\n    # Mock the registry\n    mock_registry = Mock(spec=ModelRegistry)\n    mock_registry.get_capabilities.return_value = mock_capabilities\n\n    with patch(\n        \"openai_model_registry.ModelRegistry.get_default\", return_value=mock_registry\n    ):\n\n        # Your application code here\n        result = my_function_that_uses_registry(\"gpt-4o\")\n\n        # Verify the registry was called correctly\n        mock_registry.get_capabilities.assert_called_once_with(\"gpt-4o\")\n        assert result.expected_property == expected_value\n</code></pre>"},{"location":"user-guide/testing/#testing-with-real-registry-data","title":"Testing with Real Registry Data","text":"<p>If you want to test with actual registry data but control the configuration:</p> <pre><code>import tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch\nfrom openai_model_registry import ModelRegistry, RegistryConfig\n\n\ndef test_my_function_with_real_registry():\n    \"\"\"Test with real registry using temporary configuration.\"\"\"\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        # Create a custom registry configuration\n        temp_registry_file = Path(temp_dir) / \"models.yaml\"\n        temp_constraints_file = Path(temp_dir) / \"constraints.yml\"\n\n        # Copy or create test configuration files\n        # (you can copy from the package's data directory or create custom ones)\n\n        config = RegistryConfig(\n            registry_path=str(temp_registry_file),\n            constraints_path=str(temp_constraints_file),\n        )\n\n        registry = ModelRegistry(config)\n\n        # Test your code with this registry\n        result = my_function_that_uses_registry_instance(registry, \"gpt-4o\")\n        assert result.is_valid\n</code></pre>"},{"location":"user-guide/testing/#testing-with-pyfakefs","title":"Testing with pyfakefs","text":"<p>When your tests need to interact with the filesystem (especially when using pyfakefs), you need to handle the registry's cross-platform directory behavior.</p>"},{"location":"user-guide/testing/#case-1-mock-the-directory-functions","title":"Case 1: Mock the Directory Functions","text":"<p>The cleanest approach is to mock the platformdirs functions:</p> <pre><code>import pytest\nfrom pathlib import Path\nfrom unittest.mock import patch\nfrom openai_model_registry import ModelRegistry\n\n\ndef test_my_app_with_fake_registry_paths(fs):\n    \"\"\"Test your app with fake filesystem paths.\"\"\"\n\n    # Set up fake directories\n    fake_data_dir = Path(\"/fake_data/openai-model-registry\")\n    fake_config_dir = Path(\"/fake_config/openai-model-registry\")\n\n    fs.makedirs(fake_data_dir, exist_ok=True)\n    fs.makedirs(fake_config_dir, exist_ok=True)\n\n    # Create fake registry file\n    fake_models_file = fake_data_dir / \"models.yaml\"\n    fs.create_file(\n        fake_models_file,\n        contents=\"\"\"\nversion: \"2.0.0\"\nmodels:\n  test-model-2024-01-01:\n    context_window: 4096\n    max_output_tokens: 1024\n    deprecation:\n      status: \"active\"\n      reason: \"test model\"\n\"\"\",\n    )\n\n    # Mock the directory functions\n    with patch(\n        \"openai_model_registry.config_paths.get_user_data_dir\",\n        return_value=fake_data_dir,\n    ), patch(\n        \"openai_model_registry.config_paths.get_user_config_dir\",\n        return_value=fake_config_dir,\n    ):\n\n        # Clear any cached registry instances\n        ModelRegistry.cleanup()\n\n        # Test your application\n        result = my_application_function()\n\n        # Your assertions here\n        assert result.model_used == \"test-model\"\n</code></pre>"},{"location":"user-guide/testing/#case-2-environment-variable-override","title":"Case 2: Environment Variable Override","text":"<p>Use environment variables to redirect the registry to fake locations:</p> <pre><code>import pytest\nfrom unittest.mock import patch\n\n\ndef test_my_app_with_custom_registry_path(fs, monkeypatch):\n    \"\"\"Test using environment variable to set custom registry path.\"\"\"\n\n    # Create fake registry file\n    custom_registry_path = \"/custom/registry/models.yaml\"\n    fs.create_file(\n        custom_registry_path,\n        contents=\"\"\"\nversion: \"2.0.0\"\nmodels:\n  custom-model-2024-01-01:\n    context_window: 8192\n    max_output_tokens: 2048\n    deprecation:\n      status: \"active\"\n      reason: \"custom model\"\n\"\"\",\n    )\n\n    # Set environment variable\n    monkeypatch.setenv(\"OMR_MODEL_REGISTRY_PATH\", custom_registry_path)\n\n    # Clear registry cache to pick up new environment\n    ModelRegistry.cleanup()\n\n    # Test your application\n    result = my_app_function_that_uses_custom_model()\n    assert \"custom-model\" in result.models_used\n</code></pre>"},{"location":"user-guide/testing/#testing-registry-updates-and-network-operations","title":"Testing Registry Updates and Network Operations","text":"<p>When testing code that triggers registry updates:</p> <pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom openai_model_registry import ModelRegistry\nfrom openai_model_registry.registry import RefreshStatus\n\n\ndef test_my_app_handles_registry_updates():\n    \"\"\"Test your app's behavior when registry updates are available.\"\"\"\n\n    with patch(\"requests.get\") as mock_get, patch(\n        \"openai_model_registry.config_paths.get_user_data_dir\"\n    ) as mock_data_dir:\n\n        # Mock successful update response\n        mock_response = Mock()\n        mock_response.status_code = 200\n        mock_response.text = \"\"\"\nversion: \"2.0.0\"\nmodels: {}\n\"\"\"\n        mock_get.return_value = mock_response\n\n        # Test your application's update handling\n        result = my_app_check_for_updates()\n\n        assert result.update_available == True\n        assert result.handled_correctly == True\n\n\ndef test_my_app_handles_network_errors():\n    \"\"\"Test your app's behavior when registry updates fail.\"\"\"\n\n    with patch(\"requests.get\") as mock_get:\n        # Mock network error\n        mock_get.side_effect = ConnectionError(\"Network unavailable\")\n\n        # Test your application's error handling\n        result = my_app_check_for_updates()\n\n        assert result.update_failed == True\n        assert result.fallback_used == True\n</code></pre>"},{"location":"user-guide/testing/#testing-different-model-configurations","title":"Testing Different Model Configurations","text":"<p>Test how your application handles different model scenarios:</p> <pre><code>import pytest\nfrom unittest.mock import Mock\nfrom openai_model_registry import ModelCapabilities\nfrom openai_model_registry.deprecation import DeprecationInfo\n\n\n@pytest.fixture\ndef deprecated_model_capabilities():\n    \"\"\"Create capabilities for a deprecated model.\"\"\"\n    deprecation = DeprecationInfo(\n        status=\"deprecated\",\n        deprecates_on=None,\n        sunsets_on=None,\n        replacement=\"gpt-4o\",\n        migration_guide=\"Use gpt-4o instead\",\n        reason=\"Model deprecated\",\n    )\n\n    capabilities = Mock(spec=ModelCapabilities)\n    capabilities.context_window = 8192\n    capabilities.max_output_tokens = 1024\n    capabilities.deprecation = deprecation\n    capabilities.is_deprecated = True\n\n    return capabilities\n\n\ndef test_my_app_handles_deprecated_models(deprecated_model_capabilities):\n    \"\"\"Test your app's handling of deprecated models.\"\"\"\n\n    with patch(\"openai_model_registry.ModelRegistry.get_default\") as mock_registry:\n        mock_registry.return_value.get_capabilities.return_value = (\n            deprecated_model_capabilities\n        )\n\n        result = my_app_select_model(\"deprecated-model\")\n\n        # Verify your app handles deprecation appropriately\n        assert result.warning_shown == True\n        assert result.suggested_alternative == \"gpt-4o\"\n\n\n@pytest.fixture\ndef high_capacity_model():\n    \"\"\"Create capabilities for a high-capacity model.\"\"\"\n    capabilities = Mock(spec=ModelCapabilities)\n    capabilities.context_window = 1000000  # 1M tokens\n    capabilities.max_output_tokens = 32768\n    capabilities.supports_streaming = True\n    capabilities.supports_structured = True\n\n    return capabilities\n\n\ndef test_my_app_uses_high_capacity_features(high_capacity_model):\n    \"\"\"Test your app leverages high-capacity model features.\"\"\"\n\n    with patch(\"openai_model_registry.ModelRegistry.get_default\") as mock_registry:\n        mock_registry.return_value.get_capabilities.return_value = high_capacity_model\n\n        result = my_app_process_large_document(\"huge-document.txt\")\n\n        assert result.used_streaming == True\n        assert result.context_exceeded == False\n</code></pre>"},{"location":"user-guide/testing/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/testing/#1-always-clear-registry-cache","title":"1. Always Clear Registry Cache","text":"<p>The registry uses singleton behavior, so clear the cache between tests:</p> <pre><code>import pytest\nfrom openai_model_registry import ModelRegistry\n\n\n@pytest.fixture(autouse=True)\ndef clear_registry_cache():\n    \"\"\"Automatically clear registry cache before each test.\"\"\"\n    ModelRegistry.cleanup()\n    yield\n    ModelRegistry.cleanup()\n</code></pre>"},{"location":"user-guide/testing/#2-use-realistic-test-data","title":"2. Use Realistic Test Data","text":"<p>When creating fake registry data, use realistic model configurations:</p> <pre><code># Good - includes all required fields\nREALISTIC_MODEL_CONFIG = \"\"\"\nversion: \"2.0.0\"\nmodels:\n  test-model-2024-01-01:\n    context_window: 128000\n    max_output_tokens: 16384\n    supports_streaming: true\n    supports_structured: true\n    supported_parameters:\n      - ref: \"numeric_constraints.temperature\"\n      - ref: \"numeric_constraints.top_p\"\n    deprecation:\n      status: \"active\"\n      deprecates_on: null\n      sunsets_on: null\n      replacement: null\n      migration_guide: null\n      reason: \"active\"\n    min_version:\n      year: 2024\n      month: 1\n      day: 1\n\"\"\"\n</code></pre>"},{"location":"user-guide/testing/#3-test-error-conditions","title":"3. Test Error Conditions","text":"<p>Test how your app handles registry errors:</p> <pre><code>def test_my_app_handles_missing_models():\n    \"\"\"Test app behavior when requested model doesn't exist.\"\"\"\n\n    with patch(\"openai_model_registry.ModelRegistry.get_default\") as mock_registry:\n        from openai_model_registry import ModelNotSupportedError\n\n        mock_registry.return_value.get_capabilities.side_effect = (\n            ModelNotSupportedError(\"Model not found\", model=\"nonexistent-model\")\n        )\n\n        result = my_app_use_model(\"nonexistent-model\")\n\n        assert result.used_fallback_model == True\n        assert result.error_logged == True\n</code></pre>"},{"location":"user-guide/testing/#4-test-parameter-validation-integration","title":"4. Test Parameter Validation Integration","text":"<p>If your app validates parameters using the registry:</p> <pre><code>def test_my_app_validates_parameters_correctly():\n    \"\"\"Test that your app properly validates model parameters.\"\"\"\n\n    # Mock a model with specific parameter constraints\n    mock_capabilities = Mock()\n    mock_capabilities.validate_parameter.side_effect = [\n        None,  # temperature=0.7 is valid\n        ValueError(\"temperature must be between 0 and 2\"),  # temperature=3.0 is invalid\n    ]\n\n    with patch(\"openai_model_registry.ModelRegistry.get_default\") as mock_registry:\n        mock_registry.return_value.get_capabilities.return_value = mock_capabilities\n\n        # Test valid parameters\n        result1 = my_app_call_openai(\"gpt-4o\", temperature=0.7)\n        assert result1.parameters_valid == True\n\n        # Test invalid parameters\n        result2 = my_app_call_openai(\"gpt-4o\", temperature=3.0)\n        assert result2.validation_error == True\n        assert result2.error_message == \"temperature must be between 0 and 2\"\n</code></pre>"},{"location":"user-guide/testing/#integration-testing","title":"Integration Testing","text":"<p>For integration tests that need the full registry behavior:</p> <pre><code>def test_end_to_end_with_real_registry():\n    \"\"\"Integration test using the actual registry.\"\"\"\n\n    # Use the real registry (ensure it's properly installed)\n    registry = ModelRegistry.get_default()\n\n    # Test with a model you know exists\n    try:\n        capabilities = registry.get_capabilities(\"gpt-4o\")\n        result = my_complete_workflow(\"gpt-4o\", \"test prompt\")\n\n        assert result.model_used == \"gpt-4o\"\n        assert result.context_within_limits == True\n\n    except Exception as e:\n        pytest.skip(f\"Integration test skipped due to registry issue: {e}\")\n</code></pre> <p>This testing approach ensures your application properly integrates with the OpenAI Model Registry while maintaining test reliability and isolation.</p>"},{"location":"user-guide/testing/#cli-based-debugging","title":"CLI-Based Debugging","text":"<p>For interactive debugging and inspection during testing, use the <code>omr</code> CLI:</p> <pre><code># Check data source paths and status\nomr data paths\n\n# Show environment variables affecting tests\nomr data env\n\n# List all models available in tests\nomr models list\n\n# Inspect specific model data\nomr models get gpt-4o --effective\n\n# Check cache status that might affect tests\nomr cache info\n</code></pre> <p>The CLI is particularly useful for:</p> <ul> <li>Debugging test failures - Check what data the registry is actually loading</li> <li>Validating test environments - Ensure test data is set up correctly</li> <li>Comparing configurations - Switch providers and compare model data</li> <li>Cache management - Clear cache between test runs if needed</li> </ul> <p>See the CLI Reference for complete documentation.</p>"}]}